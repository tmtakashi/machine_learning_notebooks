{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chokkan_deeplearning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tmtakashi/machine_learning_notebooks/blob/master/chokkan_deeplearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "zSA_nCDuBsPl",
        "colab_type": "code",
        "outputId": "9e30ec4b-95b3-4461-b686-33142406b2ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Tzqr-EsJ8Mvg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 第１章：ニューラルネットワークの基礎"
      ]
    },
    {
      "metadata": {
        "id": "NBdBKawg_wz3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## シンプルなネットワーク（入力層と出力層のみ）\n"
      ]
    },
    {
      "metadata": {
        "id": "_ECd6xFt7qP_",
        "colab_type": "code",
        "outputId": "fd3a5446-d9c0-47ed-e03e-8dfa71a1c005",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7106
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.optimizers import SGD\n",
        "from keras.utils import np_utils \n",
        "\n",
        "\n",
        "# 再現性のためのシード\n",
        "np.random.seed(1671)\n",
        "\n",
        "# ハイパーパラメータ\n",
        "NB_EPOCH = 200\n",
        "BATCH_SIZE = 128\n",
        "VERBOSE = 1\n",
        "NB_CLASSES = 10\n",
        "OPTIMIZER = SGD()\n",
        "N_HIDDEN = 128\n",
        "VALIDATION_SPLIT = 0.2\n",
        "\n",
        "# --------\n",
        "#データの前処理\n",
        "# --------\n",
        "\n",
        "# データの読み込み\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# 28x28の画像のreshape\n",
        "RESHAPED = 784\n",
        "\n",
        "X_train = X_train.reshape(60000, RESHAPED)\n",
        "X_test = X_test.reshape(10000, RESHAPED)\n",
        "\n",
        "# GPU計算のためにfloat32型に変換\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "# 正規化\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')\n",
        "\n",
        "# ラベルをone-hotに変換\n",
        "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
        "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
        "\n",
        "# -------------\n",
        "# モデル構築\n",
        "# -------------\n",
        "model = Sequential()\n",
        "model.add(Dense(NB_CLASSES, input_shape=(RESHAPED, )))\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, \n",
        "             metrics=['accuracy'])\n",
        "\n",
        "# -------------------------\n",
        "# Tensor Boardの設定\n",
        "# -------------------------\n",
        "\n",
        "import os \n",
        "from time import gmtime, strftime\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "def make_tensorboard(set_dir_name=''):\n",
        "  tictoc = strftime(\"%a_ %d_ %b_ &Y %H_ %M_ %S\", gmtime())\n",
        "  directory_name = tictoc\n",
        "  log_dir = set_dir_name + '_' + directory_name\n",
        "  os.mkdir(log_dir)\n",
        "  tensorboard = TensorBoard(log_dir=log_dir)\n",
        "  return tensorboard\n",
        "\n",
        "callbacks = [make_tensorboard(set_dir_name='drive/My Drive/Colab Notebooks/chokkan_deeplearning/keras_MNIST_V1')]\n",
        "\n",
        "# -----\n",
        "# 学習\n",
        "# -----\n",
        "model.fit(X_train, Y_train, \n",
        "         batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
        "         callbacks=callbacks,\n",
        "         verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
        "\n",
        "# ------\n",
        "# 評価\n",
        "# ------\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
        "print(\"Test score: \", score[0])\n",
        "print(\"Test accuracy: \", score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000 train samples\n",
            "10000 test samples\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_10 (Dense)             (None, 10)                7850      \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 7,850\n",
            "Trainable params: 7,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/200\n",
            "48000/48000 [==============================] - 2s 38us/step - loss: 1.3633 - acc: 0.6796 - val_loss: 0.8904 - val_acc: 0.8246\n",
            "Epoch 2/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.7913 - acc: 0.8272 - val_loss: 0.6572 - val_acc: 0.8546\n",
            "Epoch 3/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.6436 - acc: 0.8497 - val_loss: 0.5625 - val_acc: 0.8681\n",
            "Epoch 4/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.5717 - acc: 0.8602 - val_loss: 0.5098 - val_acc: 0.8765\n",
            "Epoch 5/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.5276 - acc: 0.8678 - val_loss: 0.4758 - val_acc: 0.8826\n",
            "Epoch 6/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.4973 - acc: 0.8726 - val_loss: 0.4515 - val_acc: 0.8866\n",
            "Epoch 7/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.4748 - acc: 0.8775 - val_loss: 0.4333 - val_acc: 0.8882\n",
            "Epoch 8/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.4574 - acc: 0.8803 - val_loss: 0.4189 - val_acc: 0.8920\n",
            "Epoch 9/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.4433 - acc: 0.8834 - val_loss: 0.4075 - val_acc: 0.8939\n",
            "Epoch 10/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.4317 - acc: 0.8850 - val_loss: 0.3977 - val_acc: 0.8966\n",
            "Epoch 11/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.4218 - acc: 0.8873 - val_loss: 0.3896 - val_acc: 0.8984\n",
            "Epoch 12/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.4134 - acc: 0.8888 - val_loss: 0.3827 - val_acc: 0.8995\n",
            "Epoch 13/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.4060 - acc: 0.8902 - val_loss: 0.3766 - val_acc: 0.9003\n",
            "Epoch 14/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.3995 - acc: 0.8918 - val_loss: 0.3712 - val_acc: 0.9013\n",
            "Epoch 15/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.3936 - acc: 0.8928 - val_loss: 0.3664 - val_acc: 0.9016\n",
            "Epoch 16/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.3884 - acc: 0.8945 - val_loss: 0.3621 - val_acc: 0.9031\n",
            "Epoch 17/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.3837 - acc: 0.8950 - val_loss: 0.3582 - val_acc: 0.9033\n",
            "Epoch 18/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.3794 - acc: 0.8962 - val_loss: 0.3546 - val_acc: 0.9039\n",
            "Epoch 19/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.3755 - acc: 0.8970 - val_loss: 0.3514 - val_acc: 0.9048\n",
            "Epoch 20/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.3718 - acc: 0.8979 - val_loss: 0.3485 - val_acc: 0.9053\n",
            "Epoch 21/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.3685 - acc: 0.8985 - val_loss: 0.3457 - val_acc: 0.9058\n",
            "Epoch 22/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.3653 - acc: 0.8995 - val_loss: 0.3431 - val_acc: 0.9058\n",
            "Epoch 23/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.3625 - acc: 0.8999 - val_loss: 0.3407 - val_acc: 0.9063\n",
            "Epoch 24/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.3598 - acc: 0.9008 - val_loss: 0.3385 - val_acc: 0.9070\n",
            "Epoch 25/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.3572 - acc: 0.9012 - val_loss: 0.3364 - val_acc: 0.9074\n",
            "Epoch 26/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.3548 - acc: 0.9019 - val_loss: 0.3345 - val_acc: 0.9084\n",
            "Epoch 27/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.3525 - acc: 0.9022 - val_loss: 0.3326 - val_acc: 0.9082\n",
            "Epoch 28/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.3504 - acc: 0.9032 - val_loss: 0.3311 - val_acc: 0.9090\n",
            "Epoch 29/200\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.3484 - acc: 0.9031 - val_loss: 0.3293 - val_acc: 0.9094\n",
            "Epoch 30/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.3465 - acc: 0.9041 - val_loss: 0.3277 - val_acc: 0.9097\n",
            "Epoch 31/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.3447 - acc: 0.9044 - val_loss: 0.3264 - val_acc: 0.9097\n",
            "Epoch 32/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.3430 - acc: 0.9047 - val_loss: 0.3249 - val_acc: 0.9097\n",
            "Epoch 33/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.3413 - acc: 0.9051 - val_loss: 0.3235 - val_acc: 0.9103\n",
            "Epoch 34/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.3397 - acc: 0.9056 - val_loss: 0.3222 - val_acc: 0.9104\n",
            "Epoch 35/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.3382 - acc: 0.9058 - val_loss: 0.3211 - val_acc: 0.9110\n",
            "Epoch 36/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.3368 - acc: 0.9062 - val_loss: 0.3198 - val_acc: 0.9110\n",
            "Epoch 37/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.3353 - acc: 0.9069 - val_loss: 0.3187 - val_acc: 0.9117\n",
            "Epoch 38/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.3340 - acc: 0.9075 - val_loss: 0.3177 - val_acc: 0.9120\n",
            "Epoch 39/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.3327 - acc: 0.9075 - val_loss: 0.3166 - val_acc: 0.9122\n",
            "Epoch 40/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.3314 - acc: 0.9078 - val_loss: 0.3159 - val_acc: 0.9118\n",
            "Epoch 41/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.3303 - acc: 0.9080 - val_loss: 0.3147 - val_acc: 0.9127\n",
            "Epoch 42/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.3291 - acc: 0.9084 - val_loss: 0.3138 - val_acc: 0.9132\n",
            "Epoch 43/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.3280 - acc: 0.9089 - val_loss: 0.3130 - val_acc: 0.9132\n",
            "Epoch 44/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.3270 - acc: 0.9091 - val_loss: 0.3121 - val_acc: 0.9132\n",
            "Epoch 45/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.3259 - acc: 0.9093 - val_loss: 0.3113 - val_acc: 0.9135\n",
            "Epoch 46/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.3249 - acc: 0.9095 - val_loss: 0.3105 - val_acc: 0.9137\n",
            "Epoch 47/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.3239 - acc: 0.9105 - val_loss: 0.3098 - val_acc: 0.9141\n",
            "Epoch 48/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.3230 - acc: 0.9105 - val_loss: 0.3090 - val_acc: 0.9146\n",
            "Epoch 49/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.3221 - acc: 0.9102 - val_loss: 0.3083 - val_acc: 0.9151\n",
            "Epoch 50/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.3212 - acc: 0.9109 - val_loss: 0.3075 - val_acc: 0.9150\n",
            "Epoch 51/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.3204 - acc: 0.9109 - val_loss: 0.3070 - val_acc: 0.9150\n",
            "Epoch 52/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.3195 - acc: 0.9112 - val_loss: 0.3063 - val_acc: 0.9148\n",
            "Epoch 53/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.3187 - acc: 0.9114 - val_loss: 0.3057 - val_acc: 0.9153\n",
            "Epoch 54/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.3180 - acc: 0.9117 - val_loss: 0.3050 - val_acc: 0.9148\n",
            "Epoch 55/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.3171 - acc: 0.9121 - val_loss: 0.3044 - val_acc: 0.9149\n",
            "Epoch 56/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.3164 - acc: 0.9121 - val_loss: 0.3037 - val_acc: 0.9156\n",
            "Epoch 57/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.3157 - acc: 0.9128 - val_loss: 0.3034 - val_acc: 0.9152\n",
            "Epoch 58/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.3149 - acc: 0.9121 - val_loss: 0.3029 - val_acc: 0.9148\n",
            "Epoch 59/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.3143 - acc: 0.9128 - val_loss: 0.3022 - val_acc: 0.9151\n",
            "Epoch 60/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.3136 - acc: 0.9129 - val_loss: 0.3016 - val_acc: 0.9161\n",
            "Epoch 61/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.3130 - acc: 0.9133 - val_loss: 0.3011 - val_acc: 0.9158\n",
            "Epoch 62/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.3123 - acc: 0.9131 - val_loss: 0.3007 - val_acc: 0.9151\n",
            "Epoch 63/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.3117 - acc: 0.9136 - val_loss: 0.3003 - val_acc: 0.9156\n",
            "Epoch 64/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.3110 - acc: 0.9137 - val_loss: 0.2997 - val_acc: 0.9158\n",
            "Epoch 65/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.3105 - acc: 0.9137 - val_loss: 0.2992 - val_acc: 0.9159\n",
            "Epoch 66/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.3098 - acc: 0.9138 - val_loss: 0.2988 - val_acc: 0.9161\n",
            "Epoch 67/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.3093 - acc: 0.9141 - val_loss: 0.2983 - val_acc: 0.9165\n",
            "Epoch 68/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.3087 - acc: 0.9139 - val_loss: 0.2979 - val_acc: 0.9166\n",
            "Epoch 69/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.3082 - acc: 0.9144 - val_loss: 0.2976 - val_acc: 0.9164\n",
            "Epoch 70/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.3077 - acc: 0.9145 - val_loss: 0.2971 - val_acc: 0.9166\n",
            "Epoch 71/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.3071 - acc: 0.9146 - val_loss: 0.2967 - val_acc: 0.9172\n",
            "Epoch 72/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.3066 - acc: 0.9147 - val_loss: 0.2964 - val_acc: 0.9167\n",
            "Epoch 73/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.3061 - acc: 0.9151 - val_loss: 0.2960 - val_acc: 0.9169\n",
            "Epoch 74/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.3056 - acc: 0.9150 - val_loss: 0.2956 - val_acc: 0.9173\n",
            "Epoch 75/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.3051 - acc: 0.9151 - val_loss: 0.2952 - val_acc: 0.9177\n",
            "Epoch 76/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.3046 - acc: 0.9152 - val_loss: 0.2950 - val_acc: 0.9173\n",
            "Epoch 77/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.3042 - acc: 0.9154 - val_loss: 0.2945 - val_acc: 0.9172\n",
            "Epoch 78/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.3037 - acc: 0.9154 - val_loss: 0.2942 - val_acc: 0.9176\n",
            "Epoch 79/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.3032 - acc: 0.9157 - val_loss: 0.2939 - val_acc: 0.9179\n",
            "Epoch 80/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.3028 - acc: 0.9156 - val_loss: 0.2936 - val_acc: 0.9177\n",
            "Epoch 81/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.3024 - acc: 0.9157 - val_loss: 0.2933 - val_acc: 0.9179\n",
            "Epoch 82/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.3019 - acc: 0.9157 - val_loss: 0.2930 - val_acc: 0.9178\n",
            "Epoch 83/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.3015 - acc: 0.9160 - val_loss: 0.2926 - val_acc: 0.9182\n",
            "Epoch 84/200\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.3011 - acc: 0.9161 - val_loss: 0.2924 - val_acc: 0.9179\n",
            "Epoch 85/200\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.3007 - acc: 0.9165 - val_loss: 0.2920 - val_acc: 0.9184\n",
            "Epoch 86/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.3003 - acc: 0.9164 - val_loss: 0.2918 - val_acc: 0.9185\n",
            "Epoch 87/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2999 - acc: 0.9165 - val_loss: 0.2914 - val_acc: 0.9185\n",
            "Epoch 88/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.2995 - acc: 0.9166 - val_loss: 0.2911 - val_acc: 0.9188\n",
            "Epoch 89/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.2991 - acc: 0.9167 - val_loss: 0.2909 - val_acc: 0.9191\n",
            "Epoch 90/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2988 - acc: 0.9169 - val_loss: 0.2906 - val_acc: 0.9191\n",
            "Epoch 91/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2984 - acc: 0.9168 - val_loss: 0.2903 - val_acc: 0.9192\n",
            "Epoch 92/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2981 - acc: 0.9170 - val_loss: 0.2901 - val_acc: 0.9196\n",
            "Epoch 93/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2977 - acc: 0.9171 - val_loss: 0.2898 - val_acc: 0.9195\n",
            "Epoch 94/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.2973 - acc: 0.9172 - val_loss: 0.2895 - val_acc: 0.9196\n",
            "Epoch 95/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2970 - acc: 0.9174 - val_loss: 0.2894 - val_acc: 0.9196\n",
            "Epoch 96/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.2967 - acc: 0.9174 - val_loss: 0.2891 - val_acc: 0.9198\n",
            "Epoch 97/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.2963 - acc: 0.9176 - val_loss: 0.2889 - val_acc: 0.9197\n",
            "Epoch 98/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.2960 - acc: 0.9174 - val_loss: 0.2886 - val_acc: 0.9202\n",
            "Epoch 99/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.2957 - acc: 0.9176 - val_loss: 0.2884 - val_acc: 0.9202\n",
            "Epoch 100/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.2953 - acc: 0.9178 - val_loss: 0.2882 - val_acc: 0.9200\n",
            "Epoch 101/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.2950 - acc: 0.9179 - val_loss: 0.2879 - val_acc: 0.9201\n",
            "Epoch 102/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.2947 - acc: 0.9180 - val_loss: 0.2877 - val_acc: 0.9204\n",
            "Epoch 103/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2944 - acc: 0.9180 - val_loss: 0.2875 - val_acc: 0.9202\n",
            "Epoch 104/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2941 - acc: 0.9184 - val_loss: 0.2873 - val_acc: 0.9202\n",
            "Epoch 105/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.2938 - acc: 0.9183 - val_loss: 0.2871 - val_acc: 0.9206\n",
            "Epoch 106/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.2935 - acc: 0.9183 - val_loss: 0.2868 - val_acc: 0.9202\n",
            "Epoch 107/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.2932 - acc: 0.9186 - val_loss: 0.2867 - val_acc: 0.9206\n",
            "Epoch 108/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.2929 - acc: 0.9185 - val_loss: 0.2864 - val_acc: 0.9208\n",
            "Epoch 109/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.2927 - acc: 0.9185 - val_loss: 0.2863 - val_acc: 0.9206\n",
            "Epoch 110/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.2923 - acc: 0.9187 - val_loss: 0.2860 - val_acc: 0.9204\n",
            "Epoch 111/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.2921 - acc: 0.9184 - val_loss: 0.2858 - val_acc: 0.9210\n",
            "Epoch 112/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.2918 - acc: 0.9187 - val_loss: 0.2857 - val_acc: 0.9207\n",
            "Epoch 113/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.2915 - acc: 0.9189 - val_loss: 0.2854 - val_acc: 0.9210\n",
            "Epoch 114/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.2913 - acc: 0.9188 - val_loss: 0.2853 - val_acc: 0.9211\n",
            "Epoch 115/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.2910 - acc: 0.9189 - val_loss: 0.2852 - val_acc: 0.9205\n",
            "Epoch 116/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.2908 - acc: 0.9189 - val_loss: 0.2849 - val_acc: 0.9213\n",
            "Epoch 117/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.2905 - acc: 0.9193 - val_loss: 0.2847 - val_acc: 0.9213\n",
            "Epoch 118/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.2902 - acc: 0.9192 - val_loss: 0.2846 - val_acc: 0.9212\n",
            "Epoch 119/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2900 - acc: 0.9191 - val_loss: 0.2844 - val_acc: 0.9212\n",
            "Epoch 120/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.2898 - acc: 0.9192 - val_loss: 0.2842 - val_acc: 0.9212\n",
            "Epoch 121/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.2895 - acc: 0.9191 - val_loss: 0.2841 - val_acc: 0.9212\n",
            "Epoch 122/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.2892 - acc: 0.9192 - val_loss: 0.2840 - val_acc: 0.9212\n",
            "Epoch 123/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.2890 - acc: 0.9194 - val_loss: 0.2838 - val_acc: 0.9211\n",
            "Epoch 124/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.2888 - acc: 0.9197 - val_loss: 0.2837 - val_acc: 0.9210\n",
            "Epoch 125/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.2885 - acc: 0.9193 - val_loss: 0.2835 - val_acc: 0.9207\n",
            "Epoch 126/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.2883 - acc: 0.9197 - val_loss: 0.2834 - val_acc: 0.9217\n",
            "Epoch 127/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.2881 - acc: 0.9194 - val_loss: 0.2832 - val_acc: 0.9212\n",
            "Epoch 128/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2879 - acc: 0.9194 - val_loss: 0.2830 - val_acc: 0.9210\n",
            "Epoch 129/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2876 - acc: 0.9196 - val_loss: 0.2828 - val_acc: 0.9217\n",
            "Epoch 130/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2874 - acc: 0.9197 - val_loss: 0.2826 - val_acc: 0.9216\n",
            "Epoch 131/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2871 - acc: 0.9200 - val_loss: 0.2827 - val_acc: 0.9211\n",
            "Epoch 132/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.2870 - acc: 0.9197 - val_loss: 0.2824 - val_acc: 0.9213\n",
            "Epoch 133/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.2868 - acc: 0.9198 - val_loss: 0.2823 - val_acc: 0.9216\n",
            "Epoch 134/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2866 - acc: 0.9199 - val_loss: 0.2822 - val_acc: 0.9214\n",
            "Epoch 135/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2863 - acc: 0.9203 - val_loss: 0.2820 - val_acc: 0.9213\n",
            "Epoch 136/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2861 - acc: 0.9196 - val_loss: 0.2818 - val_acc: 0.9215\n",
            "Epoch 137/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2859 - acc: 0.9198 - val_loss: 0.2818 - val_acc: 0.9217\n",
            "Epoch 138/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.2857 - acc: 0.9203 - val_loss: 0.2815 - val_acc: 0.9218\n",
            "Epoch 139/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2855 - acc: 0.9203 - val_loss: 0.2814 - val_acc: 0.9215\n",
            "Epoch 140/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2853 - acc: 0.9201 - val_loss: 0.2812 - val_acc: 0.9216\n",
            "Epoch 141/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2852 - acc: 0.9204 - val_loss: 0.2811 - val_acc: 0.9217\n",
            "Epoch 142/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2849 - acc: 0.9201 - val_loss: 0.2810 - val_acc: 0.9217\n",
            "Epoch 143/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2848 - acc: 0.9205 - val_loss: 0.2809 - val_acc: 0.9219\n",
            "Epoch 144/200\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.2846 - acc: 0.9208 - val_loss: 0.2808 - val_acc: 0.9217\n",
            "Epoch 145/200\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.2844 - acc: 0.9207 - val_loss: 0.2806 - val_acc: 0.9221\n",
            "Epoch 146/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.2841 - acc: 0.9206 - val_loss: 0.2806 - val_acc: 0.9220\n",
            "Epoch 147/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2840 - acc: 0.9207 - val_loss: 0.2804 - val_acc: 0.9217\n",
            "Epoch 148/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2838 - acc: 0.9209 - val_loss: 0.2803 - val_acc: 0.9218\n",
            "Epoch 149/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2836 - acc: 0.9208 - val_loss: 0.2802 - val_acc: 0.9216\n",
            "Epoch 150/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2835 - acc: 0.9210 - val_loss: 0.2800 - val_acc: 0.9225\n",
            "Epoch 151/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2833 - acc: 0.9210 - val_loss: 0.2799 - val_acc: 0.9226\n",
            "Epoch 152/200\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.2831 - acc: 0.9211 - val_loss: 0.2798 - val_acc: 0.9222\n",
            "Epoch 153/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.2829 - acc: 0.9207 - val_loss: 0.2797 - val_acc: 0.9224\n",
            "Epoch 154/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.2827 - acc: 0.9209 - val_loss: 0.2796 - val_acc: 0.9222\n",
            "Epoch 155/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.2826 - acc: 0.9208 - val_loss: 0.2795 - val_acc: 0.9225\n",
            "Epoch 156/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.2824 - acc: 0.9210 - val_loss: 0.2794 - val_acc: 0.9224\n",
            "Epoch 157/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2822 - acc: 0.9210 - val_loss: 0.2793 - val_acc: 0.9224\n",
            "Epoch 158/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2821 - acc: 0.9214 - val_loss: 0.2792 - val_acc: 0.9226\n",
            "Epoch 159/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.2819 - acc: 0.9214 - val_loss: 0.2791 - val_acc: 0.9226\n",
            "Epoch 160/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2817 - acc: 0.9213 - val_loss: 0.2790 - val_acc: 0.9225\n",
            "Epoch 161/200\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.2816 - acc: 0.9214 - val_loss: 0.2789 - val_acc: 0.9222\n",
            "Epoch 162/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2814 - acc: 0.9215 - val_loss: 0.2788 - val_acc: 0.9227\n",
            "Epoch 163/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2812 - acc: 0.9213 - val_loss: 0.2787 - val_acc: 0.9225\n",
            "Epoch 164/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2811 - acc: 0.9216 - val_loss: 0.2786 - val_acc: 0.9225\n",
            "Epoch 165/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.2809 - acc: 0.9215 - val_loss: 0.2785 - val_acc: 0.9227\n",
            "Epoch 166/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2807 - acc: 0.9216 - val_loss: 0.2784 - val_acc: 0.9225\n",
            "Epoch 167/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2806 - acc: 0.9217 - val_loss: 0.2784 - val_acc: 0.9227\n",
            "Epoch 168/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2804 - acc: 0.9219 - val_loss: 0.2782 - val_acc: 0.9228\n",
            "Epoch 169/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2803 - acc: 0.9216 - val_loss: 0.2782 - val_acc: 0.9227\n",
            "Epoch 170/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.2801 - acc: 0.9216 - val_loss: 0.2781 - val_acc: 0.9227\n",
            "Epoch 171/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.2800 - acc: 0.9220 - val_loss: 0.2780 - val_acc: 0.9226\n",
            "Epoch 172/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.2798 - acc: 0.9218 - val_loss: 0.2778 - val_acc: 0.9231\n",
            "Epoch 173/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.2797 - acc: 0.9217 - val_loss: 0.2778 - val_acc: 0.9229\n",
            "Epoch 174/200\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.2796 - acc: 0.9217 - val_loss: 0.2777 - val_acc: 0.9227\n",
            "Epoch 175/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.2794 - acc: 0.9218 - val_loss: 0.2776 - val_acc: 0.9232\n",
            "Epoch 176/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.2793 - acc: 0.9220 - val_loss: 0.2775 - val_acc: 0.9232\n",
            "Epoch 177/200\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.2791 - acc: 0.9219 - val_loss: 0.2774 - val_acc: 0.9234\n",
            "Epoch 178/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2790 - acc: 0.9221 - val_loss: 0.2774 - val_acc: 0.9228\n",
            "Epoch 179/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2788 - acc: 0.9221 - val_loss: 0.2773 - val_acc: 0.9232\n",
            "Epoch 180/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2787 - acc: 0.9221 - val_loss: 0.2771 - val_acc: 0.9235\n",
            "Epoch 181/200\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.2785 - acc: 0.9223 - val_loss: 0.2770 - val_acc: 0.9232\n",
            "Epoch 182/200\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.2784 - acc: 0.9220 - val_loss: 0.2769 - val_acc: 0.9231\n",
            "Epoch 183/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.2783 - acc: 0.9223 - val_loss: 0.2769 - val_acc: 0.9231\n",
            "Epoch 184/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2781 - acc: 0.9223 - val_loss: 0.2768 - val_acc: 0.9230\n",
            "Epoch 185/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.2780 - acc: 0.9224 - val_loss: 0.2767 - val_acc: 0.9233\n",
            "Epoch 186/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.2779 - acc: 0.9223 - val_loss: 0.2766 - val_acc: 0.9236\n",
            "Epoch 187/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.2777 - acc: 0.9224 - val_loss: 0.2766 - val_acc: 0.9233\n",
            "Epoch 188/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2776 - acc: 0.9226 - val_loss: 0.2765 - val_acc: 0.9236\n",
            "Epoch 189/200\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.2775 - acc: 0.9225 - val_loss: 0.2764 - val_acc: 0.9235\n",
            "Epoch 190/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2773 - acc: 0.9225 - val_loss: 0.2764 - val_acc: 0.9235\n",
            "Epoch 191/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.2772 - acc: 0.9225 - val_loss: 0.2763 - val_acc: 0.9237\n",
            "Epoch 192/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2770 - acc: 0.9226 - val_loss: 0.2762 - val_acc: 0.9238\n",
            "Epoch 193/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.2770 - acc: 0.9226 - val_loss: 0.2761 - val_acc: 0.9237\n",
            "Epoch 194/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2768 - acc: 0.9226 - val_loss: 0.2761 - val_acc: 0.9236\n",
            "Epoch 195/200\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2767 - acc: 0.9231 - val_loss: 0.2760 - val_acc: 0.9239\n",
            "Epoch 196/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.2766 - acc: 0.9226 - val_loss: 0.2758 - val_acc: 0.9241\n",
            "Epoch 197/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.2765 - acc: 0.9229 - val_loss: 0.2758 - val_acc: 0.9242\n",
            "Epoch 198/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.2763 - acc: 0.9231 - val_loss: 0.2758 - val_acc: 0.9236\n",
            "Epoch 199/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.2762 - acc: 0.9229 - val_loss: 0.2757 - val_acc: 0.9241\n",
            "Epoch 200/200\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.2761 - acc: 0.9230 - val_loss: 0.2756 - val_acc: 0.9241\n",
            "10000/10000 [==============================] - 0s 49us/step\n",
            "Test score:  0.27738585557043555\n",
            "Test accuracy:  0.9227\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TY2E6uboDXUx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 隠れ層の追加による精度向上\n",
        "- 隠れ層を２つ追加\n",
        "- エポック数を200 => 20にしても精度向上"
      ]
    },
    {
      "metadata": {
        "id": "iTx2ViSwEGGv",
        "colab_type": "code",
        "outputId": "f01f06f9-910f-4f44-fca2-aa83b2c2dfaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1122
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.optimizers import SGD\n",
        "from keras.utils import np_utils \n",
        "\n",
        "# 再現性のためのシード\n",
        "np.random.seed(1671)\n",
        "\n",
        "# ハイパーパラメータ\n",
        "NB_EPOCH = 20\n",
        "BATCH_SIZE = 128\n",
        "VERBOSE = 1\n",
        "NB_CLASSES = 10\n",
        "OPTIMIZER = SGD()\n",
        "N_HIDDEN = 128\n",
        "VALIDATION_SPLIT = 0.2\n",
        "\n",
        "# --------\n",
        "#データの前処理\n",
        "# --------\n",
        "\n",
        "# データの読み込み\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# 28x28の画像のreshape\n",
        "RESHAPED = 784\n",
        "\n",
        "X_train = X_train.reshape(60000, RESHAPED)\n",
        "X_test = X_test.reshape(10000, RESHAPED)\n",
        "\n",
        "# GPU計算のためにfloat32型に変換\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "# 正規化\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')\n",
        "\n",
        "# ラベルをone-hotに変換\n",
        "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
        "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
        "\n",
        "# -------------\n",
        "# モデル構築\n",
        "# -------------\n",
        "model = Sequential()\n",
        "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED, )))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(N_HIDDEN))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(NB_CLASSES))\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, \n",
        "             metrics=['accuracy'])\n",
        "\n",
        "# -------------------------\n",
        "# Tensor Boardの設定\n",
        "# -------------------------\n",
        "\n",
        "import os \n",
        "from time import gmtime, strftime\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "def make_tensorboard(set_dir_name=''):\n",
        "  tictoc = strftime(\"%a_ %d_ %b_ &Y %H_ %M_ %S\", gmtime())\n",
        "  directory_name = tictoc\n",
        "  log_dir = set_dir_name + '_' + directory_name\n",
        "  os.mkdir(log_dir)\n",
        "  tensorboard = TensorBoard(log_dir=log_dir)\n",
        "  return tensorboard\n",
        "\n",
        "callbacks = [make_tensorboard(set_dir_name='drive/My Drive/Colab Notebooks/chokkan_deeplearning/keras_MNIST_V2')]\n",
        "\n",
        "# -----\n",
        "# 学習\n",
        "# -----\n",
        "model.fit(X_train, Y_train, \n",
        "         batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
        "         callbacks=callbacks,\n",
        "         verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
        "\n",
        "# ------\n",
        "# 評価\n",
        "# ------\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
        "print(\"Test score: \", score[0])\n",
        "print(\"Test accuracy: \", score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000 train samples\n",
            "10000 test samples\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_11 (Dense)             (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 10)                1290      \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 118,282\n",
            "Trainable params: 118,282\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/20\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 1.4829 - acc: 0.6231 - val_loss: 0.7584 - val_acc: 0.8286\n",
            "Epoch 2/20\n",
            "48000/48000 [==============================] - 2s 41us/step - loss: 0.6049 - acc: 0.8464 - val_loss: 0.4550 - val_acc: 0.8853\n",
            "Epoch 3/20\n",
            "48000/48000 [==============================] - 2s 41us/step - loss: 0.4398 - acc: 0.8801 - val_loss: 0.3710 - val_acc: 0.9019\n",
            "Epoch 4/20\n",
            "48000/48000 [==============================] - 2s 41us/step - loss: 0.3767 - acc: 0.8952 - val_loss: 0.3322 - val_acc: 0.9082\n",
            "Epoch 5/20\n",
            "48000/48000 [==============================] - 2s 41us/step - loss: 0.3415 - acc: 0.9025 - val_loss: 0.3055 - val_acc: 0.9147\n",
            "Epoch 6/20\n",
            "48000/48000 [==============================] - 2s 40us/step - loss: 0.3175 - acc: 0.9085 - val_loss: 0.2880 - val_acc: 0.9182\n",
            "Epoch 7/20\n",
            "48000/48000 [==============================] - 2s 41us/step - loss: 0.2989 - acc: 0.9137 - val_loss: 0.2727 - val_acc: 0.9223\n",
            "Epoch 8/20\n",
            "48000/48000 [==============================] - 2s 42us/step - loss: 0.2839 - acc: 0.9180 - val_loss: 0.2607 - val_acc: 0.9266\n",
            "Epoch 9/20\n",
            "48000/48000 [==============================] - 2s 40us/step - loss: 0.2714 - acc: 0.9218 - val_loss: 0.2504 - val_acc: 0.9298\n",
            "Epoch 10/20\n",
            "48000/48000 [==============================] - 2s 40us/step - loss: 0.2602 - acc: 0.9253 - val_loss: 0.2430 - val_acc: 0.9308\n",
            "Epoch 11/20\n",
            "48000/48000 [==============================] - 2s 40us/step - loss: 0.2501 - acc: 0.9285 - val_loss: 0.2341 - val_acc: 0.9333\n",
            "Epoch 12/20\n",
            "48000/48000 [==============================] - 2s 41us/step - loss: 0.2409 - acc: 0.9301 - val_loss: 0.2271 - val_acc: 0.9353\n",
            "Epoch 13/20\n",
            "48000/48000 [==============================] - 2s 39us/step - loss: 0.2325 - acc: 0.9334 - val_loss: 0.2227 - val_acc: 0.9365\n",
            "Epoch 14/20\n",
            "48000/48000 [==============================] - 2s 39us/step - loss: 0.2253 - acc: 0.9353 - val_loss: 0.2147 - val_acc: 0.9396\n",
            "Epoch 15/20\n",
            "48000/48000 [==============================] - 2s 40us/step - loss: 0.2181 - acc: 0.9375 - val_loss: 0.2082 - val_acc: 0.9411\n",
            "Epoch 16/20\n",
            "48000/48000 [==============================] - 2s 40us/step - loss: 0.2116 - acc: 0.9393 - val_loss: 0.2030 - val_acc: 0.9430\n",
            "Epoch 17/20\n",
            "48000/48000 [==============================] - 2s 41us/step - loss: 0.2055 - acc: 0.9414 - val_loss: 0.1981 - val_acc: 0.9444\n",
            "Epoch 18/20\n",
            "48000/48000 [==============================] - 2s 40us/step - loss: 0.1996 - acc: 0.9430 - val_loss: 0.1932 - val_acc: 0.9459\n",
            "Epoch 19/20\n",
            "48000/48000 [==============================] - 2s 41us/step - loss: 0.1941 - acc: 0.9432 - val_loss: 0.1894 - val_acc: 0.9467\n",
            "Epoch 20/20\n",
            "48000/48000 [==============================] - 2s 42us/step - loss: 0.1890 - acc: 0.9456 - val_loss: 0.1850 - val_acc: 0.9497\n",
            "10000/10000 [==============================] - 1s 53us/step\n",
            "Test score:  0.1860071842327714\n",
            "Test accuracy:  0.9463\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZDFp8eEWHwPQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dropoutによる精度向上"
      ]
    },
    {
      "metadata": {
        "id": "wcvy8ws6Eyq-",
        "colab_type": "code",
        "outputId": "dae65b08-668b-4e9e-8f82-b761a14d2bf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1190
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.optimizers import SGD\n",
        "from keras.utils import np_utils \n",
        "\n",
        "# 再現性のためのシード\n",
        "np.random.seed(1671)\n",
        "\n",
        "# ハイパーパラメータ\n",
        "NB_EPOCH = 20\n",
        "BATCH_SIZE = 128\n",
        "VERBOSE = 1\n",
        "NB_CLASSES = 10\n",
        "OPTIMIZER = SGD()\n",
        "N_HIDDEN = 128\n",
        "VALIDATION_SPLIT = 0.2\n",
        "DROPOUT = 0.3\n",
        "\n",
        "# --------\n",
        "#データの前処理\n",
        "# --------\n",
        "\n",
        "# データの読み込み\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# 28x28の画像のreshape\n",
        "RESHAPED = 784\n",
        "\n",
        "X_train = X_train.reshape(60000, RESHAPED)\n",
        "X_test = X_test.reshape(10000, RESHAPED)\n",
        "\n",
        "# GPU計算のためにfloat32型に変換\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "# 正規化\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')\n",
        "\n",
        "# ラベルをone-hotに変換\n",
        "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
        "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
        "\n",
        "# -------------\n",
        "# モデル構築\n",
        "# -------------\n",
        "model = Sequential()\n",
        "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED, )))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(DROPOUT))\n",
        "model.add(Dense(N_HIDDEN))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(DROPOUT))\n",
        "model.add(Dense(NB_CLASSES))\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, \n",
        "             metrics=['accuracy'])\n",
        "\n",
        "# -------------------------\n",
        "# Tensor Boardの設定\n",
        "# -------------------------\n",
        "\n",
        "import os \n",
        "from time import gmtime, strftime\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "def make_tensorboard(set_dir_name=''):\n",
        "  tictoc = strftime(\"%a_ %d_ %b_ &Y %H_ %M_ %S\", gmtime())\n",
        "  directory_name = tictoc\n",
        "  log_dir = set_dir_name + '_' + directory_name\n",
        "  os.mkdir(log_dir)\n",
        "  tensorboard = TensorBoard(log_dir=log_dir)\n",
        "  return tensorboard\n",
        "\n",
        "callbacks = [make_tensorboard(set_dir_name='drive/My Drive/Colab Notebooks/chokkan_deeplearning/keras_MNIST_V3')]\n",
        "\n",
        "# -----\n",
        "# 学習\n",
        "# -----\n",
        "model.fit(X_train, Y_train, \n",
        "         batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
        "         callbacks=callbacks,\n",
        "         verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
        "\n",
        "# ------\n",
        "# 評価\n",
        "# ------\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
        "print(\"Test score: \", score[0])\n",
        "print(\"Test accuracy: \", score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000 train samples\n",
            "10000 test samples\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_15 (Dense)             (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 10)                1290      \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 118,282\n",
            "Trainable params: 118,282\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/20\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 1.7404 - acc: 0.4539 - val_loss: 0.9293 - val_acc: 0.8124\n",
            "Epoch 2/20\n",
            "48000/48000 [==============================] - 2s 43us/step - loss: 0.9232 - acc: 0.7229 - val_loss: 0.5400 - val_acc: 0.8652\n",
            "Epoch 3/20\n",
            "48000/48000 [==============================] - 2s 43us/step - loss: 0.6935 - acc: 0.7881 - val_loss: 0.4298 - val_acc: 0.8883\n",
            "Epoch 4/20\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.5947 - acc: 0.8209 - val_loss: 0.3790 - val_acc: 0.8977\n",
            "Epoch 5/20\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.5347 - acc: 0.8393 - val_loss: 0.3456 - val_acc: 0.9039\n",
            "Epoch 6/20\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.4976 - acc: 0.8524 - val_loss: 0.3232 - val_acc: 0.9107\n",
            "Epoch 7/20\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.4616 - acc: 0.8628 - val_loss: 0.3048 - val_acc: 0.9127\n",
            "Epoch 8/20\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.4386 - acc: 0.8688 - val_loss: 0.2896 - val_acc: 0.9172\n",
            "Epoch 9/20\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.4181 - acc: 0.8761 - val_loss: 0.2776 - val_acc: 0.9198\n",
            "Epoch 10/20\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.3990 - acc: 0.8838 - val_loss: 0.2657 - val_acc: 0.9234\n",
            "Epoch 11/20\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.3819 - acc: 0.8876 - val_loss: 0.2551 - val_acc: 0.9257\n",
            "Epoch 12/20\n",
            "48000/48000 [==============================] - 2s 47us/step - loss: 0.3688 - acc: 0.8921 - val_loss: 0.2465 - val_acc: 0.9283\n",
            "Epoch 13/20\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.3571 - acc: 0.8943 - val_loss: 0.2388 - val_acc: 0.9299\n",
            "Epoch 14/20\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.3466 - acc: 0.8991 - val_loss: 0.2320 - val_acc: 0.9322\n",
            "Epoch 15/20\n",
            "48000/48000 [==============================] - 2s 47us/step - loss: 0.3359 - acc: 0.9014 - val_loss: 0.2261 - val_acc: 0.9339\n",
            "Epoch 16/20\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.3244 - acc: 0.9055 - val_loss: 0.2180 - val_acc: 0.9352\n",
            "Epoch 17/20\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.3142 - acc: 0.9084 - val_loss: 0.2121 - val_acc: 0.9376\n",
            "Epoch 18/20\n",
            "48000/48000 [==============================] - 2s 43us/step - loss: 0.3103 - acc: 0.9094 - val_loss: 0.2076 - val_acc: 0.9388\n",
            "Epoch 19/20\n",
            "48000/48000 [==============================] - 2s 47us/step - loss: 0.3019 - acc: 0.9117 - val_loss: 0.2018 - val_acc: 0.9408\n",
            "Epoch 20/20\n",
            "48000/48000 [==============================] - 2s 49us/step - loss: 0.2931 - acc: 0.9130 - val_loss: 0.1974 - val_acc: 0.9419\n",
            "10000/10000 [==============================] - 1s 56us/step\n",
            "Test score:  0.19945166353732346\n",
            "Test accuracy:  0.9402\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TdUtP3gWJdtf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "学習回数を大幅に増やす"
      ]
    },
    {
      "metadata": {
        "id": "HGnjVzgWIQ7_",
        "colab_type": "code",
        "outputId": "daa352f5-cafa-4f10-b213-3bcdd25d2622",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8976
        }
      },
      "cell_type": "code",
      "source": [
        "NB_EPOCH = 250\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED, )))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(DROPOUT))\n",
        "model.add(Dense(N_HIDDEN))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(DROPOUT))\n",
        "model.add(Dense(NB_CLASSES))\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, \n",
        "             metrics=['accuracy'])\n",
        "# -----\n",
        "# 学習\n",
        "# -----\n",
        "model.fit(X_train, Y_train, \n",
        "         batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
        "         callbacks=callbacks,\n",
        "         verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
        "\n",
        "# ------\n",
        "# 評価\n",
        "# ------\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
        "print(\"Test score: \", score[0])\n",
        "print(\"Test accuracy: \", score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_18 (Dense)             (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "activation_20 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 10)                1290      \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 118,282\n",
            "Trainable params: 118,282\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/250\n",
            "48000/48000 [==============================] - 2s 48us/step - loss: 1.7465 - acc: 0.4476 - val_loss: 0.9261 - val_acc: 0.8136\n",
            "Epoch 2/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.9368 - acc: 0.7167 - val_loss: 0.5379 - val_acc: 0.8703\n",
            "Epoch 3/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.7003 - acc: 0.7884 - val_loss: 0.4288 - val_acc: 0.8882\n",
            "Epoch 4/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.5956 - acc: 0.8215 - val_loss: 0.3734 - val_acc: 0.8988\n",
            "Epoch 5/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.5376 - acc: 0.8381 - val_loss: 0.3425 - val_acc: 0.9050\n",
            "Epoch 6/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.4924 - acc: 0.8551 - val_loss: 0.3176 - val_acc: 0.9109\n",
            "Epoch 7/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.4619 - acc: 0.8644 - val_loss: 0.3002 - val_acc: 0.9143\n",
            "Epoch 8/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.4325 - acc: 0.8724 - val_loss: 0.2845 - val_acc: 0.9181\n",
            "Epoch 9/250\n",
            "48000/48000 [==============================] - 2s 47us/step - loss: 0.4145 - acc: 0.8788 - val_loss: 0.2733 - val_acc: 0.9213\n",
            "Epoch 10/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.3964 - acc: 0.8835 - val_loss: 0.2630 - val_acc: 0.9240\n",
            "Epoch 11/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.3817 - acc: 0.8881 - val_loss: 0.2530 - val_acc: 0.9272\n",
            "Epoch 12/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.3649 - acc: 0.8941 - val_loss: 0.2437 - val_acc: 0.9299\n",
            "Epoch 13/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.3553 - acc: 0.8961 - val_loss: 0.2364 - val_acc: 0.9320\n",
            "Epoch 14/250\n",
            "48000/48000 [==============================] - 2s 49us/step - loss: 0.3429 - acc: 0.8995 - val_loss: 0.2290 - val_acc: 0.9340\n",
            "Epoch 15/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.3331 - acc: 0.9018 - val_loss: 0.2233 - val_acc: 0.9356\n",
            "Epoch 16/250\n",
            "48000/48000 [==============================] - 2s 43us/step - loss: 0.3210 - acc: 0.9062 - val_loss: 0.2179 - val_acc: 0.9374\n",
            "Epoch 17/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.3157 - acc: 0.9078 - val_loss: 0.2119 - val_acc: 0.9387\n",
            "Epoch 18/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.3057 - acc: 0.9102 - val_loss: 0.2063 - val_acc: 0.9408\n",
            "Epoch 19/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.3020 - acc: 0.9126 - val_loss: 0.2016 - val_acc: 0.9417\n",
            "Epoch 20/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.2933 - acc: 0.9133 - val_loss: 0.1975 - val_acc: 0.9432\n",
            "Epoch 21/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.2833 - acc: 0.9161 - val_loss: 0.1925 - val_acc: 0.9443\n",
            "Epoch 22/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.2795 - acc: 0.9184 - val_loss: 0.1885 - val_acc: 0.9466\n",
            "Epoch 23/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.2713 - acc: 0.9195 - val_loss: 0.1847 - val_acc: 0.9472\n",
            "Epoch 24/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.2695 - acc: 0.9214 - val_loss: 0.1808 - val_acc: 0.9487\n",
            "Epoch 25/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.2581 - acc: 0.9248 - val_loss: 0.1778 - val_acc: 0.9492\n",
            "Epoch 26/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.2571 - acc: 0.9225 - val_loss: 0.1745 - val_acc: 0.9503\n",
            "Epoch 27/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.2544 - acc: 0.9246 - val_loss: 0.1711 - val_acc: 0.9515\n",
            "Epoch 28/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.2484 - acc: 0.9269 - val_loss: 0.1686 - val_acc: 0.9523\n",
            "Epoch 29/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.2435 - acc: 0.9284 - val_loss: 0.1656 - val_acc: 0.9528\n",
            "Epoch 30/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.2390 - acc: 0.9292 - val_loss: 0.1631 - val_acc: 0.9539\n",
            "Epoch 31/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.2366 - acc: 0.9299 - val_loss: 0.1603 - val_acc: 0.9545\n",
            "Epoch 32/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.2311 - acc: 0.9326 - val_loss: 0.1579 - val_acc: 0.9557\n",
            "Epoch 33/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.2301 - acc: 0.9319 - val_loss: 0.1563 - val_acc: 0.9554\n",
            "Epoch 34/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.2246 - acc: 0.9354 - val_loss: 0.1538 - val_acc: 0.9562\n",
            "Epoch 35/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.2201 - acc: 0.9348 - val_loss: 0.1516 - val_acc: 0.9573\n",
            "Epoch 36/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.2175 - acc: 0.9377 - val_loss: 0.1493 - val_acc: 0.9583\n",
            "Epoch 37/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.2181 - acc: 0.9354 - val_loss: 0.1473 - val_acc: 0.9581\n",
            "Epoch 38/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.2113 - acc: 0.9385 - val_loss: 0.1449 - val_acc: 0.9598\n",
            "Epoch 39/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.2124 - acc: 0.9382 - val_loss: 0.1437 - val_acc: 0.9600\n",
            "Epoch 40/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.2045 - acc: 0.9399 - val_loss: 0.1421 - val_acc: 0.9597\n",
            "Epoch 41/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.2002 - acc: 0.9410 - val_loss: 0.1407 - val_acc: 0.9607\n",
            "Epoch 42/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.1998 - acc: 0.9419 - val_loss: 0.1388 - val_acc: 0.9609\n",
            "Epoch 43/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.1997 - acc: 0.9412 - val_loss: 0.1372 - val_acc: 0.9618\n",
            "Epoch 44/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.1963 - acc: 0.9425 - val_loss: 0.1355 - val_acc: 0.9627\n",
            "Epoch 45/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.1913 - acc: 0.9438 - val_loss: 0.1341 - val_acc: 0.9624\n",
            "Epoch 46/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.1917 - acc: 0.9432 - val_loss: 0.1331 - val_acc: 0.9621\n",
            "Epoch 47/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.1869 - acc: 0.9443 - val_loss: 0.1311 - val_acc: 0.9625\n",
            "Epoch 48/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.1865 - acc: 0.9453 - val_loss: 0.1298 - val_acc: 0.9631\n",
            "Epoch 49/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.1858 - acc: 0.9445 - val_loss: 0.1288 - val_acc: 0.9632\n",
            "Epoch 50/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.1842 - acc: 0.9462 - val_loss: 0.1283 - val_acc: 0.9637\n",
            "Epoch 51/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.1779 - acc: 0.9479 - val_loss: 0.1269 - val_acc: 0.9641\n",
            "Epoch 52/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.1820 - acc: 0.9457 - val_loss: 0.1257 - val_acc: 0.9646\n",
            "Epoch 53/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.1771 - acc: 0.9482 - val_loss: 0.1238 - val_acc: 0.9652\n",
            "Epoch 54/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.1725 - acc: 0.9486 - val_loss: 0.1229 - val_acc: 0.9650\n",
            "Epoch 55/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.1741 - acc: 0.9490 - val_loss: 0.1221 - val_acc: 0.9654\n",
            "Epoch 56/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.1715 - acc: 0.9487 - val_loss: 0.1209 - val_acc: 0.9657\n",
            "Epoch 57/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.1704 - acc: 0.9501 - val_loss: 0.1206 - val_acc: 0.9657\n",
            "Epoch 58/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.1684 - acc: 0.9506 - val_loss: 0.1197 - val_acc: 0.9653\n",
            "Epoch 59/250\n",
            "48000/48000 [==============================] - 2s 48us/step - loss: 0.1681 - acc: 0.9503 - val_loss: 0.1186 - val_acc: 0.9656\n",
            "Epoch 60/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.1632 - acc: 0.9519 - val_loss: 0.1173 - val_acc: 0.9663\n",
            "Epoch 61/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.1636 - acc: 0.9511 - val_loss: 0.1165 - val_acc: 0.9662\n",
            "Epoch 62/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.1640 - acc: 0.9522 - val_loss: 0.1153 - val_acc: 0.9671\n",
            "Epoch 63/250\n",
            "48000/48000 [==============================] - 2s 48us/step - loss: 0.1606 - acc: 0.9538 - val_loss: 0.1147 - val_acc: 0.9671\n",
            "Epoch 64/250\n",
            "48000/48000 [==============================] - 2s 47us/step - loss: 0.1562 - acc: 0.9532 - val_loss: 0.1143 - val_acc: 0.9678\n",
            "Epoch 65/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.1596 - acc: 0.9528 - val_loss: 0.1135 - val_acc: 0.9678\n",
            "Epoch 66/250\n",
            "48000/48000 [==============================] - 2s 47us/step - loss: 0.1533 - acc: 0.9551 - val_loss: 0.1124 - val_acc: 0.9682\n",
            "Epoch 67/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.1564 - acc: 0.9541 - val_loss: 0.1118 - val_acc: 0.9686\n",
            "Epoch 68/250\n",
            "48000/48000 [==============================] - 2s 47us/step - loss: 0.1534 - acc: 0.9543 - val_loss: 0.1109 - val_acc: 0.9687\n",
            "Epoch 69/250\n",
            "48000/48000 [==============================] - 2s 48us/step - loss: 0.1528 - acc: 0.9534 - val_loss: 0.1097 - val_acc: 0.9692\n",
            "Epoch 70/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.1500 - acc: 0.9556 - val_loss: 0.1089 - val_acc: 0.9692\n",
            "Epoch 71/250\n",
            "48000/48000 [==============================] - 2s 48us/step - loss: 0.1483 - acc: 0.9562 - val_loss: 0.1088 - val_acc: 0.9692\n",
            "Epoch 72/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.1454 - acc: 0.9563 - val_loss: 0.1081 - val_acc: 0.9695\n",
            "Epoch 73/250\n",
            "48000/48000 [==============================] - 2s 47us/step - loss: 0.1463 - acc: 0.9564 - val_loss: 0.1076 - val_acc: 0.9696\n",
            "Epoch 74/250\n",
            "48000/48000 [==============================] - 2s 47us/step - loss: 0.1435 - acc: 0.9570 - val_loss: 0.1075 - val_acc: 0.9697\n",
            "Epoch 75/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.1441 - acc: 0.9577 - val_loss: 0.1058 - val_acc: 0.9702\n",
            "Epoch 76/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.1421 - acc: 0.9581 - val_loss: 0.1058 - val_acc: 0.9699\n",
            "Epoch 77/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.1424 - acc: 0.9577 - val_loss: 0.1047 - val_acc: 0.9703\n",
            "Epoch 78/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.1421 - acc: 0.9579 - val_loss: 0.1045 - val_acc: 0.9709\n",
            "Epoch 79/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.1376 - acc: 0.9594 - val_loss: 0.1044 - val_acc: 0.9696\n",
            "Epoch 80/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.1371 - acc: 0.9594 - val_loss: 0.1031 - val_acc: 0.9705\n",
            "Epoch 81/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.1371 - acc: 0.9590 - val_loss: 0.1026 - val_acc: 0.9710\n",
            "Epoch 82/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.1370 - acc: 0.9579 - val_loss: 0.1028 - val_acc: 0.9706\n",
            "Epoch 83/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.1367 - acc: 0.9590 - val_loss: 0.1024 - val_acc: 0.9710\n",
            "Epoch 84/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.1357 - acc: 0.9596 - val_loss: 0.1014 - val_acc: 0.9712\n",
            "Epoch 85/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.1334 - acc: 0.9600 - val_loss: 0.1007 - val_acc: 0.9712\n",
            "Epoch 86/250\n",
            "48000/48000 [==============================] - 2s 48us/step - loss: 0.1318 - acc: 0.9605 - val_loss: 0.1012 - val_acc: 0.9715\n",
            "Epoch 87/250\n",
            "48000/48000 [==============================] - 2s 47us/step - loss: 0.1333 - acc: 0.9601 - val_loss: 0.1003 - val_acc: 0.9716\n",
            "Epoch 88/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.1310 - acc: 0.9620 - val_loss: 0.1005 - val_acc: 0.9719\n",
            "Epoch 89/250\n",
            "48000/48000 [==============================] - 2s 47us/step - loss: 0.1309 - acc: 0.9610 - val_loss: 0.0998 - val_acc: 0.9720\n",
            "Epoch 90/250\n",
            "48000/48000 [==============================] - 2s 47us/step - loss: 0.1304 - acc: 0.9605 - val_loss: 0.0998 - val_acc: 0.9718\n",
            "Epoch 91/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.1272 - acc: 0.9620 - val_loss: 0.0984 - val_acc: 0.9724\n",
            "Epoch 92/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.1263 - acc: 0.9625 - val_loss: 0.0984 - val_acc: 0.9720\n",
            "Epoch 93/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.1268 - acc: 0.9627 - val_loss: 0.0978 - val_acc: 0.9722\n",
            "Epoch 94/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.1238 - acc: 0.9624 - val_loss: 0.0974 - val_acc: 0.9726\n",
            "Epoch 95/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.1236 - acc: 0.9628 - val_loss: 0.0964 - val_acc: 0.9732\n",
            "Epoch 96/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.1233 - acc: 0.9630 - val_loss: 0.0963 - val_acc: 0.9732\n",
            "Epoch 97/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.1223 - acc: 0.9634 - val_loss: 0.0963 - val_acc: 0.9731\n",
            "Epoch 98/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.1211 - acc: 0.9642 - val_loss: 0.0958 - val_acc: 0.9732\n",
            "Epoch 99/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.1187 - acc: 0.9643 - val_loss: 0.0957 - val_acc: 0.9731\n",
            "Epoch 100/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.1214 - acc: 0.9641 - val_loss: 0.0955 - val_acc: 0.9733\n",
            "Epoch 101/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.1195 - acc: 0.9645 - val_loss: 0.0950 - val_acc: 0.9732\n",
            "Epoch 102/250\n",
            "48000/48000 [==============================] - 2s 48us/step - loss: 0.1188 - acc: 0.9647 - val_loss: 0.0944 - val_acc: 0.9733\n",
            "Epoch 103/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.1179 - acc: 0.9640 - val_loss: 0.0944 - val_acc: 0.9733\n",
            "Epoch 104/250\n",
            "48000/48000 [==============================] - 2s 47us/step - loss: 0.1172 - acc: 0.9646 - val_loss: 0.0937 - val_acc: 0.9735\n",
            "Epoch 105/250\n",
            "48000/48000 [==============================] - 2s 49us/step - loss: 0.1169 - acc: 0.9644 - val_loss: 0.0933 - val_acc: 0.9734\n",
            "Epoch 106/250\n",
            "48000/48000 [==============================] - 2s 49us/step - loss: 0.1159 - acc: 0.9657 - val_loss: 0.0928 - val_acc: 0.9737\n",
            "Epoch 107/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.1146 - acc: 0.9661 - val_loss: 0.0930 - val_acc: 0.9742\n",
            "Epoch 108/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.1147 - acc: 0.9658 - val_loss: 0.0930 - val_acc: 0.9738\n",
            "Epoch 109/250\n",
            "48000/48000 [==============================] - 2s 47us/step - loss: 0.1135 - acc: 0.9658 - val_loss: 0.0923 - val_acc: 0.9742\n",
            "Epoch 110/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.1125 - acc: 0.9667 - val_loss: 0.0921 - val_acc: 0.9737\n",
            "Epoch 111/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.1131 - acc: 0.9658 - val_loss: 0.0919 - val_acc: 0.9735\n",
            "Epoch 112/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.1109 - acc: 0.9671 - val_loss: 0.0920 - val_acc: 0.9733\n",
            "Epoch 113/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.1106 - acc: 0.9669 - val_loss: 0.0920 - val_acc: 0.9734\n",
            "Epoch 114/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.1101 - acc: 0.9660 - val_loss: 0.0910 - val_acc: 0.9733\n",
            "Epoch 115/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.1077 - acc: 0.9675 - val_loss: 0.0906 - val_acc: 0.9739\n",
            "Epoch 116/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.1088 - acc: 0.9674 - val_loss: 0.0907 - val_acc: 0.9735\n",
            "Epoch 117/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.1095 - acc: 0.9674 - val_loss: 0.0907 - val_acc: 0.9741\n",
            "Epoch 118/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.1092 - acc: 0.9671 - val_loss: 0.0903 - val_acc: 0.9731\n",
            "Epoch 119/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.1081 - acc: 0.9678 - val_loss: 0.0900 - val_acc: 0.9742\n",
            "Epoch 120/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.1075 - acc: 0.9677 - val_loss: 0.0897 - val_acc: 0.9741\n",
            "Epoch 121/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.1071 - acc: 0.9681 - val_loss: 0.0892 - val_acc: 0.9746\n",
            "Epoch 122/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.1064 - acc: 0.9681 - val_loss: 0.0892 - val_acc: 0.9746\n",
            "Epoch 123/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.1028 - acc: 0.9694 - val_loss: 0.0891 - val_acc: 0.9739\n",
            "Epoch 124/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.1051 - acc: 0.9688 - val_loss: 0.0888 - val_acc: 0.9742\n",
            "Epoch 125/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.1050 - acc: 0.9691 - val_loss: 0.0884 - val_acc: 0.9746\n",
            "Epoch 126/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.1042 - acc: 0.9677 - val_loss: 0.0886 - val_acc: 0.9745\n",
            "Epoch 127/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.1006 - acc: 0.9701 - val_loss: 0.0885 - val_acc: 0.9745\n",
            "Epoch 128/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.1024 - acc: 0.9684 - val_loss: 0.0878 - val_acc: 0.9748\n",
            "Epoch 129/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.1017 - acc: 0.9697 - val_loss: 0.0882 - val_acc: 0.9744\n",
            "Epoch 130/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.1011 - acc: 0.9690 - val_loss: 0.0873 - val_acc: 0.9745\n",
            "Epoch 131/250\n",
            "48000/48000 [==============================] - 2s 47us/step - loss: 0.0993 - acc: 0.9696 - val_loss: 0.0878 - val_acc: 0.9749\n",
            "Epoch 132/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.1009 - acc: 0.9695 - val_loss: 0.0872 - val_acc: 0.9748\n",
            "Epoch 133/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.0995 - acc: 0.9697 - val_loss: 0.0869 - val_acc: 0.9749\n",
            "Epoch 134/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.0990 - acc: 0.9699 - val_loss: 0.0868 - val_acc: 0.9749\n",
            "Epoch 135/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.0980 - acc: 0.9706 - val_loss: 0.0870 - val_acc: 0.9750\n",
            "Epoch 136/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.0966 - acc: 0.9700 - val_loss: 0.0863 - val_acc: 0.9752\n",
            "Epoch 137/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.0975 - acc: 0.9707 - val_loss: 0.0863 - val_acc: 0.9752\n",
            "Epoch 138/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.0957 - acc: 0.9706 - val_loss: 0.0864 - val_acc: 0.9750\n",
            "Epoch 139/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0996 - acc: 0.9703 - val_loss: 0.0858 - val_acc: 0.9752\n",
            "Epoch 140/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0967 - acc: 0.9699 - val_loss: 0.0859 - val_acc: 0.9744\n",
            "Epoch 141/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.0979 - acc: 0.9701 - val_loss: 0.0852 - val_acc: 0.9747\n",
            "Epoch 142/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.0920 - acc: 0.9720 - val_loss: 0.0854 - val_acc: 0.9751\n",
            "Epoch 143/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0946 - acc: 0.9719 - val_loss: 0.0856 - val_acc: 0.9750\n",
            "Epoch 144/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0960 - acc: 0.9711 - val_loss: 0.0849 - val_acc: 0.9747\n",
            "Epoch 145/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.0942 - acc: 0.9720 - val_loss: 0.0851 - val_acc: 0.9753\n",
            "Epoch 146/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.0932 - acc: 0.9717 - val_loss: 0.0850 - val_acc: 0.9751\n",
            "Epoch 147/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.0930 - acc: 0.9724 - val_loss: 0.0853 - val_acc: 0.9754\n",
            "Epoch 148/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0928 - acc: 0.9721 - val_loss: 0.0855 - val_acc: 0.9747\n",
            "Epoch 149/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.0914 - acc: 0.9727 - val_loss: 0.0842 - val_acc: 0.9755\n",
            "Epoch 150/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0903 - acc: 0.9723 - val_loss: 0.0844 - val_acc: 0.9752\n",
            "Epoch 151/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0917 - acc: 0.9719 - val_loss: 0.0840 - val_acc: 0.9752\n",
            "Epoch 152/250\n",
            "48000/48000 [==============================] - 2s 47us/step - loss: 0.0886 - acc: 0.9732 - val_loss: 0.0847 - val_acc: 0.9752\n",
            "Epoch 153/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.0917 - acc: 0.9718 - val_loss: 0.0844 - val_acc: 0.9752\n",
            "Epoch 154/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0901 - acc: 0.9721 - val_loss: 0.0844 - val_acc: 0.9754\n",
            "Epoch 155/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.0888 - acc: 0.9728 - val_loss: 0.0837 - val_acc: 0.9753\n",
            "Epoch 156/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0897 - acc: 0.9733 - val_loss: 0.0838 - val_acc: 0.9756\n",
            "Epoch 157/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0909 - acc: 0.9720 - val_loss: 0.0841 - val_acc: 0.9756\n",
            "Epoch 158/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.0880 - acc: 0.9733 - val_loss: 0.0837 - val_acc: 0.9752\n",
            "Epoch 159/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.0876 - acc: 0.9740 - val_loss: 0.0840 - val_acc: 0.9750\n",
            "Epoch 160/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0857 - acc: 0.9737 - val_loss: 0.0835 - val_acc: 0.9751\n",
            "Epoch 161/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.0867 - acc: 0.9740 - val_loss: 0.0834 - val_acc: 0.9750\n",
            "Epoch 162/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.0866 - acc: 0.9733 - val_loss: 0.0830 - val_acc: 0.9754\n",
            "Epoch 163/250\n",
            "48000/48000 [==============================] - 2s 47us/step - loss: 0.0871 - acc: 0.9739 - val_loss: 0.0828 - val_acc: 0.9757\n",
            "Epoch 164/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.0852 - acc: 0.9742 - val_loss: 0.0829 - val_acc: 0.9752\n",
            "Epoch 165/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.0857 - acc: 0.9730 - val_loss: 0.0825 - val_acc: 0.9754\n",
            "Epoch 166/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.0850 - acc: 0.9740 - val_loss: 0.0822 - val_acc: 0.9754\n",
            "Epoch 167/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0856 - acc: 0.9745 - val_loss: 0.0827 - val_acc: 0.9757\n",
            "Epoch 168/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0851 - acc: 0.9739 - val_loss: 0.0827 - val_acc: 0.9755\n",
            "Epoch 169/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.0836 - acc: 0.9746 - val_loss: 0.0821 - val_acc: 0.9756\n",
            "Epoch 170/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0835 - acc: 0.9742 - val_loss: 0.0822 - val_acc: 0.9760\n",
            "Epoch 171/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0838 - acc: 0.9746 - val_loss: 0.0820 - val_acc: 0.9757\n",
            "Epoch 172/250\n",
            "48000/48000 [==============================] - 2s 43us/step - loss: 0.0844 - acc: 0.9746 - val_loss: 0.0819 - val_acc: 0.9757\n",
            "Epoch 173/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0818 - acc: 0.9751 - val_loss: 0.0816 - val_acc: 0.9761\n",
            "Epoch 174/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.0816 - acc: 0.9750 - val_loss: 0.0818 - val_acc: 0.9757\n",
            "Epoch 175/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0817 - acc: 0.9748 - val_loss: 0.0813 - val_acc: 0.9761\n",
            "Epoch 176/250\n",
            "48000/48000 [==============================] - 2s 43us/step - loss: 0.0814 - acc: 0.9753 - val_loss: 0.0815 - val_acc: 0.9762\n",
            "Epoch 177/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0817 - acc: 0.9750 - val_loss: 0.0814 - val_acc: 0.9766\n",
            "Epoch 178/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0832 - acc: 0.9748 - val_loss: 0.0820 - val_acc: 0.9755\n",
            "Epoch 179/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0805 - acc: 0.9752 - val_loss: 0.0823 - val_acc: 0.9759\n",
            "Epoch 180/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0814 - acc: 0.9759 - val_loss: 0.0811 - val_acc: 0.9763\n",
            "Epoch 181/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0801 - acc: 0.9753 - val_loss: 0.0812 - val_acc: 0.9760\n",
            "Epoch 182/250\n",
            "48000/48000 [==============================] - 2s 43us/step - loss: 0.0795 - acc: 0.9760 - val_loss: 0.0810 - val_acc: 0.9763\n",
            "Epoch 183/250\n",
            "48000/48000 [==============================] - 2s 43us/step - loss: 0.0801 - acc: 0.9754 - val_loss: 0.0812 - val_acc: 0.9764\n",
            "Epoch 184/250\n",
            "48000/48000 [==============================] - 2s 43us/step - loss: 0.0792 - acc: 0.9761 - val_loss: 0.0811 - val_acc: 0.9763\n",
            "Epoch 185/250\n",
            "48000/48000 [==============================] - 2s 43us/step - loss: 0.0806 - acc: 0.9749 - val_loss: 0.0810 - val_acc: 0.9762\n",
            "Epoch 186/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0808 - acc: 0.9752 - val_loss: 0.0806 - val_acc: 0.9762\n",
            "Epoch 187/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0788 - acc: 0.9757 - val_loss: 0.0812 - val_acc: 0.9757\n",
            "Epoch 188/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0780 - acc: 0.9763 - val_loss: 0.0809 - val_acc: 0.9760\n",
            "Epoch 189/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0775 - acc: 0.9761 - val_loss: 0.0810 - val_acc: 0.9753\n",
            "Epoch 190/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0765 - acc: 0.9762 - val_loss: 0.0806 - val_acc: 0.9769\n",
            "Epoch 191/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0774 - acc: 0.9764 - val_loss: 0.0801 - val_acc: 0.9766\n",
            "Epoch 192/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.0775 - acc: 0.9759 - val_loss: 0.0801 - val_acc: 0.9760\n",
            "Epoch 193/250\n",
            "48000/48000 [==============================] - 2s 43us/step - loss: 0.0755 - acc: 0.9763 - val_loss: 0.0801 - val_acc: 0.9763\n",
            "Epoch 194/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0777 - acc: 0.9765 - val_loss: 0.0805 - val_acc: 0.9762\n",
            "Epoch 195/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.0769 - acc: 0.9763 - val_loss: 0.0801 - val_acc: 0.9761\n",
            "Epoch 196/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.0761 - acc: 0.9765 - val_loss: 0.0804 - val_acc: 0.9762\n",
            "Epoch 197/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0734 - acc: 0.9773 - val_loss: 0.0799 - val_acc: 0.9762\n",
            "Epoch 198/250\n",
            "48000/48000 [==============================] - 2s 43us/step - loss: 0.0748 - acc: 0.9768 - val_loss: 0.0792 - val_acc: 0.9762\n",
            "Epoch 199/250\n",
            "48000/48000 [==============================] - 2s 43us/step - loss: 0.0735 - acc: 0.9771 - val_loss: 0.0796 - val_acc: 0.9769\n",
            "Epoch 200/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0746 - acc: 0.9774 - val_loss: 0.0793 - val_acc: 0.9767\n",
            "Epoch 201/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0748 - acc: 0.9769 - val_loss: 0.0796 - val_acc: 0.9765\n",
            "Epoch 202/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.0724 - acc: 0.9775 - val_loss: 0.0801 - val_acc: 0.9765\n",
            "Epoch 203/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.0745 - acc: 0.9770 - val_loss: 0.0797 - val_acc: 0.9762\n",
            "Epoch 204/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0727 - acc: 0.9776 - val_loss: 0.0799 - val_acc: 0.9765\n",
            "Epoch 205/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0746 - acc: 0.9767 - val_loss: 0.0795 - val_acc: 0.9762\n",
            "Epoch 206/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0721 - acc: 0.9779 - val_loss: 0.0797 - val_acc: 0.9768\n",
            "Epoch 207/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0735 - acc: 0.9767 - val_loss: 0.0793 - val_acc: 0.9768\n",
            "Epoch 208/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0722 - acc: 0.9772 - val_loss: 0.0792 - val_acc: 0.9762\n",
            "Epoch 209/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.0727 - acc: 0.9775 - val_loss: 0.0796 - val_acc: 0.9769\n",
            "Epoch 210/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.0738 - acc: 0.9766 - val_loss: 0.0790 - val_acc: 0.9772\n",
            "Epoch 211/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.0701 - acc: 0.9780 - val_loss: 0.0796 - val_acc: 0.9769\n",
            "Epoch 212/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0741 - acc: 0.9764 - val_loss: 0.0786 - val_acc: 0.9771\n",
            "Epoch 213/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.0699 - acc: 0.9785 - val_loss: 0.0789 - val_acc: 0.9766\n",
            "Epoch 214/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.0710 - acc: 0.9779 - val_loss: 0.0791 - val_acc: 0.9772\n",
            "Epoch 215/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.0698 - acc: 0.9785 - val_loss: 0.0790 - val_acc: 0.9772\n",
            "Epoch 216/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.0685 - acc: 0.9787 - val_loss: 0.0790 - val_acc: 0.9771\n",
            "Epoch 217/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.0694 - acc: 0.9778 - val_loss: 0.0790 - val_acc: 0.9770\n",
            "Epoch 218/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.0699 - acc: 0.9786 - val_loss: 0.0786 - val_acc: 0.9771\n",
            "Epoch 219/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0697 - acc: 0.9783 - val_loss: 0.0783 - val_acc: 0.9776\n",
            "Epoch 220/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0687 - acc: 0.9784 - val_loss: 0.0785 - val_acc: 0.9775\n",
            "Epoch 221/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0677 - acc: 0.9784 - val_loss: 0.0791 - val_acc: 0.9772\n",
            "Epoch 222/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0703 - acc: 0.9780 - val_loss: 0.0786 - val_acc: 0.9772\n",
            "Epoch 223/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0709 - acc: 0.9770 - val_loss: 0.0784 - val_acc: 0.9772\n",
            "Epoch 224/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.0668 - acc: 0.9799 - val_loss: 0.0785 - val_acc: 0.9771\n",
            "Epoch 225/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.0682 - acc: 0.9794 - val_loss: 0.0784 - val_acc: 0.9767\n",
            "Epoch 226/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0688 - acc: 0.9789 - val_loss: 0.0775 - val_acc: 0.9772\n",
            "Epoch 227/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.0671 - acc: 0.9791 - val_loss: 0.0782 - val_acc: 0.9767\n",
            "Epoch 228/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0664 - acc: 0.9799 - val_loss: 0.0780 - val_acc: 0.9773\n",
            "Epoch 229/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.0676 - acc: 0.9788 - val_loss: 0.0783 - val_acc: 0.9769\n",
            "Epoch 230/250\n",
            "48000/48000 [==============================] - 2s 48us/step - loss: 0.0655 - acc: 0.9795 - val_loss: 0.0775 - val_acc: 0.9773\n",
            "Epoch 231/250\n",
            "48000/48000 [==============================] - 2s 47us/step - loss: 0.0665 - acc: 0.9791 - val_loss: 0.0773 - val_acc: 0.9775\n",
            "Epoch 232/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0665 - acc: 0.9796 - val_loss: 0.0782 - val_acc: 0.9769\n",
            "Epoch 233/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.0662 - acc: 0.9795 - val_loss: 0.0780 - val_acc: 0.9772\n",
            "Epoch 234/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.0647 - acc: 0.9792 - val_loss: 0.0785 - val_acc: 0.9768\n",
            "Epoch 235/250\n",
            "48000/48000 [==============================] - 2s 47us/step - loss: 0.0669 - acc: 0.9791 - val_loss: 0.0780 - val_acc: 0.9770\n",
            "Epoch 236/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.0637 - acc: 0.9803 - val_loss: 0.0785 - val_acc: 0.9767\n",
            "Epoch 237/250\n",
            "48000/48000 [==============================] - 2s 47us/step - loss: 0.0656 - acc: 0.9796 - val_loss: 0.0786 - val_acc: 0.9766\n",
            "Epoch 238/250\n",
            "48000/48000 [==============================] - 2s 49us/step - loss: 0.0640 - acc: 0.9800 - val_loss: 0.0782 - val_acc: 0.9765\n",
            "Epoch 239/250\n",
            "48000/48000 [==============================] - 2s 49us/step - loss: 0.0649 - acc: 0.9805 - val_loss: 0.0779 - val_acc: 0.9770\n",
            "Epoch 240/250\n",
            "48000/48000 [==============================] - 2s 47us/step - loss: 0.0634 - acc: 0.9804 - val_loss: 0.0780 - val_acc: 0.9772\n",
            "Epoch 241/250\n",
            "48000/48000 [==============================] - 2s 44us/step - loss: 0.0648 - acc: 0.9794 - val_loss: 0.0782 - val_acc: 0.9766\n",
            "Epoch 242/250\n",
            "48000/48000 [==============================] - 2s 45us/step - loss: 0.0652 - acc: 0.9800 - val_loss: 0.0769 - val_acc: 0.9774\n",
            "Epoch 243/250\n",
            "48000/48000 [==============================] - 2s 48us/step - loss: 0.0626 - acc: 0.9806 - val_loss: 0.0770 - val_acc: 0.9772\n",
            "Epoch 244/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.0658 - acc: 0.9797 - val_loss: 0.0772 - val_acc: 0.9775\n",
            "Epoch 245/250\n",
            "48000/48000 [==============================] - 2s 47us/step - loss: 0.0632 - acc: 0.9801 - val_loss: 0.0772 - val_acc: 0.9779\n",
            "Epoch 246/250\n",
            "48000/48000 [==============================] - 2s 47us/step - loss: 0.0634 - acc: 0.9801 - val_loss: 0.0773 - val_acc: 0.9776\n",
            "Epoch 247/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.0623 - acc: 0.9810 - val_loss: 0.0771 - val_acc: 0.9770\n",
            "Epoch 248/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.0616 - acc: 0.9812 - val_loss: 0.0772 - val_acc: 0.9773\n",
            "Epoch 249/250\n",
            "48000/48000 [==============================] - 2s 47us/step - loss: 0.0631 - acc: 0.9800 - val_loss: 0.0775 - val_acc: 0.9779\n",
            "Epoch 250/250\n",
            "48000/48000 [==============================] - 2s 46us/step - loss: 0.0615 - acc: 0.9811 - val_loss: 0.0771 - val_acc: 0.9769\n",
            "10000/10000 [==============================] - 1s 56us/step\n",
            "Test score:  0.07217223191247904\n",
            "Test accuracy:  0.979\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "N3iMYwHlKT_A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 最適化アルゴリズムの変更"
      ]
    },
    {
      "metadata": {
        "id": "DpeND8w_Jrf_",
        "colab_type": "code",
        "outputId": "869535a4-fbd3-4445-ea3c-f55ea8640180",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1156
        }
      },
      "cell_type": "code",
      "source": [
        "# RMSpropに変更\n",
        "from keras.optimizers import RMSprop, Adam\n",
        "\n",
        "NB_EPOCH = 20\n",
        "OPTIMIZER = RMSprop()\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED, )))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(DROPOUT))\n",
        "model.add(Dense(N_HIDDEN))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(DROPOUT))\n",
        "model.add(Dense(NB_CLASSES))\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, \n",
        "             metrics=['accuracy'])\n",
        "\n",
        "callbacks = [make_tensorboard(set_dir_name='drive/My Drive/Colab Notebooks/chokkan_deeplearning/keras_MNIST_V4')]\n",
        "# -----\n",
        "# 学習\n",
        "# -----\n",
        "model.fit(X_train, Y_train, \n",
        "         batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
        "         callbacks=callbacks,\n",
        "         verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
        "\n",
        "# ------\n",
        "# 評価\n",
        "# ------\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
        "print(\"Test score: \", score[0])\n",
        "print(\"Test accuracy: \", score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_24 (Dense)             (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "activation_25 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "activation_26 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 10)                1290      \n",
            "_________________________________________________________________\n",
            "activation_27 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 118,282\n",
            "Trainable params: 118,282\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/20\n",
            "48000/48000 [==============================] - 3s 53us/step - loss: 0.4724 - acc: 0.8580 - val_loss: 0.1806 - val_acc: 0.9448\n",
            "Epoch 2/20\n",
            "48000/48000 [==============================] - 2s 51us/step - loss: 0.2249 - acc: 0.9331 - val_loss: 0.1406 - val_acc: 0.9589\n",
            "Epoch 3/20\n",
            "48000/48000 [==============================] - 3s 53us/step - loss: 0.1721 - acc: 0.9491 - val_loss: 0.1192 - val_acc: 0.9658\n",
            "Epoch 4/20\n",
            "48000/48000 [==============================] - 2s 51us/step - loss: 0.1444 - acc: 0.9575 - val_loss: 0.1156 - val_acc: 0.9655\n",
            "Epoch 5/20\n",
            "48000/48000 [==============================] - 2s 50us/step - loss: 0.1291 - acc: 0.9616 - val_loss: 0.1026 - val_acc: 0.9697\n",
            "Epoch 6/20\n",
            "48000/48000 [==============================] - 2s 51us/step - loss: 0.1167 - acc: 0.9656 - val_loss: 0.0998 - val_acc: 0.9717\n",
            "Epoch 7/20\n",
            "48000/48000 [==============================] - 2s 50us/step - loss: 0.1080 - acc: 0.9673 - val_loss: 0.1003 - val_acc: 0.9740\n",
            "Epoch 8/20\n",
            "48000/48000 [==============================] - 2s 50us/step - loss: 0.1024 - acc: 0.9690 - val_loss: 0.1008 - val_acc: 0.9740\n",
            "Epoch 9/20\n",
            "48000/48000 [==============================] - 2s 51us/step - loss: 0.0964 - acc: 0.9714 - val_loss: 0.0982 - val_acc: 0.9742\n",
            "Epoch 10/20\n",
            "48000/48000 [==============================] - 2s 50us/step - loss: 0.0885 - acc: 0.9736 - val_loss: 0.0957 - val_acc: 0.9748\n",
            "Epoch 11/20\n",
            "48000/48000 [==============================] - 2s 52us/step - loss: 0.0864 - acc: 0.9746 - val_loss: 0.0936 - val_acc: 0.9757\n",
            "Epoch 12/20\n",
            "48000/48000 [==============================] - 2s 51us/step - loss: 0.0834 - acc: 0.9748 - val_loss: 0.0912 - val_acc: 0.9768\n",
            "Epoch 13/20\n",
            "48000/48000 [==============================] - 2s 51us/step - loss: 0.0804 - acc: 0.9761 - val_loss: 0.0993 - val_acc: 0.9759\n",
            "Epoch 14/20\n",
            "48000/48000 [==============================] - 2s 52us/step - loss: 0.0744 - acc: 0.9777 - val_loss: 0.1017 - val_acc: 0.9752\n",
            "Epoch 15/20\n",
            "48000/48000 [==============================] - 3s 54us/step - loss: 0.0772 - acc: 0.9771 - val_loss: 0.1035 - val_acc: 0.9754\n",
            "Epoch 16/20\n",
            "48000/48000 [==============================] - 3s 53us/step - loss: 0.0740 - acc: 0.9787 - val_loss: 0.1011 - val_acc: 0.9759\n",
            "Epoch 17/20\n",
            "48000/48000 [==============================] - 3s 54us/step - loss: 0.0706 - acc: 0.9799 - val_loss: 0.1006 - val_acc: 0.9767\n",
            "Epoch 18/20\n",
            "48000/48000 [==============================] - 2s 50us/step - loss: 0.0692 - acc: 0.9788 - val_loss: 0.1083 - val_acc: 0.9763\n",
            "Epoch 19/20\n",
            "48000/48000 [==============================] - 2s 52us/step - loss: 0.0695 - acc: 0.9795 - val_loss: 0.1009 - val_acc: 0.9764\n",
            "Epoch 20/20\n",
            "48000/48000 [==============================] - 2s 52us/step - loss: 0.0655 - acc: 0.9809 - val_loss: 0.1040 - val_acc: 0.9773\n",
            "10000/10000 [==============================] - 1s 54us/step\n",
            "Test score:  0.09649358916103301\n",
            "Test accuracy:  0.9766\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eZM8FC8oK3TD",
        "colab_type": "code",
        "outputId": "95c7d786-b777-4990-c024-402c688d5a91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1156
        }
      },
      "cell_type": "code",
      "source": [
        "# Adamに変更\n",
        "from keras.optimizers import RMSprop, Adam\n",
        "\n",
        "NB_EPOCH = 20\n",
        "OPTIMIZER = Adam()\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED, )))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(DROPOUT))\n",
        "model.add(Dense(N_HIDDEN))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(DROPOUT))\n",
        "model.add(Dense(NB_CLASSES))\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, \n",
        "             metrics=['accuracy'])\n",
        "\n",
        "callbacks = [make_tensorboard(set_dir_name='drive/My Drive/Colab Notebooks/chokkan_deeplearning/keras_MNIST_V4')]\n",
        "# -----\n",
        "# 学習\n",
        "# -----\n",
        "model.fit(X_train, Y_train, \n",
        "         batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
        "         callbacks=callbacks,\n",
        "         verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
        "\n",
        "# ------\n",
        "# 評価\n",
        "# ------\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
        "print(\"Test score: \", score[0])\n",
        "print(\"Test accuracy: \", score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_27 (Dense)             (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "activation_28 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "activation_29 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 10)                1290      \n",
            "_________________________________________________________________\n",
            "activation_30 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 118,282\n",
            "Trainable params: 118,282\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/20\n",
            "48000/48000 [==============================] - 3s 59us/step - loss: 0.5186 - acc: 0.8398 - val_loss: 0.1795 - val_acc: 0.9463\n",
            "Epoch 2/20\n",
            "48000/48000 [==============================] - 3s 54us/step - loss: 0.2346 - acc: 0.9289 - val_loss: 0.1389 - val_acc: 0.9584\n",
            "Epoch 3/20\n",
            "48000/48000 [==============================] - 3s 56us/step - loss: 0.1800 - acc: 0.9463 - val_loss: 0.1198 - val_acc: 0.9637\n",
            "Epoch 4/20\n",
            "48000/48000 [==============================] - 3s 57us/step - loss: 0.1494 - acc: 0.9561 - val_loss: 0.1015 - val_acc: 0.9695\n",
            "Epoch 5/20\n",
            "48000/48000 [==============================] - 3s 56us/step - loss: 0.1291 - acc: 0.9615 - val_loss: 0.0981 - val_acc: 0.9705\n",
            "Epoch 6/20\n",
            "48000/48000 [==============================] - 3s 56us/step - loss: 0.1193 - acc: 0.9631 - val_loss: 0.0922 - val_acc: 0.9732\n",
            "Epoch 7/20\n",
            "48000/48000 [==============================] - 3s 55us/step - loss: 0.1044 - acc: 0.9675 - val_loss: 0.0906 - val_acc: 0.9722\n",
            "Epoch 8/20\n",
            "48000/48000 [==============================] - 3s 57us/step - loss: 0.0992 - acc: 0.9691 - val_loss: 0.0915 - val_acc: 0.9729\n",
            "Epoch 9/20\n",
            "48000/48000 [==============================] - 3s 55us/step - loss: 0.0922 - acc: 0.9719 - val_loss: 0.0918 - val_acc: 0.9737\n",
            "Epoch 10/20\n",
            "48000/48000 [==============================] - 3s 54us/step - loss: 0.0861 - acc: 0.9723 - val_loss: 0.0824 - val_acc: 0.9769\n",
            "Epoch 11/20\n",
            "48000/48000 [==============================] - 3s 58us/step - loss: 0.0781 - acc: 0.9752 - val_loss: 0.0818 - val_acc: 0.9768\n",
            "Epoch 12/20\n",
            "48000/48000 [==============================] - 3s 57us/step - loss: 0.0762 - acc: 0.9751 - val_loss: 0.0852 - val_acc: 0.9758\n",
            "Epoch 13/20\n",
            "48000/48000 [==============================] - 3s 56us/step - loss: 0.0685 - acc: 0.9776 - val_loss: 0.0818 - val_acc: 0.9767\n",
            "Epoch 14/20\n",
            "48000/48000 [==============================] - 3s 60us/step - loss: 0.0687 - acc: 0.9776 - val_loss: 0.0846 - val_acc: 0.9770\n",
            "Epoch 15/20\n",
            "48000/48000 [==============================] - 3s 56us/step - loss: 0.0643 - acc: 0.9792 - val_loss: 0.0801 - val_acc: 0.9775\n",
            "Epoch 16/20\n",
            "48000/48000 [==============================] - 3s 58us/step - loss: 0.0631 - acc: 0.9789 - val_loss: 0.0763 - val_acc: 0.9785\n",
            "Epoch 17/20\n",
            "48000/48000 [==============================] - 3s 55us/step - loss: 0.0598 - acc: 0.9810 - val_loss: 0.0809 - val_acc: 0.9770\n",
            "Epoch 18/20\n",
            "48000/48000 [==============================] - 3s 58us/step - loss: 0.0573 - acc: 0.9814 - val_loss: 0.0805 - val_acc: 0.9782\n",
            "Epoch 19/20\n",
            "48000/48000 [==============================] - 3s 59us/step - loss: 0.0564 - acc: 0.9820 - val_loss: 0.0786 - val_acc: 0.9784\n",
            "Epoch 20/20\n",
            "48000/48000 [==============================] - 3s 59us/step - loss: 0.0555 - acc: 0.9820 - val_loss: 0.0814 - val_acc: 0.9782\n",
            "10000/10000 [==============================] - 1s 58us/step\n",
            "Test score:  0.0769609311797627\n",
            "Test accuracy:  0.9796\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "85DmSAGnMXun",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- Adamのほうが若干良い\n",
        "- 少ないエポック数で高い精度が出せる\n",
        "\n",
        "- ニューロン数、エポック数の増加はコスパ悪い"
      ]
    },
    {
      "metadata": {
        "id": "dYr_qhQvM6YL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 正則化\n",
        "- モデルが複雑になると過学習を起こしやすくなる => 複雑度に対してペナルティを与える"
      ]
    },
    {
      "metadata": {
        "id": "-LQBV2A1L2vJ",
        "colab_type": "code",
        "outputId": "002ac66e-aee9-423e-c276-62c459d9f0b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1156
        }
      },
      "cell_type": "code",
      "source": [
        "from keras import regularizers\n",
        " \n",
        "NB_EPOCH = 20\n",
        "OPTIMIZER = Adam()\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED, ),\n",
        "               kernel_regularizer=regularizers.l2(0.01)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(DROPOUT))\n",
        "model.add(Dense(N_HIDDEN,\n",
        "               kernel_regularizer=regularizers.l2(0.01)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(DROPOUT))\n",
        "model.add(Dense(NB_CLASSES,\n",
        "               kernel_regularizer=regularizers.l2(0.01)))\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, \n",
        "             metrics=['accuracy'])\n",
        "\n",
        "callbacks = [make_tensorboard(set_dir_name='drive/My Drive/Colab Notebooks/chokkan_deeplearning/keras_MNIST_V4')]\n",
        "# -----\n",
        "# 学習\n",
        "# -----\n",
        "model.fit(X_train, Y_train, \n",
        "         batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
        "         callbacks=callbacks,\n",
        "         verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
        "\n",
        "# ------\n",
        "# 評価\n",
        "# ------\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
        "print(\"Test score: \", score[0])\n",
        "print(\"Test accuracy: \", score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_32 (Dense)             (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "activation_32 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "activation_33 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, 10)                1290      \n",
            "_________________________________________________________________\n",
            "activation_34 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 118,282\n",
            "Trainable params: 118,282\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/20\n",
            "48000/48000 [==============================] - 3s 69us/step - loss: 1.6676 - acc: 0.8071 - val_loss: 0.8550 - val_acc: 0.9102\n",
            "Epoch 2/20\n",
            "48000/48000 [==============================] - 3s 64us/step - loss: 0.9195 - acc: 0.8777 - val_loss: 0.8031 - val_acc: 0.9128\n",
            "Epoch 3/20\n",
            "48000/48000 [==============================] - 3s 62us/step - loss: 0.8877 - acc: 0.8839 - val_loss: 0.7852 - val_acc: 0.9147\n",
            "Epoch 4/20\n",
            "48000/48000 [==============================] - 3s 63us/step - loss: 0.8782 - acc: 0.8846 - val_loss: 0.7760 - val_acc: 0.9157\n",
            "Epoch 5/20\n",
            "48000/48000 [==============================] - 3s 60us/step - loss: 0.8679 - acc: 0.8865 - val_loss: 0.7788 - val_acc: 0.9135\n",
            "Epoch 6/20\n",
            "48000/48000 [==============================] - 3s 63us/step - loss: 0.8663 - acc: 0.8857 - val_loss: 0.7721 - val_acc: 0.9193\n",
            "Epoch 7/20\n",
            "48000/48000 [==============================] - 3s 62us/step - loss: 0.8624 - acc: 0.8853 - val_loss: 0.7665 - val_acc: 0.9160\n",
            "Epoch 8/20\n",
            "48000/48000 [==============================] - 3s 65us/step - loss: 0.8603 - acc: 0.8862 - val_loss: 0.7703 - val_acc: 0.9160\n",
            "Epoch 9/20\n",
            "48000/48000 [==============================] - 3s 61us/step - loss: 0.8585 - acc: 0.8869 - val_loss: 0.7725 - val_acc: 0.9134\n",
            "Epoch 10/20\n",
            "48000/48000 [==============================] - 3s 63us/step - loss: 0.8590 - acc: 0.8857 - val_loss: 0.7765 - val_acc: 0.9147\n",
            "Epoch 11/20\n",
            "48000/48000 [==============================] - 3s 60us/step - loss: 0.8534 - acc: 0.8886 - val_loss: 0.7649 - val_acc: 0.9163\n",
            "Epoch 12/20\n",
            "48000/48000 [==============================] - 3s 62us/step - loss: 0.8544 - acc: 0.8872 - val_loss: 0.7684 - val_acc: 0.9143\n",
            "Epoch 13/20\n",
            "48000/48000 [==============================] - 3s 62us/step - loss: 0.8583 - acc: 0.8857 - val_loss: 0.7672 - val_acc: 0.9156\n",
            "Epoch 14/20\n",
            "48000/48000 [==============================] - 3s 63us/step - loss: 0.8527 - acc: 0.8880 - val_loss: 0.7779 - val_acc: 0.9107\n",
            "Epoch 15/20\n",
            "48000/48000 [==============================] - 3s 64us/step - loss: 0.8520 - acc: 0.8876 - val_loss: 0.7648 - val_acc: 0.9187\n",
            "Epoch 16/20\n",
            "48000/48000 [==============================] - 3s 63us/step - loss: 0.8518 - acc: 0.8866 - val_loss: 0.7711 - val_acc: 0.9144\n",
            "Epoch 17/20\n",
            "48000/48000 [==============================] - 3s 58us/step - loss: 0.8535 - acc: 0.8865 - val_loss: 0.7639 - val_acc: 0.9141\n",
            "Epoch 18/20\n",
            "48000/48000 [==============================] - 3s 62us/step - loss: 0.8528 - acc: 0.8877 - val_loss: 0.7701 - val_acc: 0.9160\n",
            "Epoch 19/20\n",
            "48000/48000 [==============================] - 3s 62us/step - loss: 0.8513 - acc: 0.8873 - val_loss: 0.7652 - val_acc: 0.9153\n",
            "Epoch 20/20\n",
            "48000/48000 [==============================] - 3s 59us/step - loss: 0.8539 - acc: 0.8865 - val_loss: 0.7655 - val_acc: 0.9153\n",
            "10000/10000 [==============================] - 1s 68us/step\n",
            "Test score:  0.7707216023445129\n",
            "Test accuracy:  0.9124\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bbbFb04PRTNj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 第３章：畳み込みニューラルネットワーク\n"
      ]
    },
    {
      "metadata": {
        "id": "JBoSzPVGT0F7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## LeNetの実装\n",
        "- 深い層ほどフィルター数を増やす"
      ]
    },
    {
      "metadata": {
        "id": "uPhbscU5P7Yy",
        "colab_type": "code",
        "outputId": "f29b32ec-ed0e-4f18-9ea0-bec210269f58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers.core import Flatten, Dropout\n",
        "from keras.layers.core import Dense\n",
        "from keras.datasets import mnist\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "def lenet(input_shape, num_classes):\n",
        "  \n",
        "  model = Sequential()\n",
        "  \n",
        "  #  畳み込み層とプーリング層で特徴マップを抽出\n",
        "  model.add(Conv2D(20, kernel_size=5, padding=\"same\",\n",
        "                  input_shape=input_shape, activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Conv2D(50, kernel_size=5, padding=\"same\",\n",
        "                   activation='relu'))\n",
        "  \n",
        "  # 全結合層でクラス判定\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(500, activation='relu'))\n",
        "  model.add(Dense(num_classes))\n",
        "  model.add(Activation(\"softmax\"))\n",
        "  return model\n",
        "\n",
        "class MNISTDataset():\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.image_shape = (28, 28, 1)\n",
        "    self.num_classes = 10\n",
        "    \n",
        "  def get_batch(self):\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "    \n",
        "    x_train, x_test = [self.preprocess(d) for d in [x_train, x_test]]\n",
        "    y_train, y_test = [self.preprocess(d, label_data=True) for d in [y_train, y_test]]\n",
        "    \n",
        "    return x_train, y_train, x_test, y_test\n",
        "  \n",
        "  def preprocess(self, data, label_data=False):\n",
        "    if label_data:\n",
        "      # クラスをOne-hotに変換\n",
        "      data = keras.utils.to_categorical(data, self.num_classes)\n",
        "    else:\n",
        "      data = data.astype(\"float32\")\n",
        "      data /= 255\n",
        "      shape = (data.shape[0], ) + self.image_shape # データセットの長さを頭に加える\n",
        "      data = data.reshape(shape)\n",
        "      \n",
        "    return data\n",
        "  \n",
        "class Trainer():\n",
        "  \n",
        "  def __init__(self, model, loss, optimizer):\n",
        "    self._target = model\n",
        "    self._target.compile(loss=loss, optimizer=optimizer,\n",
        "                        metrics=[\"accuracy\"])\n",
        "    self.verbose =1\n",
        "    self.log_dir = os.path.join(\"./\", \"logdir\")\n",
        "    \n",
        "  def train(self, x_train, y_train, batch_size, epochs, validation_split):\n",
        "    if os.path.exists(self.log_dir):\n",
        "      import shutil\n",
        "      shutil.rmtree(self.log_dir)\n",
        "    os.mkdir(self.log_dir)\n",
        "    \n",
        "    self._target.fit(\n",
        "    x_train, y_train,\n",
        "    batch_size=batch_size, epochs=epochs,\n",
        "    validation_split=validation_split,\n",
        "    callbacks=[TensorBoard(log_dir=self.log_dir)],\n",
        "    verbose=self.verbose\n",
        "    )\n",
        "    \n",
        "dataset = MNISTDataset()\n",
        "\n",
        "# モデル作成\n",
        "model = lenet(dataset.image_shape, dataset.num_classes)\n",
        "\n",
        "# 学習\n",
        "x_train, y_train, x_test, y_test = dataset.get_batch()\n",
        "trainer = Trainer(model, loss=\"categorical_crossentropy\",\n",
        "                 optimizer=Adam())\n",
        "trainer.train(x_train, y_train, batch_size=128, epochs=12,\n",
        "             validation_split=0.2)\n",
        "\n",
        "# 評価\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss: \", score[0])\n",
        "print(\"Test accuracy: \", score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/12\n",
            "48000/48000 [==============================] - 10s 216us/step - loss: 0.1594 - acc: 0.9512 - val_loss: 0.0588 - val_acc: 0.9830\n",
            "Epoch 2/12\n",
            "48000/48000 [==============================] - 8s 164us/step - loss: 0.0435 - acc: 0.9864 - val_loss: 0.0429 - val_acc: 0.9877\n",
            "Epoch 3/12\n",
            "48000/48000 [==============================] - 8s 163us/step - loss: 0.0278 - acc: 0.9911 - val_loss: 0.0401 - val_acc: 0.9878\n",
            "Epoch 4/12\n",
            "48000/48000 [==============================] - 8s 163us/step - loss: 0.0195 - acc: 0.9935 - val_loss: 0.0345 - val_acc: 0.9895\n",
            "Epoch 5/12\n",
            "48000/48000 [==============================] - 8s 160us/step - loss: 0.0136 - acc: 0.9957 - val_loss: 0.0347 - val_acc: 0.9905\n",
            "Epoch 6/12\n",
            "48000/48000 [==============================] - 8s 158us/step - loss: 0.0090 - acc: 0.9969 - val_loss: 0.0394 - val_acc: 0.9905\n",
            "Epoch 7/12\n",
            "48000/48000 [==============================] - 8s 163us/step - loss: 0.0077 - acc: 0.9975 - val_loss: 0.0428 - val_acc: 0.9882\n",
            "Epoch 8/12\n",
            "48000/48000 [==============================] - 8s 167us/step - loss: 0.0065 - acc: 0.9979 - val_loss: 0.0389 - val_acc: 0.9904\n",
            "Epoch 9/12\n",
            "48000/48000 [==============================] - 8s 164us/step - loss: 0.0080 - acc: 0.9975 - val_loss: 0.0390 - val_acc: 0.9893\n",
            "Epoch 10/12\n",
            "48000/48000 [==============================] - 8s 166us/step - loss: 0.0078 - acc: 0.9972 - val_loss: 0.0380 - val_acc: 0.9911\n",
            "Epoch 11/12\n",
            "48000/48000 [==============================] - 8s 175us/step - loss: 0.0060 - acc: 0.9979 - val_loss: 0.0429 - val_acc: 0.9895\n",
            "Epoch 12/12\n",
            "48000/48000 [==============================] - 8s 175us/step - loss: 0.0043 - acc: 0.9987 - val_loss: 0.0438 - val_acc: 0.9902\n",
            "Test loss:  0.030682929307803215\n",
            "Test accuracy:  0.9915\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eHtINiVCbpSx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## CIFAR-10の画像認識"
      ]
    },
    {
      "metadata": {
        "id": "Fuc2yU4iblhL",
        "colab_type": "code",
        "outputId": "a1c902d3-48d9-485a-dce7-7aed77e0317f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers.core import Flatten, Dropout\n",
        "from keras.layers.core import Dense\n",
        "from keras.datasets import cifar10\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
        "\n",
        "def network(input_shape, num_classes):\n",
        "  \n",
        "  model = Sequential()\n",
        "  \n",
        "  #  畳み込み層とプーリング層で特徴マップを抽出\n",
        "  model.add(Conv2D(32, kernel_size=3, padding=\"same\",\n",
        "                  input_shape=input_shape, activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Conv2D(64, kernel_size=3, padding=\"same\",\n",
        "                   activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  \n",
        "  # 全結合層でクラス判定\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512, activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(num_classes))\n",
        "  model.add(Activation(\"softmax\"))\n",
        "  return model\n",
        "\n",
        "class CIFAR10Dataset():\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.image_shape = (32, 32, 3)\n",
        "    self.num_classes = 10\n",
        "    \n",
        "  def get_batch(self):\n",
        "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "    \n",
        "    x_train, x_test = [self.preprocess(d) for d in [x_train, x_test]]\n",
        "    y_train, y_test = [self.preprocess(d, label_data=True) for d in [y_train, y_test]]\n",
        "    \n",
        "    return x_train, y_train, x_test, y_test\n",
        "  \n",
        "  def preprocess(self, data, label_data=False):\n",
        "    if label_data:\n",
        "      # クラスをOne-hotに変換\n",
        "      data = keras.utils.to_categorical(data, self.num_classes)\n",
        "    else:\n",
        "      data = data.astype(\"float32\")\n",
        "      data /= 255\n",
        "      shape = (data.shape[0], ) + self.image_shape # データセットの長さを頭に加える\n",
        "      data = data.reshape(shape)\n",
        "      \n",
        "    return data\n",
        "  \n",
        "class Trainer():\n",
        "  \n",
        "  def __init__(self, model, loss, optimizer):\n",
        "    self._target = model\n",
        "    self._target.compile(loss=loss, optimizer=optimizer,\n",
        "                        metrics=[\"accuracy\"])\n",
        "    self.verbose =1\n",
        "    self.log_dir = os.path.join(\"./\", \"logdir\")\n",
        "    self.model_file_name = \"model_file.hdf5\"\n",
        "    \n",
        "  def train(self, x_train, y_train, batch_size, epochs, validation_split):\n",
        "    if os.path.exists(self.log_dir):\n",
        "      import shutil\n",
        "      shutil.rmtree(self.log_dir)\n",
        "    os.mkdir(self.log_dir)\n",
        "    \n",
        "    self._target.fit(\n",
        "    x_train, y_train,\n",
        "    batch_size=batch_size, epochs=epochs,\n",
        "    validation_split=validation_split,\n",
        "    callbacks=[TensorBoard(log_dir=self.log_dir),\n",
        "              ModelCheckpoint(os.path.join(self.log_dir,\n",
        "                                          self.model_file_name), save_best_only=True)],\n",
        "    verbose=self.verbose\n",
        "    )\n",
        "    \n",
        "dataset = CIFAR10Dataset()\n",
        "\n",
        "# モデル作成\n",
        "model = network(dataset.image_shape, dataset.num_classes)\n",
        "\n",
        "# 学習\n",
        "x_train, y_train, x_test, y_test = dataset.get_batch()\n",
        "trainer = Trainer(model, loss=\"categorical_crossentropy\",\n",
        "                 optimizer=RMSprop())\n",
        "trainer.train(x_train, y_train, batch_size=128, epochs=12,\n",
        "             validation_split=0.2)\n",
        "\n",
        "# 評価\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss: \", score[0])\n",
        "print(\"Test accuracy: \", score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 43s 0us/step\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/12\n",
            "40000/40000 [==============================] - 8s 189us/step - loss: 1.7327 - acc: 0.3809 - val_loss: 1.4056 - val_acc: 0.5070\n",
            "Epoch 2/12\n",
            "40000/40000 [==============================] - 7s 170us/step - loss: 1.3467 - acc: 0.5179 - val_loss: 1.2112 - val_acc: 0.5765\n",
            "Epoch 3/12\n",
            "40000/40000 [==============================] - 6s 155us/step - loss: 1.1820 - acc: 0.5819 - val_loss: 1.1211 - val_acc: 0.6223\n",
            "Epoch 4/12\n",
            "40000/40000 [==============================] - 6s 150us/step - loss: 1.0702 - acc: 0.6219 - val_loss: 1.0898 - val_acc: 0.6282\n",
            "Epoch 5/12\n",
            "40000/40000 [==============================] - 6s 154us/step - loss: 0.9854 - acc: 0.6553 - val_loss: 0.9472 - val_acc: 0.6739\n",
            "Epoch 6/12\n",
            "40000/40000 [==============================] - 6s 162us/step - loss: 0.9210 - acc: 0.6801 - val_loss: 0.9214 - val_acc: 0.6837\n",
            "Epoch 7/12\n",
            "40000/40000 [==============================] - 6s 159us/step - loss: 0.8579 - acc: 0.7010 - val_loss: 0.8513 - val_acc: 0.7071\n",
            "Epoch 8/12\n",
            "40000/40000 [==============================] - 6s 156us/step - loss: 0.8117 - acc: 0.7185 - val_loss: 0.8711 - val_acc: 0.7038\n",
            "Epoch 9/12\n",
            "40000/40000 [==============================] - 6s 152us/step - loss: 0.7608 - acc: 0.7332 - val_loss: 0.8766 - val_acc: 0.6960\n",
            "Epoch 10/12\n",
            "40000/40000 [==============================] - 6s 152us/step - loss: 0.7190 - acc: 0.7492 - val_loss: 0.8445 - val_acc: 0.7096\n",
            "Epoch 11/12\n",
            "40000/40000 [==============================] - 6s 151us/step - loss: 0.6794 - acc: 0.7624 - val_loss: 0.8413 - val_acc: 0.7171\n",
            "Epoch 12/12\n",
            "40000/40000 [==============================] - 6s 155us/step - loss: 0.6433 - acc: 0.7760 - val_loss: 0.8096 - val_acc: 0.7268\n",
            "Test loss:  0.8264303357124329\n",
            "Test accuracy:  0.7209\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xds7SsApeErI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 畳み込み層の追加による改善"
      ]
    },
    {
      "metadata": {
        "id": "QBVM6R5tRSIE",
        "colab_type": "code",
        "outputId": "42655ede-19ca-4d8b-ad85-96cfeb5a26d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "cell_type": "code",
      "source": [
        "def network(input_shape, num_classes):\n",
        "  \n",
        "  model = Sequential()\n",
        "  \n",
        "  #  畳み込み層とプーリング層で特徴マップを抽出\n",
        "  model.add(Conv2D(32, kernel_size=3, padding=\"same\",\n",
        "                  input_shape=input_shape, activation='relu'))\n",
        "  model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Conv2D(64, kernel_size=3, padding=\"same\",\n",
        "                   activation='relu'))\n",
        "  model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  \n",
        "  # 全結合層でクラス判定\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512, activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(num_classes))\n",
        "  model.add(Activation(\"softmax\"))\n",
        "  return model\n",
        "\n",
        "# モデル作成\n",
        "model = network(dataset.image_shape, dataset.num_classes)\n",
        "\n",
        "# 学習\n",
        "x_train, y_train, x_test, y_test = dataset.get_batch()\n",
        "trainer = Trainer(model, loss=\"categorical_crossentropy\",\n",
        "                 optimizer=RMSprop())\n",
        "trainer.train(x_train, y_train, batch_size=128, epochs=12,\n",
        "             validation_split=0.2)\n",
        "\n",
        "# 評価\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss: \", score[0])\n",
        "print(\"Test accuracy: \", score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/12\n",
            "40000/40000 [==============================] - 10s 247us/step - loss: 1.7982 - acc: 0.3513 - val_loss: 1.4907 - val_acc: 0.4629\n",
            "Epoch 2/12\n",
            "40000/40000 [==============================] - 9s 230us/step - loss: 1.3813 - acc: 0.5052 - val_loss: 1.2096 - val_acc: 0.5693\n",
            "Epoch 3/12\n",
            "40000/40000 [==============================] - 9s 233us/step - loss: 1.1574 - acc: 0.5963 - val_loss: 1.0858 - val_acc: 0.6195\n",
            "Epoch 4/12\n",
            "40000/40000 [==============================] - 10s 239us/step - loss: 1.0142 - acc: 0.6441 - val_loss: 0.9612 - val_acc: 0.6570\n",
            "Epoch 5/12\n",
            "40000/40000 [==============================] - 9s 232us/step - loss: 0.9000 - acc: 0.6845 - val_loss: 0.9354 - val_acc: 0.6768\n",
            "Epoch 6/12\n",
            "40000/40000 [==============================] - 9s 234us/step - loss: 0.8106 - acc: 0.7183 - val_loss: 0.8558 - val_acc: 0.7004\n",
            "Epoch 7/12\n",
            "40000/40000 [==============================] - 9s 234us/step - loss: 0.7437 - acc: 0.7420 - val_loss: 0.8056 - val_acc: 0.7248\n",
            "Epoch 8/12\n",
            "40000/40000 [==============================] - 9s 236us/step - loss: 0.6794 - acc: 0.7611 - val_loss: 0.8156 - val_acc: 0.7236\n",
            "Epoch 9/12\n",
            "40000/40000 [==============================] - 9s 235us/step - loss: 0.6239 - acc: 0.7827 - val_loss: 0.7854 - val_acc: 0.7449\n",
            "Epoch 10/12\n",
            "40000/40000 [==============================] - 10s 239us/step - loss: 0.5826 - acc: 0.7990 - val_loss: 0.8189 - val_acc: 0.7316\n",
            "Epoch 11/12\n",
            "40000/40000 [==============================] - 9s 237us/step - loss: 0.5482 - acc: 0.8094 - val_loss: 0.9440 - val_acc: 0.7443\n",
            "Epoch 12/12\n",
            "40000/40000 [==============================] - 9s 234us/step - loss: 0.5244 - acc: 0.8182 - val_loss: 0.9096 - val_acc: 0.7538\n",
            "Test loss:  0.90449974360466\n",
            "Test accuracy:  0.7545\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oDshk9CkermA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Data Augumentationによる改善"
      ]
    },
    {
      "metadata": {
        "id": "jT5-DxQ1eZE6",
        "colab_type": "code",
        "outputId": "b7973750-40c4-4126-da1a-c3274d63f9c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "\n",
        "class Trainer():\n",
        "  \n",
        "  def __init__(self, model, loss, optimizer):\n",
        "    self._target = model\n",
        "    self._target.compile(loss=loss, optimizer=optimizer,\n",
        "                        metrics=[\"accuracy\"])\n",
        "    self.verbose =1\n",
        "    self.log_dir = os.path.join(\"drive/My Drive/Colab Notebooks/chokkan_deeplearning\", \"logdir\")\n",
        "    self.model_file_name = \"model_file.hdf5\"\n",
        "    \n",
        "  def train(self, x_train, y_train, batch_size, epochs, validation_split):\n",
        "    if os.path.exists(self.log_dir):\n",
        "      import shutil\n",
        "      shutil.rmtree(self.log_dir)\n",
        "    os.mkdir(self.log_dir)\n",
        "    \n",
        "    datagen = ImageDataGenerator(\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True)\n",
        "    \n",
        "    datagen.fit(x_train)\n",
        "    \n",
        "    # 検証データのためのsplit\n",
        "    indices = np.arange(x_train.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    validation_size = int(x_train.shape[0] * validation_split)\n",
        "    x_train, x_valid = x_train[indices[:-validation_size], :], x_train[indices[-validation_size:],:]\n",
        "    y_train, y_valid = y_train[indices[:-validation_size], :], y_train[indices[-validation_size:],:]\n",
        "    \n",
        "    \n",
        "    self._target.fit_generator(\n",
        "    datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "    steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=(x_valid, y_valid),\n",
        "    callbacks=[TensorBoard(log_dir=self.log_dir),\n",
        "              ModelCheckpoint(os.path.join(self.log_dir,\n",
        "                                          self.model_file_name), save_best_only=True)],\n",
        "    verbose=self.verbose,\n",
        "    workers=4\n",
        "    )\n",
        "    \n",
        "# モデル作成\n",
        "model = network(dataset.image_shape, dataset.num_classes)\n",
        "\n",
        "# 学習\n",
        "x_train, y_train, x_test, y_test = dataset.get_batch()\n",
        "trainer = Trainer(model, loss=\"categorical_crossentropy\",\n",
        "                 optimizer=RMSprop())\n",
        "trainer.train(x_train, y_train, batch_size=128, epochs=15,\n",
        "             validation_split=0.2)\n",
        "\n",
        "# 評価\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss: \", score[0])\n",
        "print(\"Test accuracy: \", score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "312/312 [==============================] - 32s 103ms/step - loss: 1.8883 - acc: 0.3102 - val_loss: 1.4418 - val_acc: 0.4777\n",
            "Epoch 2/15\n",
            "312/312 [==============================] - 31s 98ms/step - loss: 1.5188 - acc: 0.4539 - val_loss: 1.2843 - val_acc: 0.5410\n",
            "Epoch 3/15\n",
            "312/312 [==============================] - 30s 98ms/step - loss: 1.3413 - acc: 0.5211 - val_loss: 1.1984 - val_acc: 0.5798\n",
            "Epoch 4/15\n",
            "312/312 [==============================] - 31s 98ms/step - loss: 1.2259 - acc: 0.5636 - val_loss: 1.0359 - val_acc: 0.6339\n",
            "Epoch 5/15\n",
            "312/312 [==============================] - 30s 97ms/step - loss: 1.1339 - acc: 0.5970 - val_loss: 1.0013 - val_acc: 0.6459\n",
            "Epoch 6/15\n",
            "312/312 [==============================] - 31s 98ms/step - loss: 1.0575 - acc: 0.6304 - val_loss: 0.9328 - val_acc: 0.6716\n",
            "Epoch 7/15\n",
            "312/312 [==============================] - 30s 98ms/step - loss: 1.0077 - acc: 0.6473 - val_loss: 0.8040 - val_acc: 0.7167\n",
            "Epoch 8/15\n",
            "312/312 [==============================] - 30s 97ms/step - loss: 0.9555 - acc: 0.6633 - val_loss: 0.7997 - val_acc: 0.7220\n",
            "Epoch 9/15\n",
            "312/312 [==============================] - 30s 97ms/step - loss: 0.9158 - acc: 0.6814 - val_loss: 0.8476 - val_acc: 0.7102\n",
            "Epoch 10/15\n",
            "312/312 [==============================] - 30s 96ms/step - loss: 0.8902 - acc: 0.6917 - val_loss: 0.8406 - val_acc: 0.7249\n",
            "Epoch 11/15\n",
            "312/312 [==============================] - 31s 98ms/step - loss: 0.8681 - acc: 0.7003 - val_loss: 0.8152 - val_acc: 0.7242\n",
            "Epoch 12/15\n",
            "312/312 [==============================] - 30s 98ms/step - loss: 0.8551 - acc: 0.7060 - val_loss: 0.8330 - val_acc: 0.7342\n",
            "Epoch 13/15\n",
            "312/312 [==============================] - 31s 99ms/step - loss: 0.8380 - acc: 0.7119 - val_loss: 0.7943 - val_acc: 0.7324\n",
            "Epoch 14/15\n",
            "312/312 [==============================] - 30s 97ms/step - loss: 0.8230 - acc: 0.7158 - val_loss: 0.7605 - val_acc: 0.7471\n",
            "Epoch 15/15\n",
            "312/312 [==============================] - 30s 97ms/step - loss: 0.8248 - acc: 0.7200 - val_loss: 0.8225 - val_acc: 0.7370\n",
            "Test loss:  0.8246774942398071\n",
            "Test accuracy:  0.7359\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WPLkN__Rh8Ec",
        "colab_type": "code",
        "outputId": "4f663185-9eaa-4087-967e-a6a6b209fdd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import pathlib\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from keras.models import load_model\n",
        "\n",
        "model_path = \"logdir/model_file.hdf5\"\n",
        "images_folder = \"drive/My Drive/Colab Notebooks/chokkan_deeplearning/sample_images\"\n",
        "\n",
        "#  モデルの読み込み\n",
        "model = load_model(os.path.join(\"drive/My Drive/Colab Notebooks/chokkan_deeplearning\", model_path))\n",
        "image_shape = (32, 32, 3)\n",
        "\n",
        "# 画像の読み込み\n",
        "def crop_resize(image_path):\n",
        "  image = Image.open(image_path)\n",
        "  length = min(image.size)\n",
        "  crop = image.crop((0, 0, length, length))\n",
        "  resized = crop.resize(image_shape[:2])\n",
        "  img = np.array(resized).astype(\"float32\")\n",
        "  img /= 255\n",
        "  return img\n",
        "\n",
        "folder = Path(images_folder)\n",
        "image_paths = [str(f) for f in folder.glob(\"*.png\")]\n",
        "images = [crop_resize(p) for p in image_paths]\n",
        "images = np.asarray(images)\n",
        "\n",
        "predicted = model.predict_classes(images)\n",
        "\n",
        "# 判定できなかった場合に出力\n",
        "assert predicted[0] == 3, \"image should be cat.\"\n",
        "assert predicted[1] == 5, \"image should be dog.\"\n",
        "\n",
        "print(\"You can detect cat&dog!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You can detect cat&dog!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ioJ16u-1mmpV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 大規模な画像認識のための非常に深いネットワーク"
      ]
    },
    {
      "metadata": {
        "id": "UJzT24sems4c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### VGG16"
      ]
    },
    {
      "metadata": {
        "id": "NP5RyCFupHGZ",
        "colab_type": "code",
        "outputId": "ffe534e4-79a3-4b27-f878-80c2401fb311",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
        "import keras.preprocessing.image as Image\n",
        "import numpy as np\n",
        "\n",
        "model = VGG16(weights=\"imagenet\", include_top=True)\n",
        "\n",
        "image_path = \"drive/My Drive/Colab Notebooks/chokkan_deeplearning/sample_images_pretrain/steaming_train.png\"\n",
        "image = Image.load_img(image_path, target_size=(224, 224))\n",
        "x = Image.img_to_array(image)\n",
        "x = np.expand_dims(x, axis=0) # バッチサイズのための次元を追加\n",
        "x = preprocess_input(x)\n",
        "\n",
        "result = model.predict(x)\n",
        "result = decode_predictions(result, top=3)[0]\n",
        "print(result[0][1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\n",
            "40960/35363 [==================================] - 0s 5us/step\n",
            "steam_locomotive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "K9GtTkSmulzB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 学習済みモデルを特徴抽出器として活用する"
      ]
    },
    {
      "metadata": {
        "id": "orUZvW3ptytb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.applications.vgg19 import VGG19\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.vgg19 import preprocess_input\n",
        "from keras.models import Model\n",
        "import numpy as np\n",
        "\n",
        "base_model = VGG19(weights=\"imagenet\")\n",
        "model = Model(inputs=base_model.input,\n",
        "             outputs=base_model.get_layer(\"block4_pool\").output)\n",
        "\n",
        "img_path =  \"drive/My Drive/Colab Notebooks/chokkan_deeplearning/sample_images_pretrain/elephant.jpg\"\n",
        "img = image.load_img(img_path, target_size=(224, 224))\n",
        "x = Image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0) # バッチサイズのための次元を追加\n",
        "x = preprocess_input(x)\n",
        "\n",
        "result = model.predict(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BUmB58JRERi6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Inception-v3を用いた転移学習"
      ]
    },
    {
      "metadata": {
        "id": "1jW-nFzxDM2V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, GlobalAveragePooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "# 基本となる学習済みモデル\n",
        "base_model = InceptionV3(weights=\"imagenet\", include_top=False)\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(200, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# 付け加えた層のみ学習\n",
        "for layer in base_model.layers:\n",
        "  layer.trainable = False\n",
        "  \n",
        "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")\n",
        "\n",
        "#model.fit_generator(...)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ntseGPLYGkSX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 第４章：GANとWaveNet\n",
        "\n",
        "外部ライブラリに依存し過ぎなので、https://github.com/eriklindernoren/Keras-GAN を参考にした"
      ]
    },
    {
      "metadata": {
        "id": "EgbjOq1kPNBy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## DCGANによるMNIST生成"
      ]
    },
    {
      "metadata": {
        "id": "98B1qmIXMFKq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class DCGAN():\n",
        "  def __init__(self):\n",
        "    # 入力サイズ\n",
        "    self.img_rows = 28\n",
        "    self.img_cols = 28\n",
        "    self.channels = 1\n",
        "    self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "    self.latent_dim = 100\n",
        "    \n",
        "    optimizer = Adam(0.0002, 0.5)\n",
        "    \n",
        "    # 識別モデルの構築\n",
        "    self.discriminator = self.build_discriminator()\n",
        "    self.discriminator.compile(loss='binary_crossentropy',\n",
        "                              optimizer=optimizer,\n",
        "                              metrics=['accuracy'])\n",
        "    \n",
        "    # 生成モデルの構築\n",
        "    self.generator = self.build_generator()\n",
        "    \n",
        "    # 生成モデルの入力は一様分布Z\n",
        "    z = Input(shape=(self.latent_dim,))\n",
        "    img = self.generator(z)\n",
        "    \n",
        "    #　結合モデルでは生成モデルのみ学習させる\n",
        "    self.discriminator.trainable = False\n",
        "    \n",
        "    # 識別モデルの判定結果\n",
        "    valid = self.discriminator(img)\n",
        "    \n",
        "    # 結合モデル\n",
        "    self.combined = Model(z, valid)\n",
        "    self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "    \n",
        "  def build_generator(self):\n",
        "    \n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Dense(128*7*7, activation='relu', input_dim=self.latent_dim))\n",
        "    model.add(Reshape((7, 7, 128)))\n",
        "    model.add(UpSampling2D())\n",
        "    model.add(Conv2D(128, kernel_size=3, padding='same'))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(UpSampling2D())\n",
        "    model.add(Conv2D(64, kernel_size=3, padding='same'))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Conv2D(self.channels, kernel_size=3, padding=\"same\"))\n",
        "    model.add(Activation(\"tanh\"))\n",
        "    \n",
        "    model.summary()\n",
        "    \n",
        "    noise = Input(shape=(self.latent_dim,))\n",
        "    img = model(noise)\n",
        "    \n",
        "    return Model(noise, img)\n",
        "  \n",
        "  def build_discriminator(self):\n",
        "    \n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
        "    model.add(ZeroPadding2D(padding=((0, 1), (0, 1))))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    \n",
        "    model.summary()\n",
        "    \n",
        "    img = Input(shape=self.img_shape)\n",
        "    validity = model(img)\n",
        "    \n",
        "    return Model(img, validity)\n",
        "  \n",
        "  def train(self, iterations, batch_size=128, save_interval=50):\n",
        "    \n",
        "    # データセットの読み込み\n",
        "    (X_train, _), (_, _) = mnist.load_data()\n",
        "    \n",
        "    # -1~1で正規化\n",
        "    X_train = X_train / 127.5 - 1.\n",
        "    X_train = np.expand_dims(X_train, axis=3)\n",
        "    \n",
        "    valid = np.ones((batch_size, 1))\n",
        "    fake = np.zeros((batch_size, 1))\n",
        "    \n",
        "    for iteration in range(iterations):\n",
        "      \n",
        "      # ----------------------\n",
        "      # 識別モデルの学習\n",
        "      # ----------------------\n",
        "      \n",
        "      # 画像の取り出し\n",
        "      idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "      imgs = X_train[idx]\n",
        "      \n",
        "      # ノイズ発生および画像生成\n",
        "      noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "      gen_imgs = self.generator.predict(noise)\n",
        "      \n",
        "      # 学習\n",
        "      d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
        "      d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
        "      d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "      \n",
        "      # ---------------------\n",
        "      # 生成モデルの学習\n",
        "      # ---------------------\n",
        "      \n",
        "      # 騙せたときに1を返してほしい\n",
        "      g_loss = self.combined.train_on_batch(noise, valid)\n",
        "     \n",
        "      if iteration % save_interval == 0:\n",
        "        print(\"%d[D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (iteration, d_loss[0], 100*d_loss[1], g_loss))\n",
        "        self.save_imgs(iteration)\n",
        "      \n",
        "  def save_imgs(self, iteration):\n",
        "    r, c = 5, 5\n",
        "    noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
        "    gen_imgs = self.generator.predict(noise)\n",
        "    \n",
        "    # 画像を0-1にスケーリング\n",
        "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "    \n",
        "    fig, axs = plt.subplots(r, c)\n",
        "    cnt = 0\n",
        "    for i in range(r):\n",
        "      for j in range(c):\n",
        "        axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n",
        "        axs[i, j].axis('off')\n",
        "        cnt += 1\n",
        "    fig.savefig(\"drive/My Drive/Colab Notebooks/chokkan_deeplearning/dcgan_mnist/mnist_%d.png\" % iteration)\n",
        "    plt.close()   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wgUfDeoampGx",
        "colab_type": "code",
        "outputId": "77448d1e-98e7-4dfc-ea12-a9f0a46ac654",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2689
        }
      },
      "cell_type": "code",
      "source": [
        "dcgan = DCGAN()\n",
        "dcgan.train(iterations=4000, batch_size=32, save_interval=50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_26 (Conv2D)           (None, 14, 14, 32)        320       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_17 (LeakyReLU)   (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 7, 7, 64)          18496     \n",
            "_________________________________________________________________\n",
            "zero_padding2d_5 (ZeroPaddin (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 8, 8, 64)          256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_18 (LeakyReLU)   (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 4, 4, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 4, 4, 128)         512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_19 (LeakyReLU)   (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_29 (Conv2D)           (None, 4, 4, 256)         295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 4, 4, 256)         1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_20 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_20 (Dropout)         (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 4097      \n",
            "=================================================================\n",
            "Total params: 393,729\n",
            "Trainable params: 392,833\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_9 (Dense)              (None, 6272)              633472    \n",
            "_________________________________________________________________\n",
            "reshape_4 (Reshape)          (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_7 (UpSampling2 (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_30 (Conv2D)           (None, 14, 14, 128)       147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_22 (Batc (None, 14, 14, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_8 (UpSampling2 (None, 28, 28, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_31 (Conv2D)           (None, 28, 28, 64)        73792     \n",
            "_________________________________________________________________\n",
            "batch_normalization_23 (Batc (None, 28, 28, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_32 (Conv2D)           (None, 28, 28, 1)         577       \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 28, 28, 1)         0         \n",
            "=================================================================\n",
            "Total params: 856,193\n",
            "Trainable params: 855,809\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  sample_weight: User-provided `sample_weight` argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0[D loss: 1.147271, acc.: 39.06%] [G loss: 0.492921]\n",
            "50[D loss: 0.789120, acc.: 48.44%] [G loss: 1.824549]\n",
            "100[D loss: 0.819251, acc.: 56.25%] [G loss: 1.229795]\n",
            "150[D loss: 0.924419, acc.: 43.75%] [G loss: 1.149137]\n",
            "200[D loss: 0.794385, acc.: 56.25%] [G loss: 1.144859]\n",
            "250[D loss: 0.783535, acc.: 54.69%] [G loss: 1.209517]\n",
            "300[D loss: 0.796604, acc.: 48.44%] [G loss: 0.961871]\n",
            "350[D loss: 0.750702, acc.: 54.69%] [G loss: 1.111688]\n",
            "400[D loss: 0.660403, acc.: 64.06%] [G loss: 0.957798]\n",
            "450[D loss: 0.838665, acc.: 43.75%] [G loss: 0.993033]\n",
            "500[D loss: 0.769370, acc.: 60.94%] [G loss: 1.059499]\n",
            "550[D loss: 0.759797, acc.: 57.81%] [G loss: 1.051556]\n",
            "600[D loss: 0.673682, acc.: 59.38%] [G loss: 1.163917]\n",
            "650[D loss: 0.835735, acc.: 37.50%] [G loss: 1.029230]\n",
            "700[D loss: 0.755871, acc.: 53.12%] [G loss: 1.018878]\n",
            "750[D loss: 0.674510, acc.: 59.38%] [G loss: 1.111059]\n",
            "800[D loss: 0.705506, acc.: 56.25%] [G loss: 1.085922]\n",
            "850[D loss: 0.694029, acc.: 56.25%] [G loss: 0.864191]\n",
            "900[D loss: 0.597656, acc.: 70.31%] [G loss: 1.129182]\n",
            "950[D loss: 0.662743, acc.: 67.19%] [G loss: 1.121566]\n",
            "1000[D loss: 0.676016, acc.: 54.69%] [G loss: 0.902607]\n",
            "1050[D loss: 0.686895, acc.: 60.94%] [G loss: 0.983043]\n",
            "1100[D loss: 0.775701, acc.: 51.56%] [G loss: 1.114795]\n",
            "1150[D loss: 0.812491, acc.: 43.75%] [G loss: 1.051205]\n",
            "1200[D loss: 0.780252, acc.: 56.25%] [G loss: 1.199407]\n",
            "1250[D loss: 0.714033, acc.: 51.56%] [G loss: 1.104353]\n",
            "1300[D loss: 0.722257, acc.: 46.88%] [G loss: 0.916217]\n",
            "1350[D loss: 0.732193, acc.: 50.00%] [G loss: 1.085733]\n",
            "1400[D loss: 0.685465, acc.: 67.19%] [G loss: 0.933265]\n",
            "1450[D loss: 0.662104, acc.: 64.06%] [G loss: 0.960232]\n",
            "1500[D loss: 0.685483, acc.: 59.38%] [G loss: 1.059554]\n",
            "1550[D loss: 0.632898, acc.: 62.50%] [G loss: 1.050061]\n",
            "1600[D loss: 0.719462, acc.: 60.94%] [G loss: 0.957061]\n",
            "1650[D loss: 0.710896, acc.: 62.50%] [G loss: 0.993057]\n",
            "1700[D loss: 0.589424, acc.: 67.19%] [G loss: 0.887305]\n",
            "1750[D loss: 0.649752, acc.: 62.50%] [G loss: 1.101094]\n",
            "1800[D loss: 0.767628, acc.: 50.00%] [G loss: 0.949166]\n",
            "1850[D loss: 0.723529, acc.: 54.69%] [G loss: 0.993335]\n",
            "1900[D loss: 0.634476, acc.: 60.94%] [G loss: 1.104787]\n",
            "1950[D loss: 0.674272, acc.: 62.50%] [G loss: 1.094388]\n",
            "2000[D loss: 0.656223, acc.: 56.25%] [G loss: 0.972230]\n",
            "2050[D loss: 0.717224, acc.: 53.12%] [G loss: 1.041815]\n",
            "2100[D loss: 0.651419, acc.: 62.50%] [G loss: 1.031345]\n",
            "2150[D loss: 0.710316, acc.: 53.12%] [G loss: 0.856730]\n",
            "2200[D loss: 0.703514, acc.: 56.25%] [G loss: 1.046403]\n",
            "2250[D loss: 0.682146, acc.: 57.81%] [G loss: 1.002180]\n",
            "2300[D loss: 0.731586, acc.: 53.12%] [G loss: 1.064353]\n",
            "2350[D loss: 0.714810, acc.: 53.12%] [G loss: 0.995804]\n",
            "2400[D loss: 0.637806, acc.: 62.50%] [G loss: 0.945019]\n",
            "2450[D loss: 0.712352, acc.: 56.25%] [G loss: 0.978463]\n",
            "2500[D loss: 0.772910, acc.: 45.31%] [G loss: 0.976227]\n",
            "2550[D loss: 0.636909, acc.: 59.38%] [G loss: 1.186325]\n",
            "2600[D loss: 0.578645, acc.: 73.44%] [G loss: 0.955263]\n",
            "2650[D loss: 0.584318, acc.: 70.31%] [G loss: 1.215030]\n",
            "2700[D loss: 0.716818, acc.: 54.69%] [G loss: 0.917244]\n",
            "2750[D loss: 0.756010, acc.: 48.44%] [G loss: 1.089552]\n",
            "2800[D loss: 0.684105, acc.: 64.06%] [G loss: 0.937579]\n",
            "2850[D loss: 0.704303, acc.: 54.69%] [G loss: 1.022288]\n",
            "2900[D loss: 0.636171, acc.: 65.62%] [G loss: 1.157617]\n",
            "2950[D loss: 0.590885, acc.: 68.75%] [G loss: 1.063012]\n",
            "3000[D loss: 0.592635, acc.: 65.62%] [G loss: 0.895928]\n",
            "3050[D loss: 0.613921, acc.: 68.75%] [G loss: 1.000343]\n",
            "3100[D loss: 0.638913, acc.: 68.75%] [G loss: 1.044507]\n",
            "3150[D loss: 0.663207, acc.: 60.94%] [G loss: 1.010108]\n",
            "3200[D loss: 0.635813, acc.: 62.50%] [G loss: 0.837516]\n",
            "3250[D loss: 0.651640, acc.: 56.25%] [G loss: 0.933273]\n",
            "3300[D loss: 0.635416, acc.: 62.50%] [G loss: 1.018734]\n",
            "3350[D loss: 0.607259, acc.: 65.62%] [G loss: 1.065849]\n",
            "3400[D loss: 0.710230, acc.: 59.38%] [G loss: 0.997708]\n",
            "3450[D loss: 0.676049, acc.: 59.38%] [G loss: 1.133780]\n",
            "3500[D loss: 0.681436, acc.: 62.50%] [G loss: 0.881905]\n",
            "3550[D loss: 0.701035, acc.: 53.12%] [G loss: 0.988615]\n",
            "3600[D loss: 0.775568, acc.: 48.44%] [G loss: 0.952617]\n",
            "3650[D loss: 0.698578, acc.: 54.69%] [G loss: 1.065085]\n",
            "3700[D loss: 0.662009, acc.: 62.50%] [G loss: 1.017173]\n",
            "3750[D loss: 0.709377, acc.: 57.81%] [G loss: 0.964972]\n",
            "3800[D loss: 0.706534, acc.: 54.69%] [G loss: 0.967857]\n",
            "3850[D loss: 0.800033, acc.: 39.06%] [G loss: 1.034319]\n",
            "3900[D loss: 0.722825, acc.: 46.88%] [G loss: 0.994223]\n",
            "3950[D loss: 0.626104, acc.: 65.62%] [G loss: 0.933953]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Z4U4rJGXSCFL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## DCGANによるCIFAR10生成"
      ]
    },
    {
      "metadata": {
        "id": "227NbAmIm2L4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import cifar10\n",
        "from keras.layers import Input, Dense, Reshape, Flatten\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l1_l2\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class DCGAN_CIFAR():\n",
        "  def __init__(self):\n",
        "    # 入力サイズ\n",
        "    self.img_rows = 32\n",
        "    self.img_cols = 32\n",
        "    self.channels = 3\n",
        "    self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "    self.latent_dim = 100\n",
        "    \n",
        "    optimizer = Adam(0.0002, 0.5)\n",
        "    \n",
        "    # 識別モデルの構築\n",
        "    self.discriminator = self.build_discriminator()\n",
        "    self.discriminator.compile(loss='binary_crossentropy',\n",
        "                              optimizer=optimizer,\n",
        "                              metrics=['accuracy'])\n",
        "    \n",
        "    # 生成モデルの構築\n",
        "    self.generator = self.build_generator()\n",
        "    \n",
        "    # 生成モデルの入力は一様分布Z\n",
        "    z = Input(shape=(self.latent_dim,))\n",
        "    img = self.generator(z)\n",
        "    \n",
        "    #　結合モデルでは生成モデルのみ学習させる\n",
        "    self.discriminator.trainable = False\n",
        "    \n",
        "    # 識別モデルの判定結果\n",
        "    valid = self.discriminator(img)\n",
        "    \n",
        "    # 結合モデル\n",
        "    self.combined = Model(z, valid)\n",
        "    self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "    \n",
        "  def build_generator(self):\n",
        "    \n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Dense(128*8*8, activation='relu', input_dim=self.latent_dim))\n",
        "    model.add(Reshape((8, 8, 128)))\n",
        "    model.add(UpSampling2D())\n",
        "    model.add(Conv2D(128, kernel_size=3, padding='same'))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(UpSampling2D())\n",
        "    model.add(Conv2D(64, kernel_size=3, padding='same'))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Conv2D(self.channels, kernel_size=3, padding=\"same\"))\n",
        "    model.add(Activation(\"tanh\"))\n",
        "    \n",
        "    model.summary()\n",
        "    \n",
        "    noise = Input(shape=(self.latent_dim,))\n",
        "    img = model(noise)\n",
        "    \n",
        "    return Model(noise, img)\n",
        "  \n",
        "  def build_discriminator(self):\n",
        "    \n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
        "    model.add(ZeroPadding2D(padding=((0, 1), (0, 1))))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    \n",
        "    model.summary()\n",
        "    \n",
        "    img = Input(shape=self.img_shape)\n",
        "    validity = model(img)\n",
        "    \n",
        "    return Model(img, validity)\n",
        "  def train(self, iterations, batch_size=128, save_interval=50):\n",
        "    \n",
        "    # データセットの読み込み\n",
        "    (X_train, _), (_, _) = cifar10.load_data()\n",
        "    \n",
        "    # -1~1で正規化\n",
        "    X_train = X_train / 127.5 - 1.\n",
        "    \n",
        "    valid = np.ones((batch_size, 1))\n",
        "    fake = np.zeros((batch_size, 1))\n",
        "    \n",
        "    for iteration in range(iterations):\n",
        "      \n",
        "      # ----------------------\n",
        "      # 識別モデルの学習\n",
        "      # ----------------------\n",
        "      \n",
        "      # 画像の取り出し\n",
        "      idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "      imgs = X_train[idx]\n",
        "      \n",
        "      # ノイズ発生および画像生成\n",
        "      noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "      gen_imgs = self.generator.predict(noise)\n",
        "      \n",
        "      # 学習\n",
        "      d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
        "      d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
        "      d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "      \n",
        "      # ---------------------\n",
        "      # 生成モデルの学習\n",
        "      # ---------------------\n",
        "      \n",
        "      # 騙せたときに1を返してほしい\n",
        "      g_loss = self.combined.train_on_batch(noise, valid)\n",
        "     \n",
        "      if iteration % save_interval == 0:\n",
        "        print(\"%d[D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (iteration, d_loss[0], 100*d_loss[1], g_loss))\n",
        "        self.save_imgs(iteration)\n",
        "      \n",
        "  def save_imgs(self, iteration):\n",
        "    r, c = 5, 5\n",
        "    noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
        "    gen_imgs = self.generator.predict(noise)\n",
        "    \n",
        "    # 画像を0-1にスケーリング\n",
        "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "    \n",
        "    fig, axs = plt.subplots(r, c)\n",
        "    cnt = 0\n",
        "    for i in range(r):\n",
        "      for j in range(c):\n",
        "        axs[i, j].imshow(gen_imgs[cnt, :, :, :])\n",
        "        axs[i, j].axis('off')\n",
        "        cnt += 1\n",
        "    fig.savefig(\"drive/My Drive/Colab Notebooks/chokkan_deeplearning/dcgan_cifar10/cifar_%d.png\" % iteration)\n",
        "    plt.close()   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tlrn1lNISh31",
        "colab_type": "code",
        "outputId": "4ae6d05a-8b5a-4f9d-a060-ae55e131a64a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4049
        }
      },
      "cell_type": "code",
      "source": [
        "dcgan = DCGAN_CIFAR()\n",
        "dcgan.train(iterations=8000, batch_size=32, save_interval=50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_75 (Conv2D)           (None, 16, 16, 32)        896       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_47 (LeakyReLU)   (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_29 (Dropout)         (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_76 (Conv2D)           (None, 8, 8, 64)          18496     \n",
            "_________________________________________________________________\n",
            "zero_padding2d_8 (ZeroPaddin (None, 9, 9, 64)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_44 (Batc (None, 9, 9, 64)          256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_48 (LeakyReLU)   (None, 9, 9, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_30 (Dropout)         (None, 9, 9, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_77 (Conv2D)           (None, 5, 5, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_45 (Batc (None, 5, 5, 128)         512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_49 (LeakyReLU)   (None, 5, 5, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_31 (Dropout)         (None, 5, 5, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_78 (Conv2D)           (None, 5, 5, 256)         295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_46 (Batc (None, 5, 5, 256)         1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_50 (LeakyReLU)   (None, 5, 5, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_32 (Dropout)         (None, 5, 5, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_13 (Flatten)         (None, 6400)              0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 1)                 6401      \n",
            "=================================================================\n",
            "Total params: 396,609\n",
            "Trainable params: 395,713\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_20 (Dense)             (None, 8192)              827392    \n",
            "_________________________________________________________________\n",
            "reshape_12 (Reshape)         (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_16 (UpSampling (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_79 (Conv2D)           (None, 16, 16, 128)       147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_47 (Batc (None, 16, 16, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_25 (Activation)   (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_17 (UpSampling (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_80 (Conv2D)           (None, 32, 32, 64)        73792     \n",
            "_________________________________________________________________\n",
            "batch_normalization_48 (Batc (None, 32, 32, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_26 (Activation)   (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_81 (Conv2D)           (None, 32, 32, 3)         1731      \n",
            "_________________________________________________________________\n",
            "activation_27 (Activation)   (None, 32, 32, 3)         0         \n",
            "=================================================================\n",
            "Total params: 1,051,267\n",
            "Trainable params: 1,050,883\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0[D loss: 0.993627, acc.: 45.31%] [G loss: 0.490350]\n",
            "50[D loss: 0.552495, acc.: 71.88%] [G loss: 1.068514]\n",
            "100[D loss: 0.663663, acc.: 60.94%] [G loss: 1.919299]\n",
            "150[D loss: 0.427101, acc.: 81.25%] [G loss: 2.052676]\n",
            "200[D loss: 0.742191, acc.: 60.94%] [G loss: 1.548181]\n",
            "250[D loss: 1.124821, acc.: 46.88%] [G loss: 1.213459]\n",
            "300[D loss: 0.360107, acc.: 76.56%] [G loss: 2.002065]\n",
            "350[D loss: 0.604558, acc.: 68.75%] [G loss: 1.639582]\n",
            "400[D loss: 0.860352, acc.: 50.00%] [G loss: 1.087935]\n",
            "450[D loss: 0.519880, acc.: 75.00%] [G loss: 1.436579]\n",
            "500[D loss: 0.577512, acc.: 68.75%] [G loss: 1.500663]\n",
            "550[D loss: 0.591313, acc.: 64.06%] [G loss: 1.499802]\n",
            "600[D loss: 0.734878, acc.: 59.38%] [G loss: 1.341720]\n",
            "650[D loss: 0.830927, acc.: 53.12%] [G loss: 0.786268]\n",
            "700[D loss: 0.738660, acc.: 56.25%] [G loss: 1.365353]\n",
            "750[D loss: 0.553006, acc.: 68.75%] [G loss: 0.978468]\n",
            "800[D loss: 0.949355, acc.: 45.31%] [G loss: 1.007817]\n",
            "850[D loss: 0.571557, acc.: 73.44%] [G loss: 0.875395]\n",
            "900[D loss: 0.691920, acc.: 62.50%] [G loss: 0.996779]\n",
            "950[D loss: 0.578342, acc.: 65.62%] [G loss: 1.614569]\n",
            "1000[D loss: 0.794388, acc.: 45.31%] [G loss: 0.956459]\n",
            "1050[D loss: 0.837386, acc.: 48.44%] [G loss: 0.896627]\n",
            "1100[D loss: 0.673527, acc.: 60.94%] [G loss: 1.207337]\n",
            "1150[D loss: 0.568168, acc.: 65.62%] [G loss: 0.882217]\n",
            "1200[D loss: 0.676259, acc.: 60.94%] [G loss: 1.050317]\n",
            "1250[D loss: 0.944942, acc.: 42.19%] [G loss: 0.877661]\n",
            "1300[D loss: 0.698189, acc.: 57.81%] [G loss: 1.076361]\n",
            "1350[D loss: 0.475483, acc.: 82.81%] [G loss: 0.776408]\n",
            "1400[D loss: 0.854781, acc.: 42.19%] [G loss: 1.238320]\n",
            "1450[D loss: 0.501846, acc.: 79.69%] [G loss: 1.077631]\n",
            "1500[D loss: 0.608985, acc.: 65.62%] [G loss: 0.648034]\n",
            "1550[D loss: 0.548347, acc.: 70.31%] [G loss: 0.969681]\n",
            "1600[D loss: 0.633653, acc.: 65.62%] [G loss: 1.191881]\n",
            "1650[D loss: 0.509382, acc.: 76.56%] [G loss: 1.011804]\n",
            "1700[D loss: 0.795684, acc.: 46.88%] [G loss: 0.958289]\n",
            "1750[D loss: 0.649623, acc.: 60.94%] [G loss: 0.809833]\n",
            "1800[D loss: 0.639071, acc.: 68.75%] [G loss: 0.976792]\n",
            "1850[D loss: 0.885713, acc.: 35.94%] [G loss: 0.677510]\n",
            "1900[D loss: 0.545406, acc.: 75.00%] [G loss: 1.223681]\n",
            "1950[D loss: 0.706588, acc.: 56.25%] [G loss: 0.978057]\n",
            "2000[D loss: 0.673787, acc.: 62.50%] [G loss: 0.941121]\n",
            "2050[D loss: 0.785729, acc.: 51.56%] [G loss: 0.708759]\n",
            "2100[D loss: 0.715460, acc.: 54.69%] [G loss: 0.967115]\n",
            "2150[D loss: 0.784601, acc.: 45.31%] [G loss: 0.986364]\n",
            "2200[D loss: 0.756730, acc.: 53.12%] [G loss: 0.562284]\n",
            "2250[D loss: 0.594271, acc.: 67.19%] [G loss: 1.128270]\n",
            "2300[D loss: 0.690865, acc.: 64.06%] [G loss: 0.926987]\n",
            "2350[D loss: 0.588897, acc.: 67.19%] [G loss: 1.023982]\n",
            "2400[D loss: 1.058742, acc.: 21.88%] [G loss: 0.780239]\n",
            "2450[D loss: 0.398858, acc.: 81.25%] [G loss: 1.144431]\n",
            "2500[D loss: 0.588315, acc.: 70.31%] [G loss: 0.934264]\n",
            "2550[D loss: 0.893085, acc.: 32.81%] [G loss: 0.814669]\n",
            "2600[D loss: 0.649404, acc.: 70.31%] [G loss: 1.186458]\n",
            "2650[D loss: 0.606516, acc.: 64.06%] [G loss: 0.812921]\n",
            "2700[D loss: 0.492618, acc.: 82.81%] [G loss: 0.675119]\n",
            "2750[D loss: 0.596063, acc.: 68.75%] [G loss: 0.770557]\n",
            "2800[D loss: 0.680080, acc.: 59.38%] [G loss: 1.736560]\n",
            "2850[D loss: 0.646988, acc.: 57.81%] [G loss: 0.950998]\n",
            "2900[D loss: 0.771947, acc.: 51.56%] [G loss: 0.457786]\n",
            "2950[D loss: 0.756545, acc.: 46.88%] [G loss: 1.321730]\n",
            "3000[D loss: 0.918878, acc.: 35.94%] [G loss: 1.114043]\n",
            "3050[D loss: 0.420068, acc.: 87.50%] [G loss: 0.753053]\n",
            "3100[D loss: 0.939612, acc.: 32.81%] [G loss: 0.940440]\n",
            "3150[D loss: 0.506658, acc.: 84.38%] [G loss: 0.774271]\n",
            "3200[D loss: 0.671069, acc.: 59.38%] [G loss: 0.900304]\n",
            "3250[D loss: 0.514666, acc.: 76.56%] [G loss: 0.683700]\n",
            "3300[D loss: 0.731781, acc.: 46.88%] [G loss: 1.536450]\n",
            "3350[D loss: 0.547019, acc.: 76.56%] [G loss: 0.561001]\n",
            "3400[D loss: 0.694118, acc.: 60.94%] [G loss: 0.811443]\n",
            "3450[D loss: 0.752535, acc.: 51.56%] [G loss: 1.099104]\n",
            "3500[D loss: 0.580506, acc.: 75.00%] [G loss: 0.873324]\n",
            "3550[D loss: 1.092734, acc.: 23.44%] [G loss: 0.938713]\n",
            "3600[D loss: 0.568604, acc.: 71.88%] [G loss: 0.856912]\n",
            "3650[D loss: 0.920518, acc.: 42.19%] [G loss: 0.659164]\n",
            "3700[D loss: 0.760871, acc.: 42.19%] [G loss: 0.735719]\n",
            "3750[D loss: 0.963695, acc.: 25.00%] [G loss: 0.775561]\n",
            "3800[D loss: 0.743237, acc.: 51.56%] [G loss: 0.945582]\n",
            "3850[D loss: 0.582368, acc.: 73.44%] [G loss: 0.799899]\n",
            "3900[D loss: 0.542540, acc.: 76.56%] [G loss: 0.799744]\n",
            "3950[D loss: 0.686267, acc.: 60.94%] [G loss: 1.030633]\n",
            "4000[D loss: 0.575006, acc.: 73.44%] [G loss: 0.681642]\n",
            "4050[D loss: 0.398967, acc.: 90.62%] [G loss: 0.856363]\n",
            "4100[D loss: 0.503529, acc.: 78.12%] [G loss: 1.126076]\n",
            "4150[D loss: 0.820921, acc.: 54.69%] [G loss: 0.902991]\n",
            "4200[D loss: 0.872143, acc.: 35.94%] [G loss: 0.733480]\n",
            "4250[D loss: 0.811472, acc.: 39.06%] [G loss: 0.645829]\n",
            "4300[D loss: 0.668706, acc.: 59.38%] [G loss: 0.566282]\n",
            "4350[D loss: 0.514271, acc.: 76.56%] [G loss: 1.095864]\n",
            "4400[D loss: 0.755117, acc.: 53.12%] [G loss: 0.668173]\n",
            "4450[D loss: 0.641183, acc.: 64.06%] [G loss: 0.478838]\n",
            "4500[D loss: 0.625328, acc.: 67.19%] [G loss: 0.984834]\n",
            "4550[D loss: 0.632786, acc.: 62.50%] [G loss: 0.827799]\n",
            "4600[D loss: 0.687980, acc.: 51.56%] [G loss: 0.469555]\n",
            "4650[D loss: 0.609685, acc.: 67.19%] [G loss: 0.583631]\n",
            "4700[D loss: 0.820812, acc.: 45.31%] [G loss: 0.814698]\n",
            "4750[D loss: 0.810052, acc.: 50.00%] [G loss: 0.668078]\n",
            "4800[D loss: 0.768993, acc.: 46.88%] [G loss: 0.616359]\n",
            "4850[D loss: 0.653041, acc.: 65.62%] [G loss: 0.523911]\n",
            "4900[D loss: 0.636528, acc.: 65.62%] [G loss: 0.798890]\n",
            "4950[D loss: 0.676767, acc.: 59.38%] [G loss: 0.785599]\n",
            "5000[D loss: 0.651184, acc.: 65.62%] [G loss: 0.865003]\n",
            "5050[D loss: 0.648088, acc.: 64.06%] [G loss: 1.069708]\n",
            "5100[D loss: 0.531783, acc.: 71.88%] [G loss: 0.904559]\n",
            "5150[D loss: 0.570120, acc.: 70.31%] [G loss: 0.794107]\n",
            "5200[D loss: 0.793722, acc.: 42.19%] [G loss: 0.796680]\n",
            "5250[D loss: 0.549474, acc.: 78.12%] [G loss: 0.845825]\n",
            "5300[D loss: 0.607266, acc.: 75.00%] [G loss: 0.735176]\n",
            "5350[D loss: 0.742776, acc.: 51.56%] [G loss: 0.861619]\n",
            "5400[D loss: 0.640913, acc.: 68.75%] [G loss: 0.780783]\n",
            "5450[D loss: 0.573761, acc.: 70.31%] [G loss: 0.747327]\n",
            "5500[D loss: 0.757371, acc.: 46.88%] [G loss: 0.735950]\n",
            "5550[D loss: 0.764221, acc.: 45.31%] [G loss: 0.634956]\n",
            "5600[D loss: 0.768719, acc.: 43.75%] [G loss: 0.787968]\n",
            "5650[D loss: 0.678762, acc.: 50.00%] [G loss: 0.552807]\n",
            "5700[D loss: 0.855067, acc.: 32.81%] [G loss: 0.620781]\n",
            "5750[D loss: 0.961080, acc.: 42.19%] [G loss: 0.583061]\n",
            "5800[D loss: 0.669715, acc.: 56.25%] [G loss: 0.875259]\n",
            "5850[D loss: 0.728409, acc.: 46.88%] [G loss: 0.707423]\n",
            "5900[D loss: 0.784605, acc.: 39.06%] [G loss: 0.826379]\n",
            "5950[D loss: 0.694956, acc.: 51.56%] [G loss: 0.818432]\n",
            "6000[D loss: 0.582655, acc.: 71.88%] [G loss: 0.729103]\n",
            "6050[D loss: 0.723500, acc.: 45.31%] [G loss: 0.865825]\n",
            "6100[D loss: 0.566912, acc.: 79.69%] [G loss: 0.994964]\n",
            "6150[D loss: 0.617735, acc.: 73.44%] [G loss: 0.674482]\n",
            "6200[D loss: 0.727203, acc.: 54.69%] [G loss: 0.824281]\n",
            "6250[D loss: 0.755523, acc.: 40.62%] [G loss: 0.782844]\n",
            "6300[D loss: 0.726330, acc.: 51.56%] [G loss: 0.659648]\n",
            "6350[D loss: 0.725931, acc.: 45.31%] [G loss: 0.661877]\n",
            "6400[D loss: 0.636509, acc.: 64.06%] [G loss: 0.759193]\n",
            "6450[D loss: 0.671057, acc.: 57.81%] [G loss: 0.804004]\n",
            "6500[D loss: 0.656883, acc.: 50.00%] [G loss: 0.662560]\n",
            "6550[D loss: 0.632080, acc.: 68.75%] [G loss: 0.784434]\n",
            "6600[D loss: 0.619023, acc.: 65.62%] [G loss: 0.721020]\n",
            "6650[D loss: 0.748604, acc.: 46.88%] [G loss: 0.680923]\n",
            "6700[D loss: 0.744877, acc.: 45.31%] [G loss: 0.603715]\n",
            "6750[D loss: 0.669723, acc.: 60.94%] [G loss: 0.881281]\n",
            "6800[D loss: 0.731002, acc.: 50.00%] [G loss: 0.555086]\n",
            "6850[D loss: 0.727072, acc.: 51.56%] [G loss: 0.716316]\n",
            "6900[D loss: 0.799602, acc.: 43.75%] [G loss: 0.782897]\n",
            "6950[D loss: 0.643103, acc.: 71.88%] [G loss: 0.897230]\n",
            "7000[D loss: 0.636438, acc.: 60.94%] [G loss: 0.785696]\n",
            "7050[D loss: 0.771975, acc.: 40.62%] [G loss: 0.762845]\n",
            "7100[D loss: 0.572989, acc.: 64.06%] [G loss: 0.794542]\n",
            "7150[D loss: 0.671277, acc.: 54.69%] [G loss: 0.860587]\n",
            "7200[D loss: 0.782236, acc.: 45.31%] [G loss: 0.863429]\n",
            "7250[D loss: 0.683842, acc.: 54.69%] [G loss: 0.854035]\n",
            "7300[D loss: 0.786627, acc.: 42.19%] [G loss: 0.670169]\n",
            "7350[D loss: 0.683506, acc.: 54.69%] [G loss: 0.857229]\n",
            "7400[D loss: 0.757780, acc.: 45.31%] [G loss: 0.749021]\n",
            "7450[D loss: 0.587915, acc.: 76.56%] [G loss: 0.641209]\n",
            "7500[D loss: 0.785742, acc.: 42.19%] [G loss: 0.681331]\n",
            "7550[D loss: 0.602424, acc.: 73.44%] [G loss: 0.802328]\n",
            "7600[D loss: 0.740444, acc.: 43.75%] [G loss: 0.810274]\n",
            "7650[D loss: 0.648993, acc.: 57.81%] [G loss: 0.766746]\n",
            "7700[D loss: 0.851470, acc.: 26.56%] [G loss: 0.820595]\n",
            "7750[D loss: 0.664957, acc.: 70.31%] [G loss: 0.877043]\n",
            "7800[D loss: 0.800217, acc.: 39.06%] [G loss: 0.752046]\n",
            "7850[D loss: 0.730188, acc.: 46.88%] [G loss: 0.771236]\n",
            "7900[D loss: 0.749481, acc.: 40.62%] [G loss: 0.846162]\n",
            "7950[D loss: 0.613484, acc.: 62.50%] [G loss: 0.746116]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_PnTtKdIZFzb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 単語分散表現"
      ]
    },
    {
      "metadata": {
        "id": "Zibxt0JkZK1C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## word2vec\n"
      ]
    },
    {
      "metadata": {
        "id": "rVvWx4KnwGWA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Skip-gram"
      ]
    },
    {
      "metadata": {
        "id": "gzwoZCVoSmFX",
        "colab_type": "code",
        "outputId": "68d44986-e3f9-417a-a705-e7d7becefbe5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Dot, Dense, Reshape, Embedding\n",
        "from keras.models import Sequential\n",
        "\n",
        "vocab_size = 5000\n",
        "embed_size = 300\n",
        "\n",
        "# http://cookie-box.hatenablog.com/entry/2018/10/14/184801\n",
        "class SkipGramDiscriminator():\n",
        "  def __init__(self, vocab_size, embed_size):\n",
        "    self.vocab_size = vocab_size #  語彙数\n",
        "    self.embed_size = embed_size # 埋め込み次元数\n",
        "  def create_model(self):\n",
        "    # 中心語ID => 中心語数値ベクトル表現\n",
        "    x0 = Input(shape=(1, ))\n",
        "    y0 = Embedding(self.vocab_size, self.embed_size,\n",
        "                  embeddings_initializer='glorot_uniform')(x0)\n",
        "    y0 = Reshape((self.embed_size, ))(y0)\n",
        "    self.word_embedder = Model(x0, y0)\n",
        "    # 文脈語ID => 文脈語数値ベクトル表現\n",
        "    x1 = Input(shape=(1, ))\n",
        "    y1 = Embedding(self.vocab_size, self.embed_size,\n",
        "                  embeddings_initializer='glorot_uniform')(x1)\n",
        "    y1 = Reshape((self.embed_size, ))(y1)\n",
        "    self.context_embedder = Model(x1, y1)\n",
        "    # 内積 => ロジスティック回帰\n",
        "    y = Dot(axes=-1)([y0, y1])\n",
        "    y = Dense(1, kernel_initializer='glorot_uniform', activation='sigmoid')(y)\n",
        "    self.discriminator = Model(inputs=[x0, x1], outputs=y)\n",
        "    self.discriminator.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    self.discriminator.summary()\n",
        "    \n",
        "from keras.preprocessing.text import * \n",
        "from keras.preprocessing.sequence import skipgrams\n",
        "\n",
        "text = \"I love green eggs and ham .\"\n",
        "\n",
        "# 各単語を整数IDにマッピングする辞書を作成\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "\n",
        "word2id = tokenizer.word_index\n",
        "id2word = {v:k for k, v in word2id.items()}\n",
        "\n",
        "wids = [word2id[w] for w in text_to_word_sequence(text)]\n",
        "pairs, labels = skipgrams(wids, len(word2id), window_size=1)\n",
        "\n",
        "print(len(pairs), len(labels))\n",
        "for i in range(10):\n",
        "  print(\"({:s} ({:d}), {:s}({:d})) -> {:d}\".format(\n",
        "            id2word[pairs[i][0]], pairs[i][0],\n",
        "            id2word[pairs[i][1]], pairs[i][1],\n",
        "            labels[i]))\n",
        "  \n",
        "sg = SkipGramDiscriminator(6, 3)\n",
        "sg.create_model()\n",
        "x0_samples = np.array([[1], [4], [1], [4], [2]]) # 中心語： love,and,love,and,green\n",
        "x1_samples = np.array([[0], [5], [2], [2], [2]]) # 文脈語： i,ham,green,green,green\n",
        "y_samples = sg.discriminator.predict([x0_samples, x1_samples])\n",
        "print(y_samples) # 中心語と文脈語のペアであるかどうかの判定結果（学習まだ）\n",
        "\n",
        "print('中心語の数値ベクトル表現は中心語の Embedding 層の重みそのもの\\n', sg.word_embedder.get_weights())\n",
        "\n",
        "# IDから数値ベクトル表現を取り出せることの確認\n",
        "print('i の数値ベクトル表現: ', sg.word_embedder.predict([[0]]))\n",
        "print('love の数値ベクトル表現', sg.word_embedder.predict([[1]])) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20 20\n",
            "(ham (6), eggs(4)) -> 0\n",
            "(and (5), eggs(4)) -> 1\n",
            "(i (1), i(1)) -> 0\n",
            "(ham (6), and(5)) -> 1\n",
            "(eggs (4), green(3)) -> 1\n",
            "(love (2), eggs(4)) -> 0\n",
            "(eggs (4), love(2)) -> 0\n",
            "(eggs (4), and(5)) -> 1\n",
            "(love (2), green(3)) -> 1\n",
            "(love (2), i(1)) -> 0\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_32 (InputLayer)           (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_33 (InputLayer)           (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_24 (Embedding)        (None, 1, 3)         18          input_32[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_25 (Embedding)        (None, 1, 3)         18          input_33[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "reshape_35 (Reshape)            (None, 3)            0           embedding_24[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "reshape_36 (Reshape)            (None, 3)            0           embedding_25[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dot_7 (Dot)                     (None, 1)            0           reshape_35[0][0]                 \n",
            "                                                                 reshape_36[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_23 (Dense)                (None, 1)            2           dot_7[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 38\n",
            "Trainable params: 38\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "[[0.54055285]\n",
            " [0.4963607 ]\n",
            " [0.5062143 ]\n",
            " [0.51901454]\n",
            " [0.57434756]]\n",
            "中心語の数値ベクトル表現は中心語の Embedding 層の重みそのもの\n",
            " [array([[ 0.7863369 ,  0.72815084, -0.31997436],\n",
            "       [-0.7508456 , -0.24750179, -0.5425505 ],\n",
            "       [-0.7867362 ,  0.710812  ,  0.7613447 ],\n",
            "       [ 0.17927694, -0.08255535,  0.0070312 ],\n",
            "       [ 0.19015789,  0.47959006,  0.36572266],\n",
            "       [ 0.26324594,  0.15091437,  0.3512994 ]], dtype=float32)]\n",
            "i の数値ベクトル表現:  [[ 0.7863369   0.72815084 -0.31997436]]\n",
            "love の数値ベクトル表現 [[-0.7508456  -0.24750179 -0.5425505 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yOULQyhfwJuG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### CBOW"
      ]
    },
    {
      "metadata": {
        "id": "t59SkJrNcvIz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Lambda, Embedding\n",
        "import keras.backend as K\n",
        "\n",
        "vocab_size = 5000\n",
        "embed_size = 300\n",
        "window_size = 1\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embed_size,\n",
        "                                        embeddings_initializer='glorot_uniform',\n",
        "                                        input_length=window_size*2))\n",
        "model.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size, )))\n",
        "model.add(Dense(vocab_size, kernel_initializer='glorot_uniform', activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GLd3jxAr5IfG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### gensimを使って実装"
      ]
    },
    {
      "metadata": {
        "id": "UR7dnbyQh16o",
        "colab_type": "code",
        "outputId": "ea5f9828-f09b-4e53-d1c8-6b0f05fe8c6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/a4/d10c0acc8528d838cda5eede0ee9c784caa598dbf40bd0911ff8d067a7eb/gensim-3.6.0-cp36-cp36m-manylinux1_x86_64.whl (23.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 23.6MB 1.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.6)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.1.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n",
            "Collecting smart-open>=1.2.1 (from gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/1f/6f27e3682124de63ac97a0a5876da6186de6c19410feab66c1543afab055/smart_open-1.7.1.tar.gz\n",
            "Collecting boto>=2.32 (from smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/10/c0b78c27298029e4454a472a1919bde20cb182dab1662cec7f2ca1dcc523/boto-2.49.0-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.4MB 15.9MB/s \n",
            "\u001b[?25hCollecting bz2file (from smart-open>=1.2.1->gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n",
            "Collecting boto3 (from smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/b9/7df67f1775d240ac8d111211f967fa75ecc9968ae79ffa0594e36345445f/boto3-1.9.62-py2.py3-none-any.whl (128kB)\n",
            "\u001b[K    100% |████████████████████████████████| 133kB 27.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2018.11.29)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n",
            "Collecting s3transfer<0.2.0,>=0.1.10 (from boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/14/2a0004d487464d120c9fb85313a75cd3d71a7506955be458eebfe19a6b1d/s3transfer-0.1.13-py2.py3-none-any.whl (59kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 23.3MB/s \n",
            "\u001b[?25hCollecting botocore<1.13.0,>=1.12.62 (from boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e2/77/35e82076e3beb506280f94213a258819378115f174e516ce69b3a2336e1c/botocore-1.12.62-py2.py3-none-any.whl (5.1MB)\n",
            "\u001b[K    100% |████████████████████████████████| 5.1MB 7.1MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.2.1->gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/b7/31/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365/jmespath-0.9.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.62->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n",
            "Collecting docutils>=0.10 (from botocore<1.13.0,>=1.12.62->boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/fa/08e9e6e0e3cbd1d362c3bbee8d01d0aedb2155c4ac112b19ef3cae8eed8d/docutils-0.14-py3-none-any.whl (543kB)\n",
            "\u001b[K    100% |████████████████████████████████| 552kB 26.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: smart-open, bz2file\n",
            "  Running setup.py bdist_wheel for smart-open ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/23/00/44/e5b939f7a80c04e32297dbd6d96fa3065af89ecf57e2b5f89f\n",
            "  Running setup.py bdist_wheel for bz2file ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
            "Successfully built smart-open bz2file\n",
            "Installing collected packages: boto, bz2file, jmespath, docutils, botocore, s3transfer, boto3, smart-open, gensim\n",
            "Successfully installed boto-2.49.0 boto3-1.9.62 botocore-1.12.62 bz2file-0.98 docutils-0.14 gensim-3.6.0 jmespath-0.9.3 s3transfer-0.1.13 smart-open-1.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WUAV0Zj75QAg",
        "colab_type": "code",
        "outputId": "a554ed73-a3bb-451b-954a-ca2866feebed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4236
        }
      },
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "\n",
        "from gensim.models import word2vec\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', \n",
        "                    level=logging.INFO)\n",
        "\n",
        "DATA_DIR = os.path.join('drive/My Drive/Colab Notebooks/chokkan_deeplearning', \"data\")\n",
        "sentences = word2vec.Text8Corpus(os.path.join(DATA_DIR, \"text8\"), 50)\n",
        "model = word2vec.Word2Vec(sentences, size=300, min_count=30)\n",
        "\n",
        "model.init_sims(replace=True)\n",
        "model.save(\"word2vec_gensim.bin\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2018-12-09 08:04:20,737 : INFO : collecting all words and their counts\n",
            "2018-12-09 08:04:21,466 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2018-12-09 08:04:21,709 : INFO : PROGRESS: at sentence #10000, processed 500000 words, keeping 33463 word types\n",
            "2018-12-09 08:04:21,952 : INFO : PROGRESS: at sentence #20000, processed 1000000 words, keeping 52754 word types\n",
            "2018-12-09 08:04:22,174 : INFO : PROGRESS: at sentence #30000, processed 1500000 words, keeping 65588 word types\n",
            "2018-12-09 08:04:22,400 : INFO : PROGRESS: at sentence #40000, processed 2000000 words, keeping 78382 word types\n",
            "2018-12-09 08:04:22,619 : INFO : PROGRESS: at sentence #50000, processed 2500000 words, keeping 88007 word types\n",
            "2018-12-09 08:04:22,828 : INFO : PROGRESS: at sentence #60000, processed 3000000 words, keeping 96644 word types\n",
            "2018-12-09 08:04:23,069 : INFO : PROGRESS: at sentence #70000, processed 3500000 words, keeping 104308 word types\n",
            "2018-12-09 08:04:23,370 : INFO : PROGRESS: at sentence #80000, processed 4000000 words, keeping 111460 word types\n",
            "2018-12-09 08:04:23,668 : INFO : PROGRESS: at sentence #90000, processed 4500000 words, keeping 118751 word types\n",
            "2018-12-09 08:04:23,894 : INFO : PROGRESS: at sentence #100000, processed 5000000 words, keeping 125354 word types\n",
            "2018-12-09 08:04:24,112 : INFO : PROGRESS: at sentence #110000, processed 5500000 words, keeping 133140 word types\n",
            "2018-12-09 08:04:24,335 : INFO : PROGRESS: at sentence #120000, processed 6000000 words, keeping 139565 word types\n",
            "2018-12-09 08:04:24,556 : INFO : PROGRESS: at sentence #130000, processed 6500000 words, keeping 145781 word types\n",
            "2018-12-09 08:04:24,782 : INFO : PROGRESS: at sentence #140000, processed 7000000 words, keeping 151933 word types\n",
            "2018-12-09 08:04:25,015 : INFO : PROGRESS: at sentence #150000, processed 7500000 words, keeping 158045 word types\n",
            "2018-12-09 08:04:25,240 : INFO : PROGRESS: at sentence #160000, processed 8000000 words, keeping 164114 word types\n",
            "2018-12-09 08:04:25,459 : INFO : PROGRESS: at sentence #170000, processed 8500000 words, keeping 171255 word types\n",
            "2018-12-09 08:04:25,689 : INFO : PROGRESS: at sentence #180000, processed 9000000 words, keeping 178162 word types\n",
            "2018-12-09 08:04:25,917 : INFO : PROGRESS: at sentence #190000, processed 9500000 words, keeping 184128 word types\n",
            "2018-12-09 08:04:26,154 : INFO : PROGRESS: at sentence #200000, processed 10000000 words, keeping 189074 word types\n",
            "2018-12-09 08:04:26,383 : INFO : PROGRESS: at sentence #210000, processed 10500000 words, keeping 194510 word types\n",
            "2018-12-09 08:04:26,621 : INFO : PROGRESS: at sentence #220000, processed 11000000 words, keeping 198757 word types\n",
            "2018-12-09 08:04:26,848 : INFO : PROGRESS: at sentence #230000, processed 11500000 words, keeping 203440 word types\n",
            "2018-12-09 08:04:27,096 : INFO : PROGRESS: at sentence #240000, processed 12000000 words, keeping 207894 word types\n",
            "2018-12-09 08:04:27,321 : INFO : PROGRESS: at sentence #250000, processed 12500000 words, keeping 212667 word types\n",
            "2018-12-09 08:04:27,555 : INFO : PROGRESS: at sentence #260000, processed 13000000 words, keeping 217127 word types\n",
            "2018-12-09 08:04:27,785 : INFO : PROGRESS: at sentence #270000, processed 13500000 words, keeping 221415 word types\n",
            "2018-12-09 08:04:28,014 : INFO : PROGRESS: at sentence #280000, processed 14000000 words, keeping 226854 word types\n",
            "2018-12-09 08:04:28,263 : INFO : PROGRESS: at sentence #290000, processed 14500000 words, keeping 231423 word types\n",
            "2018-12-09 08:04:28,491 : INFO : PROGRESS: at sentence #300000, processed 15000000 words, keeping 237390 word types\n",
            "2018-12-09 08:04:28,717 : INFO : PROGRESS: at sentence #310000, processed 15500000 words, keeping 241696 word types\n",
            "2018-12-09 08:04:28,945 : INFO : PROGRESS: at sentence #320000, processed 16000000 words, keeping 245648 word types\n",
            "2018-12-09 08:04:29,182 : INFO : PROGRESS: at sentence #330000, processed 16500000 words, keeping 249620 word types\n",
            "2018-12-09 08:04:29,415 : INFO : PROGRESS: at sentence #340000, processed 17000000 words, keeping 253833 word types\n",
            "2018-12-09 08:04:29,422 : INFO : collected 253854 word types from a corpus of 17005207 raw words and 340105 sentences\n",
            "2018-12-09 08:04:29,427 : INFO : Loading a fresh vocabulary\n",
            "2018-12-09 08:04:29,596 : INFO : effective_min_count=30 retains 25097 unique words (9% of original 253854, drops 228757)\n",
            "2018-12-09 08:04:29,598 : INFO : effective_min_count=30 leaves 16191060 word corpus (95% of original 17005207, drops 814147)\n",
            "2018-12-09 08:04:29,690 : INFO : deleting the raw counts dictionary of 253854 items\n",
            "2018-12-09 08:04:29,702 : INFO : sample=0.001 downsamples 38 most-common words\n",
            "2018-12-09 08:04:29,707 : INFO : downsampling leaves estimated 11928484 word corpus (73.7% of prior 16191060)\n",
            "2018-12-09 08:04:29,782 : INFO : estimated required memory for 25097 words and 300 dimensions: 72781300 bytes\n",
            "2018-12-09 08:04:29,783 : INFO : resetting layer weights\n",
            "2018-12-09 08:04:30,176 : INFO : training model with 3 workers on 25097 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
            "2018-12-09 08:04:31,218 : INFO : EPOCH 1 - PROGRESS: at 2.65% examples, 308096 words/s, in_qsize 4, out_qsize 1\n",
            "2018-12-09 08:04:32,247 : INFO : EPOCH 1 - PROGRESS: at 5.59% examples, 321459 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:04:33,255 : INFO : EPOCH 1 - PROGRESS: at 8.41% examples, 323427 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:04:34,274 : INFO : EPOCH 1 - PROGRESS: at 11.23% examples, 323750 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:04:35,293 : INFO : EPOCH 1 - PROGRESS: at 14.05% examples, 325374 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:04:36,323 : INFO : EPOCH 1 - PROGRESS: at 16.82% examples, 324513 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:04:37,327 : INFO : EPOCH 1 - PROGRESS: at 19.58% examples, 325047 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:04:38,333 : INFO : EPOCH 1 - PROGRESS: at 22.35% examples, 325386 words/s, in_qsize 4, out_qsize 1\n",
            "2018-12-09 08:04:39,338 : INFO : EPOCH 1 - PROGRESS: at 25.11% examples, 326175 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:04:40,341 : INFO : EPOCH 1 - PROGRESS: at 27.87% examples, 326993 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:04:41,372 : INFO : EPOCH 1 - PROGRESS: at 30.58% examples, 326046 words/s, in_qsize 5, out_qsize 1\n",
            "2018-12-09 08:04:42,408 : INFO : EPOCH 1 - PROGRESS: at 33.34% examples, 325680 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:04:43,426 : INFO : EPOCH 1 - PROGRESS: at 36.17% examples, 326451 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:04:44,437 : INFO : EPOCH 1 - PROGRESS: at 38.87% examples, 325876 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:04:45,445 : INFO : EPOCH 1 - PROGRESS: at 41.69% examples, 326442 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:04:46,476 : INFO : EPOCH 1 - PROGRESS: at 44.46% examples, 326182 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:04:47,510 : INFO : EPOCH 1 - PROGRESS: at 47.10% examples, 325131 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:04:48,514 : INFO : EPOCH 1 - PROGRESS: at 49.87% examples, 325340 words/s, in_qsize 6, out_qsize 0\n",
            "2018-12-09 08:04:49,515 : INFO : EPOCH 1 - PROGRESS: at 52.51% examples, 324821 words/s, in_qsize 4, out_qsize 1\n",
            "2018-12-09 08:04:50,535 : INFO : EPOCH 1 - PROGRESS: at 55.28% examples, 324841 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:04:51,545 : INFO : EPOCH 1 - PROGRESS: at 57.98% examples, 324630 words/s, in_qsize 4, out_qsize 1\n",
            "2018-12-09 08:04:52,580 : INFO : EPOCH 1 - PROGRESS: at 60.80% examples, 324731 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:04:53,607 : INFO : EPOCH 1 - PROGRESS: at 63.63% examples, 324895 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:04:54,631 : INFO : EPOCH 1 - PROGRESS: at 66.33% examples, 324461 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:04:55,647 : INFO : EPOCH 1 - PROGRESS: at 69.10% examples, 324546 words/s, in_qsize 4, out_qsize 1\n",
            "2018-12-09 08:04:56,676 : INFO : EPOCH 1 - PROGRESS: at 71.98% examples, 324989 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:04:57,684 : INFO : EPOCH 1 - PROGRESS: at 74.68% examples, 324931 words/s, in_qsize 4, out_qsize 1\n",
            "2018-12-09 08:04:58,719 : INFO : EPOCH 1 - PROGRESS: at 77.62% examples, 324858 words/s, in_qsize 3, out_qsize 2\n",
            "2018-12-09 08:04:59,742 : INFO : EPOCH 1 - PROGRESS: at 80.39% examples, 324713 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:00,748 : INFO : EPOCH 1 - PROGRESS: at 83.09% examples, 324459 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:01,749 : INFO : EPOCH 1 - PROGRESS: at 85.91% examples, 324921 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:02,765 : INFO : EPOCH 1 - PROGRESS: at 88.68% examples, 324833 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:03,777 : INFO : EPOCH 1 - PROGRESS: at 91.50% examples, 325071 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:04,822 : INFO : EPOCH 1 - PROGRESS: at 94.32% examples, 324930 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:05,847 : INFO : EPOCH 1 - PROGRESS: at 97.21% examples, 325174 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:06,827 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2018-12-09 08:05:06,831 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2018-12-09 08:05:06,841 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2018-12-09 08:05:06,843 : INFO : EPOCH - 1 : training on 17005207 raw words (11929593 effective words) took 36.7s, 325410 effective words/s\n",
            "2018-12-09 08:05:07,888 : INFO : EPOCH 2 - PROGRESS: at 2.71% examples, 313931 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:08,892 : INFO : EPOCH 2 - PROGRESS: at 5.53% examples, 322003 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:09,892 : INFO : EPOCH 2 - PROGRESS: at 8.35% examples, 324570 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:10,901 : INFO : EPOCH 2 - PROGRESS: at 11.11% examples, 323603 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:11,926 : INFO : EPOCH 2 - PROGRESS: at 13.88% examples, 323588 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:12,933 : INFO : EPOCH 2 - PROGRESS: at 16.64% examples, 324134 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:13,959 : INFO : EPOCH 2 - PROGRESS: at 19.46% examples, 324900 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:14,978 : INFO : EPOCH 2 - PROGRESS: at 22.17% examples, 323804 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:16,020 : INFO : EPOCH 2 - PROGRESS: at 24.99% examples, 324241 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:17,033 : INFO : EPOCH 2 - PROGRESS: at 27.70% examples, 324237 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:18,034 : INFO : EPOCH 2 - PROGRESS: at 30.34% examples, 323753 words/s, in_qsize 4, out_qsize 1\n",
            "2018-12-09 08:05:19,055 : INFO : EPOCH 2 - PROGRESS: at 33.17% examples, 324571 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:20,082 : INFO : EPOCH 2 - PROGRESS: at 35.99% examples, 325162 words/s, in_qsize 6, out_qsize 0\n",
            "2018-12-09 08:05:21,095 : INFO : EPOCH 2 - PROGRESS: at 38.75% examples, 325185 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:22,101 : INFO : EPOCH 2 - PROGRESS: at 41.58% examples, 325844 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:23,130 : INFO : EPOCH 2 - PROGRESS: at 44.34% examples, 325642 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:24,134 : INFO : EPOCH 2 - PROGRESS: at 47.10% examples, 325967 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:25,160 : INFO : EPOCH 2 - PROGRESS: at 49.93% examples, 326248 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:26,177 : INFO : EPOCH 2 - PROGRESS: at 52.75% examples, 326430 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:27,195 : INFO : EPOCH 2 - PROGRESS: at 55.57% examples, 326771 words/s, in_qsize 4, out_qsize 1\n",
            "2018-12-09 08:05:28,230 : INFO : EPOCH 2 - PROGRESS: at 58.39% examples, 326719 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:29,257 : INFO : EPOCH 2 - PROGRESS: at 61.28% examples, 327154 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:30,279 : INFO : EPOCH 2 - PROGRESS: at 64.10% examples, 327275 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:31,295 : INFO : EPOCH 2 - PROGRESS: at 66.92% examples, 327451 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:32,309 : INFO : EPOCH 2 - PROGRESS: at 69.63% examples, 327143 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:33,315 : INFO : EPOCH 2 - PROGRESS: at 72.27% examples, 326764 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:34,319 : INFO : EPOCH 2 - PROGRESS: at 75.04% examples, 326800 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:35,329 : INFO : EPOCH 2 - PROGRESS: at 77.92% examples, 326760 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:36,343 : INFO : EPOCH 2 - PROGRESS: at 80.68% examples, 326684 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:37,349 : INFO : EPOCH 2 - PROGRESS: at 83.44% examples, 326618 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:38,350 : INFO : EPOCH 2 - PROGRESS: at 86.27% examples, 326942 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:39,361 : INFO : EPOCH 2 - PROGRESS: at 89.09% examples, 327108 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:40,389 : INFO : EPOCH 2 - PROGRESS: at 91.85% examples, 326876 words/s, in_qsize 4, out_qsize 1\n",
            "2018-12-09 08:05:41,416 : INFO : EPOCH 2 - PROGRESS: at 94.68% examples, 326846 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:42,417 : INFO : EPOCH 2 - PROGRESS: at 97.44% examples, 326861 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:43,289 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2018-12-09 08:05:43,292 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2018-12-09 08:05:43,308 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2018-12-09 08:05:43,310 : INFO : EPOCH - 2 : training on 17005207 raw words (11930126 effective words) took 36.5s, 327219 effective words/s\n",
            "2018-12-09 08:05:44,322 : INFO : EPOCH 3 - PROGRESS: at 2.59% examples, 310097 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:45,327 : INFO : EPOCH 3 - PROGRESS: at 5.41% examples, 320559 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:46,333 : INFO : EPOCH 3 - PROGRESS: at 8.23% examples, 322850 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:47,371 : INFO : EPOCH 3 - PROGRESS: at 11.06% examples, 321862 words/s, in_qsize 4, out_qsize 1\n",
            "2018-12-09 08:05:48,392 : INFO : EPOCH 3 - PROGRESS: at 13.82% examples, 322258 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:49,411 : INFO : EPOCH 3 - PROGRESS: at 16.64% examples, 323661 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:50,417 : INFO : EPOCH 3 - PROGRESS: at 19.35% examples, 323259 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:51,436 : INFO : EPOCH 3 - PROGRESS: at 22.17% examples, 324090 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:52,454 : INFO : EPOCH 3 - PROGRESS: at 24.93% examples, 324574 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:53,473 : INFO : EPOCH 3 - PROGRESS: at 27.70% examples, 325040 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:54,486 : INFO : EPOCH 3 - PROGRESS: at 30.52% examples, 326043 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:55,491 : INFO : EPOCH 3 - PROGRESS: at 33.28% examples, 326486 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:56,501 : INFO : EPOCH 3 - PROGRESS: at 35.99% examples, 326357 words/s, in_qsize 5, out_qsize 1\n",
            "2018-12-09 08:05:57,505 : INFO : EPOCH 3 - PROGRESS: at 38.81% examples, 326912 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:05:58,527 : INFO : EPOCH 3 - PROGRESS: at 41.58% examples, 326620 words/s, in_qsize 4, out_qsize 1\n",
            "2018-12-09 08:05:59,560 : INFO : EPOCH 3 - PROGRESS: at 44.40% examples, 326732 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:06:00,562 : INFO : EPOCH 3 - PROGRESS: at 47.16% examples, 327013 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:06:01,566 : INFO : EPOCH 3 - PROGRESS: at 49.93% examples, 327139 words/s, in_qsize 4, out_qsize 1\n",
            "2018-12-09 08:06:02,607 : INFO : EPOCH 3 - PROGRESS: at 52.81% examples, 327304 words/s, in_qsize 5, out_qsize 1\n",
            "2018-12-09 08:06:03,619 : INFO : EPOCH 3 - PROGRESS: at 55.57% examples, 327373 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:06:04,620 : INFO : EPOCH 3 - PROGRESS: at 58.28% examples, 327157 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:06:05,654 : INFO : EPOCH 3 - PROGRESS: at 61.04% examples, 326839 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:06:06,674 : INFO : EPOCH 3 - PROGRESS: at 63.86% examples, 327034 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:06:07,683 : INFO : EPOCH 3 - PROGRESS: at 66.63% examples, 327004 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:06:08,691 : INFO : EPOCH 3 - PROGRESS: at 69.39% examples, 327089 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:06:09,698 : INFO : EPOCH 3 - PROGRESS: at 72.21% examples, 327467 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:06:10,711 : INFO : EPOCH 3 - PROGRESS: at 75.04% examples, 327628 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:06:11,721 : INFO : EPOCH 3 - PROGRESS: at 77.86% examples, 327316 words/s, in_qsize 6, out_qsize 0\n",
            "2018-12-09 08:06:12,735 : INFO : EPOCH 3 - PROGRESS: at 80.68% examples, 327454 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:06:13,774 : INFO : EPOCH 3 - PROGRESS: at 83.50% examples, 327244 words/s, in_qsize 4, out_qsize 1\n",
            "2018-12-09 08:06:14,797 : INFO : EPOCH 3 - PROGRESS: at 86.33% examples, 327311 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:06:15,808 : INFO : EPOCH 3 - PROGRESS: at 89.15% examples, 327480 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:06:16,814 : INFO : EPOCH 3 - PROGRESS: at 91.91% examples, 327425 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:06:17,824 : INFO : EPOCH 3 - PROGRESS: at 94.68% examples, 327346 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:06:18,842 : INFO : EPOCH 3 - PROGRESS: at 97.50% examples, 327372 words/s, in_qsize 5, out_qsize 1\n",
            "2018-12-09 08:06:19,710 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2018-12-09 08:06:19,712 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2018-12-09 08:06:19,728 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2018-12-09 08:06:19,729 : INFO : EPOCH - 3 : training on 17005207 raw words (11928256 effective words) took 36.4s, 327592 effective words/s\n",
            "2018-12-09 08:06:20,760 : INFO : EPOCH 4 - PROGRESS: at 2.59% examples, 303346 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:06:21,793 : INFO : EPOCH 4 - PROGRESS: at 5.47% examples, 316146 words/s, in_qsize 4, out_qsize 1\n",
            "2018-12-09 08:06:22,796 : INFO : EPOCH 4 - PROGRESS: at 8.29% examples, 320242 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:06:23,816 : INFO : EPOCH 4 - PROGRESS: at 11.17% examples, 322887 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:06:24,824 : INFO : EPOCH 4 - PROGRESS: at 13.88% examples, 322630 words/s, in_qsize 4, out_qsize 1\n",
            "2018-12-09 08:06:25,824 : INFO : EPOCH 4 - PROGRESS: at 16.64% examples, 323763 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:06:26,852 : INFO : EPOCH 4 - PROGRESS: at 19.52% examples, 325375 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:06:27,881 : INFO : EPOCH 4 - PROGRESS: at 22.35% examples, 325556 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:06:28,889 : INFO : EPOCH 4 - PROGRESS: at 25.05% examples, 325457 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:06:29,898 : INFO : EPOCH 4 - PROGRESS: at 27.81% examples, 326159 words/s, in_qsize 4, out_qsize 1\n",
            "2018-12-09 08:06:30,918 : INFO : EPOCH 4 - PROGRESS: at 30.58% examples, 326229 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:06:31,929 : INFO : EPOCH 4 - PROGRESS: at 33.40% examples, 327032 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:06:32,936 : INFO : EPOCH 4 - PROGRESS: at 36.11% examples, 326861 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:06:33,944 : INFO : EPOCH 4 - PROGRESS: at 38.87% examples, 326829 words/s, in_qsize 4, out_qsize 1\n",
            "2018-12-09 08:06:34,964 : INFO : EPOCH 4 - PROGRESS: at 41.69% examples, 327046 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:06:35,974 : INFO : EPOCH 4 - PROGRESS: at 44.46% examples, 327196 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:06:36,984 : INFO : EPOCH 4 - PROGRESS: at 47.16% examples, 326878 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:06:37,984 : INFO : EPOCH 4 - PROGRESS: at 49.93% examples, 327087 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:06:39,028 : INFO : EPOCH 4 - PROGRESS: at 52.75% examples, 326876 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:06:40,042 : INFO : EPOCH 4 - PROGRESS: at 55.57% examples, 327247 words/s, in_qsize 6, out_qsize 0\n",
            "2018-12-09 08:06:41,076 : INFO : EPOCH 4 - PROGRESS: at 58.39% examples, 327189 words/s, in_qsize 4, out_qsize 1\n",
            "2018-12-09 08:06:42,091 : INFO : EPOCH 4 - PROGRESS: at 61.22% examples, 327464 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:06:43,095 : INFO : EPOCH 4 - PROGRESS: at 64.04% examples, 327830 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:06:44,096 : INFO : EPOCH 4 - PROGRESS: at 66.74% examples, 327627 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:06:45,125 : INFO : EPOCH 4 - PROGRESS: at 69.57% examples, 327652 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:06:46,163 : INFO : EPOCH 4 - PROGRESS: at 72.39% examples, 327638 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:06:47,167 : INFO : EPOCH 4 - PROGRESS: at 75.21% examples, 327797 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:06:48,174 : INFO : EPOCH 4 - PROGRESS: at 78.09% examples, 327841 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:06:49,197 : INFO : EPOCH 4 - PROGRESS: at 80.92% examples, 327871 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:06:50,222 : INFO : EPOCH 4 - PROGRESS: at 83.74% examples, 327829 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:06:51,240 : INFO : EPOCH 4 - PROGRESS: at 86.44% examples, 327470 words/s, in_qsize 4, out_qsize 2\n",
            "2018-12-09 08:06:52,257 : INFO : EPOCH 4 - PROGRESS: at 89.27% examples, 327586 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:06:53,271 : INFO : EPOCH 4 - PROGRESS: at 92.03% examples, 327514 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:06:54,273 : INFO : EPOCH 4 - PROGRESS: at 94.79% examples, 327442 words/s, in_qsize 4, out_qsize 1\n",
            "2018-12-09 08:06:55,314 : INFO : EPOCH 4 - PROGRESS: at 97.62% examples, 327246 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:06:56,127 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2018-12-09 08:06:56,142 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2018-12-09 08:06:56,159 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2018-12-09 08:06:56,160 : INFO : EPOCH - 4 : training on 17005207 raw words (11928584 effective words) took 36.4s, 327467 effective words/s\n",
            "2018-12-09 08:06:57,170 : INFO : EPOCH 5 - PROGRESS: at 2.59% examples, 311054 words/s, in_qsize 6, out_qsize 0\n",
            "2018-12-09 08:06:58,224 : INFO : EPOCH 5 - PROGRESS: at 5.35% examples, 309867 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:06:59,225 : INFO : EPOCH 5 - PROGRESS: at 8.12% examples, 314213 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:07:00,258 : INFO : EPOCH 5 - PROGRESS: at 10.94% examples, 315677 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:07:01,265 : INFO : EPOCH 5 - PROGRESS: at 13.64% examples, 316670 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:07:02,280 : INFO : EPOCH 5 - PROGRESS: at 16.47% examples, 319102 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:07:03,288 : INFO : EPOCH 5 - PROGRESS: at 19.23% examples, 320291 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:07:04,295 : INFO : EPOCH 5 - PROGRESS: at 21.99% examples, 321197 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:07:05,308 : INFO : EPOCH 5 - PROGRESS: at 24.76% examples, 322143 words/s, in_qsize 4, out_qsize 2\n",
            "2018-12-09 08:07:06,333 : INFO : EPOCH 5 - PROGRESS: at 27.58% examples, 323301 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:07:07,336 : INFO : EPOCH 5 - PROGRESS: at 30.34% examples, 324142 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:07:08,351 : INFO : EPOCH 5 - PROGRESS: at 33.11% examples, 324510 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:07:09,351 : INFO : EPOCH 5 - PROGRESS: at 35.81% examples, 324676 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:07:10,353 : INFO : EPOCH 5 - PROGRESS: at 38.52% examples, 324542 words/s, in_qsize 6, out_qsize 0\n",
            "2018-12-09 08:07:11,354 : INFO : EPOCH 5 - PROGRESS: at 41.34% examples, 325263 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:07:12,376 : INFO : EPOCH 5 - PROGRESS: at 44.10% examples, 325264 words/s, in_qsize 6, out_qsize 0\n",
            "2018-12-09 08:07:13,377 : INFO : EPOCH 5 - PROGRESS: at 46.87% examples, 325678 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:07:14,386 : INFO : EPOCH 5 - PROGRESS: at 49.63% examples, 325781 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:07:15,390 : INFO : EPOCH 5 - PROGRESS: at 52.45% examples, 326280 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:07:16,391 : INFO : EPOCH 5 - PROGRESS: at 55.22% examples, 326554 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:07:17,404 : INFO : EPOCH 5 - PROGRESS: at 57.98% examples, 326568 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:07:18,423 : INFO : EPOCH 5 - PROGRESS: at 60.75% examples, 326477 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:07:19,444 : INFO : EPOCH 5 - PROGRESS: at 63.51% examples, 326378 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:07:20,469 : INFO : EPOCH 5 - PROGRESS: at 66.33% examples, 326475 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:07:21,488 : INFO : EPOCH 5 - PROGRESS: at 69.10% examples, 326402 words/s, in_qsize 4, out_qsize 1\n",
            "2018-12-09 08:07:22,498 : INFO : EPOCH 5 - PROGRESS: at 71.92% examples, 326758 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:07:23,498 : INFO : EPOCH 5 - PROGRESS: at 74.68% examples, 326970 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:07:24,521 : INFO : EPOCH 5 - PROGRESS: at 77.62% examples, 326944 words/s, in_qsize 4, out_qsize 1\n",
            "2018-12-09 08:07:25,541 : INFO : EPOCH 5 - PROGRESS: at 80.50% examples, 327248 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:07:26,544 : INFO : EPOCH 5 - PROGRESS: at 83.27% examples, 327180 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:07:27,558 : INFO : EPOCH 5 - PROGRESS: at 86.09% examples, 327407 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:07:28,579 : INFO : EPOCH 5 - PROGRESS: at 88.97% examples, 327627 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:07:29,598 : INFO : EPOCH 5 - PROGRESS: at 91.74% examples, 327489 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:07:30,604 : INFO : EPOCH 5 - PROGRESS: at 94.62% examples, 327853 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:07:31,625 : INFO : EPOCH 5 - PROGRESS: at 97.38% examples, 327625 words/s, in_qsize 5, out_qsize 0\n",
            "2018-12-09 08:07:32,501 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2018-12-09 08:07:32,521 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2018-12-09 08:07:32,526 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2018-12-09 08:07:32,528 : INFO : EPOCH - 5 : training on 17005207 raw words (11929197 effective words) took 36.4s, 328086 effective words/s\n",
            "2018-12-09 08:07:32,531 : INFO : training on a 85026035 raw words (59645756 effective words) took 182.4s, 327088 effective words/s\n",
            "2018-12-09 08:07:32,533 : INFO : precomputing L2-norms of word weight vectors\n",
            "2018-12-09 08:07:32,776 : INFO : saving Word2Vec object under word2vec_gensim.bin, separately None\n",
            "2018-12-09 08:07:32,778 : INFO : not storing attribute vectors_norm\n",
            "2018-12-09 08:07:32,781 : INFO : not storing attribute cum_table\n",
            "2018-12-09 08:07:33,443 : INFO : saved word2vec_gensim.bin\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "rtX77UU06lsm",
        "colab_type": "code",
        "outputId": "7aeb3356-f47f-4dea-eb6d-f2ea2e049256",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "cell_type": "code",
      "source": [
        "model.init_sims(replace=True)\n",
        "model.save(\"drive/My Drive/Colab Notebooks/chokkan_deeplearning/data/word2vec_gemsim.bin\")\n",
        "model = word2vec.Word2Vec.load('drive/My Drive/Colab Notebooks/chokkan_deeplearning/data/word2vec_gemsim.bin')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2018-12-09 08:11:33,430 : INFO : precomputing L2-norms of word weight vectors\n",
            "2018-12-09 08:11:33,670 : INFO : saving Word2Vec object under drive/My Drive/Colab Notebooks/chokkan_deeplearning/data/word2vec_gemsim.bin, separately None\n",
            "2018-12-09 08:11:33,672 : INFO : not storing attribute vectors_norm\n",
            "2018-12-09 08:11:33,674 : INFO : not storing attribute cum_table\n",
            "2018-12-09 08:11:34,433 : INFO : saved drive/My Drive/Colab Notebooks/chokkan_deeplearning/data/word2vec_gemsim.bin\n",
            "2018-12-09 08:11:34,435 : INFO : loading Word2Vec object from drive/My Drive/Colab Notebooks/chokkan_deeplearning/data/word2vec_gemsim.bin\n",
            "2018-12-09 08:11:35,001 : INFO : loading wv recursively from drive/My Drive/Colab Notebooks/chokkan_deeplearning/data/word2vec_gemsim.bin.wv.* with mmap=None\n",
            "2018-12-09 08:11:35,002 : INFO : setting ignored attribute vectors_norm to None\n",
            "2018-12-09 08:11:35,003 : INFO : loading vocabulary recursively from drive/My Drive/Colab Notebooks/chokkan_deeplearning/data/word2vec_gemsim.bin.vocabulary.* with mmap=None\n",
            "2018-12-09 08:11:35,006 : INFO : loading trainables recursively from drive/My Drive/Colab Notebooks/chokkan_deeplearning/data/word2vec_gemsim.bin.trainables.* with mmap=None\n",
            "2018-12-09 08:11:35,007 : INFO : setting ignored attribute cum_table to None\n",
            "2018-12-09 08:11:35,010 : INFO : loaded drive/My Drive/Colab Notebooks/chokkan_deeplearning/data/word2vec_gemsim.bin\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "fcuH71-jBFHO",
        "colab_type": "code",
        "outputId": "eb5fb13e-a75b-4d7e-e8b4-0ac549455783",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "list(model.wv.vocab.keys())[0:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['anarchism',\n",
              " 'originated',\n",
              " 'as',\n",
              " 'a',\n",
              " 'term',\n",
              " 'of',\n",
              " 'abuse',\n",
              " 'first',\n",
              " 'used',\n",
              " 'against']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "metadata": {
        "id": "2arW4P7yBeok",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model[\"woman\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3G-ebGhnBgdO",
        "colab_type": "code",
        "outputId": "2676e19b-457a-41ec-ae69-caab3794c6a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "cell_type": "code",
      "source": [
        "model.most_similar(\"woman\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "2018-12-09 08:12:45,755 : INFO : precomputing L2-norms of word weight vectors\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('child', 0.7281472682952881),\n",
              " ('girl', 0.7064403295516968),\n",
              " ('man', 0.6807918548583984),\n",
              " ('lover', 0.6249527931213379),\n",
              " ('person', 0.617744505405426),\n",
              " ('lady', 0.6149004697799683),\n",
              " ('herself', 0.614129900932312),\n",
              " ('prostitute', 0.6023421287536621),\n",
              " ('daughter', 0.5937902331352234),\n",
              " ('baby', 0.5883060693740845)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "metadata": {
        "id": "MCqH2TOWBpnu",
        "colab_type": "code",
        "outputId": "ab857beb-20e8-40dc-ba37-8e9d7def53a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "cell_type": "code",
      "source": [
        "model.most_similar(positive=['woman', 'king'], negative=['man'], topn=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('queen', 0.6095590591430664),\n",
              " ('throne', 0.5634428262710571),\n",
              " ('princess', 0.5530728101730347),\n",
              " ('elizabeth', 0.5459858179092407),\n",
              " ('emperor', 0.5451050400733948),\n",
              " ('prince', 0.5418133735656738),\n",
              " ('sigismund', 0.5339542627334595),\n",
              " ('daughter', 0.5274652242660522),\n",
              " ('isabella', 0.5246189832687378),\n",
              " ('empress', 0.5207471251487732)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "metadata": {
        "id": "h1PUfe13Bz02",
        "colab_type": "code",
        "outputId": "adc71422-a096-451a-8a49-c8fe368866b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "model.similarity(\"girl\", \"woman\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.70644027"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "metadata": {
        "id": "j_Dge_nLIcUs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Glove"
      ]
    },
    {
      "metadata": {
        "id": "fX5AVrGPLVjj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### ゼロから分散表現を学習する"
      ]
    },
    {
      "metadata": {
        "id": "Ud4OsoWMCUfb",
        "colab_type": "code",
        "outputId": "1b894554-7cda-41b5-88ca-772e68cc199f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import collections\n",
        "\n",
        "from keras.layers import Dense, Dropout, Conv1D, Embedding, GlobalMaxPooling1D\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import codecs\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "-C5hWmSFQcMZ",
        "colab_type": "code",
        "outputId": "c6b67100-17ff-4124-b0f6-0c996d1e727d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        }
      },
      "cell_type": "code",
      "source": [
        "nltk.download(\"popular\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "7aal3v4ELUYB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "INPUT_FILE = \"drive/My Drive/Colab Notebooks/chokkan_deeplearning/data/training.txt\"\n",
        "LOG_DIR = \"drive/My Drive/Colab Notebooks/chokkan_deeplearning/data/logs\"\n",
        "VOCAB_SIZE = 5000\n",
        "EMBED_SIZE = 100\n",
        "NUM_FILTERS = 256\n",
        "NUM_WORDS = 3\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "counter = collections.Counter()\n",
        "with codecs.open(INPUT_FILE, \"r\", encoding=\"utf-8\") as fin:\n",
        "  maxlen = 0\n",
        "  for line in fin:\n",
        "    _, sent = line.strip().split(\"\\t\")\n",
        "    words = [x.lower() for x in nltk.word_tokenize(sent)]\n",
        "    maxlen = max(maxlen, len(words))\n",
        "    for word in words:\n",
        "      counter[word] += 1\n",
        "\n",
        "word2index = collections.defaultdict(int)\n",
        "for wid, word in enumerate(counter.most_common(VOCAB_SIZE)):\n",
        "  word2index[word[0]] = wid + 1\n",
        "vocab_sz = len(word2index) + 1\n",
        "index2word = {v: k for k, v in word2index.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D3n3_O_H_HvO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 第６章：リカレントニューラルネットワーク"
      ]
    },
    {
      "metadata": {
        "id": "AUCduE8N-ZCO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Dense, Activation, SimpleRNN\n",
        "from keras.models import Sequential\n",
        "import numpy as np\n",
        "import codecs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s86LXCCqBO8w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "INPUT_FILE = \"drive/My Drive/Colab Notebooks/chokkan_deeplearning/data/alice_in_wonderland.txt\"\n",
        "\n",
        "with codecs.open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "  lines = [line.strip().lower() for line in f\n",
        "               if len(line) !=0]\n",
        "  text = \" \".join(lines)\n",
        "  \n",
        "chars = set(text)\n",
        "nb_chars = len(chars)\n",
        "char2index = dict((c, i) for i, c in enumerate(chars))\n",
        "index2char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O8A5M4hrC7iH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "SEQLEN = 10\n",
        "STEP = 1\n",
        "\n",
        "input_chars = []\n",
        "label_chars = []\n",
        "for i in range(0, len(text) - SEQLEN, STEP):\n",
        "  input_chars.append(text[i:i + SEQLEN])\n",
        "  label_chars.append(text[i + SEQLEN])\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LsdysO8ZD22E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)\n",
        "y = np.zeros((len(input_chars), nb_chars), dtype=np.bool)\n",
        "for i, input_char in enumerate(input_chars):\n",
        "  for j, ch in enumerate(input_char):\n",
        "    X[i, j, char2index[ch]] = 1\n",
        "  y[i, char2index[label_chars[i]]] = 1\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3jkTKjp9D543",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "HIDDEN_SIZE =128\n",
        "BATCH_SIZE = 128\n",
        "NUM_ITERATIONS = 25\n",
        "NUM_EPOCHS_PER_ITERATION = 1\n",
        "NUM_PREDS_PER_EPOCH = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(HIDDEN_SIZE, return_sequences=False,\n",
        "                   input_shape=(SEQLEN, nb_chars),\n",
        "                   unroll=True))\n",
        "model.add(Dense(nb_chars))\n",
        "model.add(Activation(\"softmax\"))\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer='rmsprop')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_W3xUmpPz59M",
        "colab_type": "code",
        "outputId": "8c85b8a6-6a58-4cc9-fdd0-f8c389fb0025",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2567
        }
      },
      "cell_type": "code",
      "source": [
        "for iteration in range(NUM_ITERATIONS):\n",
        "  print(\"=\" * 50)\n",
        "  print(\"Iteration #: %d\" % (iteration))\n",
        "  model.fit(X, y, batch_size=BATCH_SIZE,\n",
        "           epochs=NUM_EPOCHS_PER_ITERATION)\n",
        "  \n",
        "  test_idx = np.random.randint(len(input_chars))\n",
        "  test_chars = input_chars[test_idx]\n",
        "  print(\"Generating from seed: %s\" %(test_chars))\n",
        "  print(test_chars, end=\"\")\n",
        "  for i in range(NUM_PREDS_PER_EPOCH):\n",
        "    Xtest = np.zeros((1, SEQLEN, nb_chars))\n",
        "    for i, ch in enumerate(test_chars):\n",
        "      Xtest[0, i, char2index[ch]] = 1\n",
        "    pred = model.predict(Xtest, verbose=0)[0]\n",
        "    ypred = index2char[np.argmax(pred)]\n",
        "    print(ypred, end=\"\")\n",
        "    test_chars = test_chars[1:] + ypred\n",
        "  print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Iteration #: 0\n",
            "Epoch 1/1\n",
            "162739/162739 [==============================] - 9s 54us/step - loss: 1.7773\n",
            "Generating from seed: harp kick,\n",
            "harp kick, and the gryphon a done of the she said alice so the grown in a long the mart of the reat the said a\n",
            "==================================================\n",
            "Iteration #: 1\n",
            "Epoch 1/1\n",
            "162739/162739 [==============================] - 9s 54us/step - loss: 1.7252\n",
            "Generating from seed: ?’  ‘yes,’\n",
            "?’  ‘yes,’ said the could not the rouse said the could not the rouse said the could not the rouse said the cou\n",
            "==================================================\n",
            "Iteration #: 2\n",
            "Epoch 1/1\n",
            "162739/162739 [==============================] - 9s 54us/step - loss: 1.6835\n",
            "Generating from seed: 4 and the \n",
            "4 and the groped and the said the dormouse of the project gutenberg-tm enechion was she was she was she was sh\n",
            "==================================================\n",
            "Iteration #: 3\n",
            "Epoch 1/1\n",
            "162739/162739 [==============================] - 9s 54us/step - loss: 1.6473\n",
            "Generating from seed: ven if i f\n",
            "ven if i for the rook to the mork to the the rook to the mork to the the rook to the mork to the the rook to t\n",
            "==================================================\n",
            "Iteration #: 4\n",
            "Epoch 1/1\n",
            "162739/162739 [==============================] - 9s 54us/step - loss: 1.6145\n",
            "Generating from seed: french and\n",
            "french and the project gutenberg-tm eat one way a great her head began to she said to herself to herself to he\n",
            "==================================================\n",
            "Iteration #: 5\n",
            "Epoch 1/1\n",
            "162739/162739 [==============================] - 9s 54us/step - loss: 1.5885\n",
            "Generating from seed: y for your\n",
            "y for your was the caterpelf the could be and which was the caterpelf the could be and which was the caterpelf\n",
            "==================================================\n",
            "Iteration #: 6\n",
            "Epoch 1/1\n",
            "162739/162739 [==============================] - 9s 54us/step - loss: 1.5640\n",
            "Generating from seed: e mock tur\n",
            "e mock turtle in the parte to the the project gutenberg to the stole she was and the march hare and the march \n",
            "==================================================\n",
            "Iteration #: 7\n",
            "Epoch 1/1\n",
            "162739/162739 [==============================] - 9s 54us/step - loss: 1.5436\n",
            "Generating from seed: s curious \n",
            "s curious to alice found the mock turtle replied alice was the gryphon and was going to her had the parter sta\n",
            "==================================================\n",
            "Iteration #: 8\n",
            "Epoch 1/1\n",
            "162739/162739 [==============================] - 9s 54us/step - loss: 1.5253\n",
            "Generating from seed: earnestly,\n",
            "earnestly, and the morke for a little for a little for a little for a little for a little for a little for a l\n",
            "==================================================\n",
            "Iteration #: 9\n",
            "Epoch 1/1\n",
            "162739/162739 [==============================] - 9s 54us/step - loss: 1.5095\n",
            "Generating from seed: ut a thous\n",
            "ut a thoused and the gryphon, and the gryphon, and the gryphon, and the gryphon, and the gryphon, and the gryp\n",
            "==================================================\n",
            "Iteration #: 10\n",
            "Epoch 1/1\n",
            "162739/162739 [==============================] - 9s 54us/step - loss: 1.4948\n",
            "Generating from seed: errupted: \n",
            "errupted: ‘what i suppose the mock turtle, ‘but it was so the march hare so much a termouse of the caterpillar\n",
            "==================================================\n",
            "Iteration #: 11\n",
            "Epoch 1/1\n",
            "162739/162739 [==============================] - 9s 54us/step - loss: 1.4816\n",
            "Generating from seed: i_ don’t b\n",
            "i_ don’t be a little the furte don’t say you know when the dight the dormouse the pursen the ling to the gryph\n",
            "==================================================\n",
            "Iteration #: 12\n",
            "Epoch 1/1\n",
            "162739/162739 [==============================] - 9s 55us/step - loss: 1.4698\n",
            "Generating from seed: at will be\n",
            "at will be a little some of the mouse to see the mouse to see the mouse to see the mouse to see the mouse to s\n",
            "==================================================\n",
            "Iteration #: 13\n",
            "Epoch 1/1\n",
            "162739/162739 [==============================] - 9s 54us/step - loss: 1.4585\n",
            "Generating from seed:  would eve\n",
            " would ever to the part come and the pabsing and she said to the project gutenberg-tm electronic works and the\n",
            "==================================================\n",
            "Iteration #: 14\n",
            "Epoch 1/1\n",
            "162739/162739 [==============================] - 9s 54us/step - loss: 1.4486\n",
            "Generating from seed: ading away\n",
            "ading away was a little she said the mouse to see the mouse to see the mouse to see the mouse to see the mouse\n",
            "==================================================\n",
            "Iteration #: 15\n",
            "Epoch 1/1\n",
            "162739/162739 [==============================] - 9s 54us/step - loss: 1.4388\n",
            "Generating from seed: down the m\n",
            "down the mock turtle would be a little betting of the tried to herself to the tried to herself to the tried to\n",
            "==================================================\n",
            "Iteration #: 16\n",
            "Epoch 1/1\n",
            "162739/162739 [==============================] - 9s 54us/step - loss: 1.4309\n",
            "Generating from seed: s to see w\n",
            "s to see were the was the mock turtle so make out of the was the mock turtle so make out of the was the mock t\n",
            "==================================================\n",
            "Iteration #: 17\n",
            "Epoch 1/1\n",
            "162739/162739 [==============================] - 9s 54us/step - loss: 1.4226\n",
            "Generating from seed: l i begin,\n",
            "l i begin, and she was a little been here the part must be a little been here the part must be a little been h\n",
            "==================================================\n",
            "Iteration #: 18\n",
            "Epoch 1/1\n",
            "162739/162739 [==============================] - 9s 54us/step - loss: 1.4149\n",
            "Generating from seed: m for her.\n",
            "m for her.  ‘there was not like the dormouse the white rabbit alice, and was a little she said the mock turtle\n",
            "==================================================\n",
            "Iteration #: 19\n",
            "Epoch 1/1\n",
            "162739/162739 [==============================] - 9s 54us/step - loss: 1.4076\n",
            "Generating from seed: a serpent,\n",
            "a serpent, and was the mock turtle so the project gutenberg-tm license to see the mouse to see the mouse to se\n",
            "==================================================\n",
            "Iteration #: 20\n",
            "Epoch 1/1\n",
            "162739/162739 [==============================] - 9s 54us/step - loss: 1.4013\n",
            "Generating from seed: dodo.  the\n",
            "dodo.  the hatter before the reas now under to herself to see the hatter before the reas now under to herself \n",
            "==================================================\n",
            "Iteration #: 21\n",
            "Epoch 1/1\n",
            "162739/162739 [==============================] - 9s 54us/step - loss: 1.3952\n",
            "Generating from seed: beg your p\n",
            "beg your pards to the this could be no hand the mock turtle so the mock turtle so the mock turtle so the mock \n",
            "==================================================\n",
            "Iteration #: 22\n",
            "Epoch 1/1\n",
            "162739/162739 [==============================] - 9s 54us/step - loss: 1.3889\n",
            "Generating from seed: the other \n",
            "the other again.  ‘i do many of the look of the ground her and the mock turtle down the mock turtle down the m\n",
            "==================================================\n",
            "Iteration #: 23\n",
            "Epoch 1/1\n",
            "162739/162739 [==============================] - 9s 54us/step - loss: 1.3834\n",
            "Generating from seed: fairly,’ a\n",
            "fairly,’ alice dasis to the dormouse said the king said to herself, ‘and the dormouse said the king said to he\n",
            "==================================================\n",
            "Iteration #: 24\n",
            "Epoch 1/1\n",
            "162739/162739 [==============================] - 9s 54us/step - loss: 1.3784\n",
            "Generating from seed: e,’ said t\n",
            "e,’ said the dormouse in the door and the project gutenberg-tm electronic works the rabbit had she was all the\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mAqlUy5S3i23",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b3FSPeDbObfH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## LSTMで評判分析"
      ]
    },
    {
      "metadata": {
        "id": "oW0viPmMOerf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Activation, Dense, Dropout, Embedding, LSTM\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing import sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "import collections\n",
        "import nltk\n",
        "import numpy as np\n",
        "import os\n",
        "import codecs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0IrS8ateO6Bt",
        "colab_type": "code",
        "outputId": "311fa379-6486-4081-db6c-1c9728cad3ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.11.0)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "dcBh360JO-_I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "maxlen = 0\n",
        "word_freqs = collections.Counter()\n",
        "num_recs = 0\n",
        "with codecs.open(\"umich-sentiment-train.txt\",\n",
        "                \"r\", 'utf-8') as ftrain:\n",
        "  for line in ftrain:\n",
        "    label, sentence = line.strip().split(\"\\t\")\n",
        "    words = nltk.word_tokenize(sentence.lower())\n",
        "    maxlen = max(maxlen, len(words))\n",
        "    for word in words:\n",
        "      word_freqs[word] += 1\n",
        "    num_recs += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V7Ue1RT4Q6CL",
        "colab_type": "code",
        "outputId": "8bb6ad5e-1af1-4489-dbd9-5805f4347315",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(maxlen)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "42\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5VeB3LyzSjn6",
        "colab_type": "code",
        "outputId": "d7a5736e-3b6e-4ac4-b2b8-b480f1a25b4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(len(word_freqs))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2328\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aXXdIhqtSmOF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "MAX_FEATURES = 2000\n",
        "MAX_SENTENCE_LENGTH = 40\n",
        "vocab_size = min(MAX_FEATURES, len(word_freqs)) + 2"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
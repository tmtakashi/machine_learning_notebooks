{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec_notes.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tmtakashi/machine_learning_notebooks/blob/master/word2vec_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "DoSHxt14gLaW",
        "colab_type": "code",
        "outputId": "5edae48f-c3c4-4c6b-a03f-3d9e5903a605",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AZvaLK9VgiYE",
        "colab_type": "code",
        "outputId": "1e059bb9-30d6-47ee-9c3e-92968f4cfe76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "% cd /content/drive/My Drive/Colab Notebooks/fusic/word2vec"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/fusic/word2vec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EJI9pY2iTi84",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# word2vecまとめ"
      ]
    },
    {
      "metadata": {
        "id": "x9mWw4JfRtnX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 自然言語処理の手法\n",
        "- シソーラス\n",
        "  - 類似語をつなぎ合わせて単語ネットワークを作る\n",
        "  - 最も有名なシソーラス：WordNet\n",
        "  - 問題点：\n",
        "    - 時代の変化に対応しにくい\n",
        "    - 高人的コスト\n",
        "    - 大まかにグループ化されているため細かいニュアンスが表現しにくいｚ\n",
        "    \n",
        "- カウントベース\n",
        "    - 前提：「単語の意味は、周囲の単語によって形成される」（分布仮説）\n",
        "    - 単語の共起行列（他の単語と隣り合う頻度を表す）を特異値分解（次元削減）→　単語の分散表現を得る。\n",
        "    - 問題点：コーパス全体（英語だと約100万語）に対して特異値分解をすると計算量がエグい。\n",
        "    \n",
        "- 推論ベース\n",
        "    - 学習データの一部を使って逐次的にニューラルネットワークで処理。\n",
        "    - word2vec\n",
        "      - Skip-gramとCBOW    \n",
        "- 最近ではGloveというカウントベースと推論ベースを組み合わせたモデルも出てきている\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "2z1iff1Ykjy2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 推論ベースの手法の概要\n",
        "- 周囲の単語が与えられたときに、目的の単語を予測 or vice versa\n",
        "    - 推論問題を繰り返し解いて学習\n",
        "    - コンテキスト→モデル→確率分布（予測）\n",
        "### ニューラルネットワークにおける単語の処理方法\n",
        "- 単語→単語ID→one-hot表現\n",
        "    - ex. \"You say goodbye and I say hello.\"の一文をコーパスとして捉える\n",
        "        - 単語ID => 0 ~ 6\n",
        "        \n",
        "- 重み行列の各行を抽出している　→　各行が各単語のベクトル表現になっている"
      ]
    },
    {
      "metadata": {
        "id": "q0MSXmWuRtnX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Skip-gram\n",
        "- 入力：中心語（中央の単語)\n",
        "- 出力：文脈語（周囲の単語）\n",
        "\n",
        "- 例文：I love green eggs and ham.\n",
        "    - ウインドウサイズ1を仮定、（文脈語、中心語）のペアに分解\n",
        "        - ([I, green], love), ([love, eggs], green), ([green, and], eggs)\n",
        "    - 負例（実際の文章に含まれる組み合わせとはことなるもの）を生成\n",
        "        - (love, Sam), (love, zebra), (green, thing)\n",
        "    - 正例と負例を生成\n",
        "        - ((love, I), 1), ((love, green), 1), ..., ((love, Sam), 0), ((love, zebra), 0), ...\n",
        "        \n",
        "- ネットワークを学習させEmbedding層の重みを得れば単語IDから単語のベクトル表現の重みを得られる。"
      ]
    },
    {
      "metadata": {
        "id": "G9WiYNOlRtnY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Embedding層\n",
        "- 各行を抜き出すだけでよいのに、行列の掛け算をするのは非効率\n",
        "- 重みパラメータから単語IDに該当する行を抜き出す層"
      ]
    },
    {
      "metadata": {
        "id": "U4XBeG0sRtnZ",
        "colab_type": "code",
        "outputId": "82faed86-340b-441f-b801-4d5839258fcb",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.layers import Dot, Dense, Reshape, Embedding, Input\n",
        "from keras.models import Model\n",
        "\n",
        "vocab_size = 5000\n",
        "embed_size = 300\n",
        "\n",
        "# http://cookie-box.hatenablog.com/entry/2018/10/14/184801\n",
        "class SkipGramDiscriminator():\n",
        "  def __init__(self, vocab_size, embed_size):\n",
        "    self.vocab_size = vocab_size #  語彙数\n",
        "    self.embed_size = embed_size # 埋め込み次元数\n",
        "  def create_model(self):\n",
        "    # 中心語ID => 中心語数値ベクトル表現\n",
        "    x0 = Input(shape=(1, ))\n",
        "    y0 = Embedding(self.vocab_size, self.embed_size,\n",
        "                  embeddings_initializer='glorot_uniform')(x0)\n",
        "    y0 = Reshape((self.embed_size, ))(y0)\n",
        "    self.word_embedder = Model(x0, y0)\n",
        "    # 文脈語ID => 文脈語数値ベクトル表現（実装上は左右いずれかの単語）\n",
        "    x1 = Input(shape=(1, )) \n",
        "    y1 = Embedding(self.vocab_size, self.embed_size,\n",
        "                  embeddings_initializer='glorot_uniform')(x1)\n",
        "    y1 = Reshape((self.embed_size, ))(y1)\n",
        "    self.context_embedder = Model(x1, y1)\n",
        "    # 内積 => ロジスティック回帰\n",
        "    y = Dot(axes=-1)([y0, y1])\n",
        "    y = Dense(1, kernel_initializer='glorot_uniform', activation='sigmoid')(y)\n",
        "    self.discriminator = Model(inputs=[x0, x1], outputs=y)\n",
        "    self.discriminator.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    self.discriminator.summary()\n",
        "    \n",
        "from keras.preprocessing.text import * \n",
        "from keras.preprocessing.sequence import skipgrams\n",
        "\n",
        "text = \"I love green eggs and ham .\"\n",
        "\n",
        "# 各単語を整数IDにマッピングする辞書を作成\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "\n",
        "word2id = tokenizer.word_index\n",
        "id2word = {v:k for k, v in word2id.items()}\n",
        "\n",
        "wids = [word2id[w] for w in text_to_word_sequence(text)]\n",
        "pairs, labels = skipgrams(wids, len(word2id), window_size=1)\n",
        "\n",
        "print(len(pairs), len(labels))\n",
        "for i in range(10):\n",
        "  print(\"({:s} ({:d}), {:s}({:d})) -> {:d}\".format(\n",
        "            id2word[pairs[i][0]], pairs[i][0],\n",
        "            id2word[pairs[i][1]], pairs[i][1],\n",
        "            labels[i]))\n",
        "  \n",
        "sg = SkipGramDiscriminator(6, 3)\n",
        "sg.create_model()\n",
        "x0_samples = np.array([[1], [4], [1], [4], [2]]) # 中心語： love,and,love,and,green\n",
        "x1_samples = np.array([[0], [5], [2], [2], [2]]) # 文脈語： i,ham,green,green,green\n",
        "y_samples = sg.discriminator.predict([x0_samples, x1_samples])\n",
        "print(y_samples) # 中心語と文脈語のペアであるかどうかの判定結果（学習まだ）\n",
        "\n",
        "print('中心語の数値ベクトル表現は中心語の Embedding 層の重みそのもの\\n', sg.word_embedder.get_weights())\n",
        "\n",
        "# IDから数値ベクトル表現を取り出せることの確認\n",
        "print('i の数値ベクトル表現: ', sg.word_embedder.predict([[0]]))\n",
        "print('love の数値ベクトル表現', sg.word_embedder.predict([[1]])) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20 20\n",
            "(ham (6), green(3)) -> 0\n",
            "(i (1), love(2)) -> 1\n",
            "(love (2), green(3)) -> 1\n",
            "(eggs (4), love(2)) -> 0\n",
            "(and (5), i(1)) -> 0\n",
            "(and (5), i(1)) -> 0\n",
            "(and (5), ham(6)) -> 1\n",
            "(green (3), eggs(4)) -> 1\n",
            "(green (3), love(2)) -> 1\n",
            "(i (1), i(1)) -> 0\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_7 (InputLayer)            (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_6 (Embedding)         (None, 1, 3)         18          input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_7 (Embedding)         (None, 1, 3)         18          input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "reshape_6 (Reshape)             (None, 3)            0           embedding_6[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "reshape_7 (Reshape)             (None, 3)            0           embedding_7[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dot_3 (Dot)                     (None, 1)            0           reshape_6[0][0]                  \n",
            "                                                                 reshape_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 1)            2           dot_3[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 38\n",
            "Trainable params: 38\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "[[0.4907244 ]\n",
            " [0.5193212 ]\n",
            " [0.49118945]\n",
            " [0.526052  ]\n",
            " [0.51384187]]\n",
            "中心語の数値ベクトル表現は中心語の Embedding 層の重みそのもの\n",
            " [array([[ 0.04059505,  0.29851747,  0.00752139],\n",
            "       [-0.7448236 ,  0.16728735, -0.7247461 ],\n",
            "       [-0.08465606,  0.11112696, -0.79564244],\n",
            "       [-0.27035898, -0.43362698, -0.6831656 ],\n",
            "       [ 0.5476346 , -0.08124566, -0.4291393 ],\n",
            "       [-0.20885772,  0.3852538 ,  0.13520509]], dtype=float32)]\n",
            "i の数値ベクトル表現:  [[0.04059505 0.29851747 0.00752139]]\n",
            "love の数値ベクトル表現 [[-0.7448236   0.16728735 -0.7247461 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Nxi0DrRYRtne",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## CBOW(continuous bag-of-words)\n",
        "- 入力：文脈語（周囲の単語）\n",
        "- 出力：中心語（中央の単語）\n",
        "    - このモデルができるだけ正確な推測ができるように訓練\n",
        "- Embedding層→Lambda層（分散表現の平均値を計算）→Dense層→Softmax層\n",
        "- Embedding層の重みが結果として得られる。"
      ]
    },
    {
      "metadata": {
        "id": "kvx-Ouk5Rtnf",
        "colab_type": "code",
        "outputId": "837539da-1d60-4482-bed3-67d712264066",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Lambda, Embedding\n",
        "import keras.backend as K\n",
        "\n",
        "vocab_size = 5000\n",
        "embed_size = 300\n",
        "window_size = 1\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embed_size,\n",
        "                                        embeddings_initializer='glorot_uniform',\n",
        "                                        input_length=window_size*2))\n",
        "model.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size, )))\n",
        "model.add(Dense(vocab_size, kernel_initializer='glorot_uniform', activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "8qcaS9y4T-4A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 分散表現の抽出\n",
        "- 分類問題を解くことが目的ではない。\n",
        "- 単語を低次元の分散表現に変換する重み行列に興味がある。"
      ]
    },
    {
      "metadata": {
        "id": "hTEbgYuEaYId",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## gensim\n",
        "- word2vecの実装を含むライブラリ\n",
        "- text8 ：Wikipediaの文章から作られたコーパス"
      ]
    },
    {
      "metadata": {
        "id": "X_me_159a1qX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from gensim.models import word2vec\n",
        "\n",
        "sentences = word2vec.Text8Corpus(\"text8\", 50) # 50単語ずつの文に分割\n",
        "model = word2vec.Word2Vec(sentences, size=300, min_count=30)\n",
        "\n",
        "model.init_sims(replace=True) #　正規化（メモリの消費量を大幅に削減できるらしい）\n",
        "model.save(\"word2vec_gensim.bin\")\n",
        "model = word2vec.Word2Vec.load(\"word2vec_gensim.bin\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0s1ZUUS6EQAY",
        "colab_type": "code",
        "outputId": "23076fa6-d0c5-463f-9bfb-2df671c9300f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# 単語一覧取得\n",
        "len(list(model.wv.vocab.keys()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25097"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "metadata": {
        "id": "V-OPHqWHFVzi",
        "colab_type": "code",
        "outputId": "e429e713-ba87-4cf9-ea4f-ebfdf63cbf57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1054
        }
      },
      "cell_type": "code",
      "source": [
        "# 単語の分散表現取得\n",
        "model.wv['microsoft']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.01450746,  0.02820498, -0.00613089, -0.1292292 , -0.00107782,\n",
              "       -0.03602763, -0.01437078, -0.04320436,  0.01124886, -0.01775628,\n",
              "        0.01124919, -0.03617769,  0.0851421 , -0.02290596, -0.06446875,\n",
              "       -0.09365334,  0.01692839,  0.02593309, -0.06995475,  0.02601767,\n",
              "        0.04263943, -0.15761417,  0.00831248,  0.01263973,  0.04029242,\n",
              "        0.0277396 , -0.1320366 , -0.05115852,  0.03440702, -0.00313816,\n",
              "       -0.03922254,  0.01206683,  0.02870189, -0.040955  , -0.01835576,\n",
              "        0.08676125,  0.00474388,  0.01969292, -0.00248312, -0.05054165,\n",
              "       -0.01282372, -0.04106919, -0.00888264,  0.01175106, -0.01316832,\n",
              "        0.08711268,  0.12674089, -0.16148517, -0.08232152, -0.0202497 ,\n",
              "        0.12008891, -0.02567862, -0.04227788,  0.08244476, -0.02162206,\n",
              "       -0.13875192,  0.01047679,  0.05519961,  0.12552491,  0.0270695 ,\n",
              "        0.05264525,  0.0920076 , -0.01546904,  0.09309576,  0.01681585,\n",
              "       -0.05215343,  0.02041112, -0.08895825,  0.02945577,  0.04379288,\n",
              "        0.07396246,  0.07157107,  0.06855446, -0.03233825,  0.07136349,\n",
              "        0.01633662,  0.03508019,  0.02522191,  0.01226883, -0.05878791,\n",
              "        0.04449095,  0.03183516,  0.01896029, -0.01697352, -0.04750479,\n",
              "        0.01474957, -0.00475677,  0.03969185, -0.06743894,  0.00194906,\n",
              "        0.01673318, -0.06021326, -0.0308819 , -0.03801684, -0.00601734,\n",
              "        0.07600353, -0.05814238, -0.07760204, -0.05324456,  0.00512588,\n",
              "        0.03969277,  0.0016431 ,  0.0401759 ,  0.02626919, -0.00031212,\n",
              "       -0.05097511, -0.11852146,  0.00370805,  0.05900479, -0.01662554,\n",
              "        0.01776863,  0.01720092,  0.01278029,  0.04730816,  0.03290856,\n",
              "       -0.0751992 ,  0.01977133,  0.02237769, -0.00833736,  0.03884539,\n",
              "       -0.01843629,  0.03548817, -0.03416052, -0.02539284,  0.03118646,\n",
              "       -0.03310662,  0.03611555,  0.09818263, -0.1043175 , -0.03379919,\n",
              "        0.02047588, -0.13518813,  0.09431419, -0.0361793 , -0.0382911 ,\n",
              "        0.10360285, -0.09380223, -0.05475898,  0.03560813,  0.11146089,\n",
              "       -0.09195495, -0.04522151, -0.04258702, -0.01748872, -0.05302097,\n",
              "       -0.09608251, -0.06568243, -0.08550535, -0.06904449, -0.06233306,\n",
              "       -0.03454594, -0.03894078,  0.05211314, -0.02152418,  0.02382646,\n",
              "        0.12750931, -0.06067916, -0.01968712, -0.03592602, -0.08803196,\n",
              "       -0.04861832, -0.01427706, -0.02630964, -0.05191366,  0.02467615,\n",
              "       -0.03097321,  0.05176264,  0.00355011,  0.01006512,  0.0021838 ,\n",
              "       -0.0191271 , -0.15727001,  0.00705152,  0.04346587,  0.01286337,\n",
              "        0.03731839,  0.05442433, -0.02835036, -0.03993667, -0.03756635,\n",
              "       -0.01964434, -0.05789667, -0.00501582, -0.02117725, -0.09700752,\n",
              "       -0.00215982, -0.01896305,  0.05086284, -0.13907206, -0.01794295,\n",
              "        0.05764528,  0.00131347, -0.07168549,  0.01808144,  0.03952692,\n",
              "        0.05741769, -0.07529162, -0.00535429, -0.04153927,  0.05697857,\n",
              "        0.13523367,  0.07641754, -0.08975778, -0.06913309, -0.02239506,\n",
              "        0.04259809, -0.02146992,  0.11278977, -0.00890694, -0.01963741,\n",
              "        0.04003086,  0.01286743,  0.01516447,  0.02762489, -0.03177286,\n",
              "        0.06105325,  0.02883936, -0.01883138,  0.03115547, -0.03563529,\n",
              "        0.03523046,  0.05385902,  0.03344951, -0.01578994,  0.03403401,\n",
              "        0.09990811, -0.0122599 ,  0.033762  ,  0.03977434,  0.01730652,\n",
              "        0.07296807,  0.0085524 , -0.04505455,  0.06564508,  0.1655733 ,\n",
              "       -0.06092126, -0.00398711, -0.07302188,  0.01890582,  0.00622562,\n",
              "       -0.01860921,  0.01088604, -0.02773072,  0.02774422, -0.03771222,\n",
              "       -0.01377107, -0.0175447 ,  0.02442771, -0.00947993, -0.03705153,\n",
              "        0.04030732, -0.05124028,  0.0183252 , -0.105419  , -0.01863501,\n",
              "        0.02911474,  0.01427798,  0.00840152,  0.01287775,  0.08953036,\n",
              "       -0.05295999,  0.09768299, -0.06189993,  0.04651929, -0.04634933,\n",
              "       -0.17332783,  0.08040877, -0.01146907, -0.10073996, -0.01379406,\n",
              "       -0.00331642, -0.04061096,  0.05050775, -0.07333137,  0.09792157,\n",
              "        0.10549308, -0.13852541,  0.01912831,  0.031602  , -0.05230172,\n",
              "        0.05856447, -0.0727037 , -0.01598583,  0.06653196,  0.10745455,\n",
              "       -0.07174947, -0.05286383, -0.02801774, -0.02509957, -0.00531946,\n",
              "        0.02933531, -0.03625868, -0.01472284,  0.05948167,  0.02143773,\n",
              "        0.01342037,  0.0836764 , -0.00346402, -0.07015313,  0.08876685],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "metadata": {
        "id": "LmsNNMAyGCpg",
        "colab_type": "code",
        "outputId": "7d9773da-79fc-4e9f-f92b-4c839a7c61ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "cell_type": "code",
      "source": [
        "# 類似した語を表示（コサイン類似度）\n",
        "model.wv.most_similar('man')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('woman', 0.6701514720916748),\n",
              " ('girl', 0.5859889388084412),\n",
              " ('creature', 0.5346399545669556),\n",
              " ('boy', 0.4951860010623932),\n",
              " ('person', 0.48739463090896606),\n",
              " ('men', 0.4811897873878479),\n",
              " ('stranger', 0.4765658974647522),\n",
              " ('thief', 0.4699924886226654),\n",
              " ('ahab', 0.46971046924591064),\n",
              " ('gentleman', 0.4658960998058319)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "metadata": {
        "id": "kNSgnDc0aj42",
        "colab_type": "code",
        "outputId": "9f7c11bf-890e-41a6-ac19-d4b487b7e45d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('google')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('yahoo', 0.7883809804916382),\n",
              " ('wiki', 0.7523242235183716),\n",
              " ('newsgroup', 0.7492305040359497),\n",
              " ('https', 0.7419716119766235),\n",
              " ('usenet', 0.7396144866943359),\n",
              " ('ftp', 0.7395017147064209),\n",
              " ('msn', 0.7351704835891724),\n",
              " ('faq', 0.7306565046310425),\n",
              " ('tutorials', 0.7125604152679443),\n",
              " ('cpan', 0.7059267163276672)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "metadata": {
        "id": "bDaJhDpXWPKe",
        "colab_type": "code",
        "outputId": "edb831f1-9a6d-48e4-93a0-1bff5deb2fcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "cell_type": "code",
      "source": [
        "# woman + king - man\n",
        "model.wv.most_similar(positive=['banana', 'orange'], negative=['peach'], topn=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('sugar', 0.5563383102416992),\n",
              " ('plantations', 0.5513566732406616),\n",
              " ('juice', 0.5482078790664673),\n",
              " ('cotton', 0.5477759838104248),\n",
              " ('potato', 0.5443365573883057),\n",
              " ('wool', 0.5394809246063232),\n",
              " ('herring', 0.5300459861755371),\n",
              " ('shrimp', 0.5292163491249084),\n",
              " ('timber', 0.5209341049194336),\n",
              " ('beef', 0.5176421999931335)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "metadata": {
        "id": "wb-k4FpIIOLZ",
        "colab_type": "code",
        "outputId": "24d46129-9a6b-4211-edf9-b0b9fdfb536e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "model.wv.similarity(\"girl\", \"boy\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6871714"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "metadata": {
        "id": "bZlkvNPcXJ7a",
        "colab_type": "code",
        "outputId": "4f4b056f-a843-4106-e4cd-c167f4bc9ce7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "model.wv.similarity('girl', 'car')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.32335734"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "metadata": {
        "id": "K9wzAvV4aCTG",
        "colab_type": "code",
        "outputId": "69efd60b-69c7-4e89-ac2f-abdc7443eb29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "model.wv.similarity('apple', 'google')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.22032908"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "metadata": {
        "id": "JIUZG991Jy63",
        "colab_type": "code",
        "outputId": "6aff74d1-a962-4ceb-dd9d-1fda436f31d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "model.wv.similarity(\"will\", \"going\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.32553226"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "metadata": {
        "id": "xWJOn4ouQN0B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## GloVe\n",
        "- カウントベースの手法\n",
        "- $R = PQ \\approx R'$になるように$P, Q$をSGDで学習させる。\n",
        "  - $R:$共起行列 $P:$単語特徴行列 $Q: $文脈特徴行列\n",
        "  - 再構築誤差を最小化させる\n",
        "- 並列化させることで学習が高速になる\n",
        "- word2vecよりも精度が高い\n",
        "- 実装があまりない\n",
        "- glove-pythonというライブラリがあるhttps://github.com/maciejkula/glove-python\n",
        "- Pytorch実装 https://nbviewer.jupyter.org/github/DSKSD/DeepNLP-models-Pytorch/blob/master/notebooks/03.GloVe.ipynb\n",
        "  "
      ]
    },
    {
      "metadata": {
        "id": "7-T1ymOvbiH8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## CNNを用いた分散表現の学習\n",
        "- 一度に数単語を処理する畳み込みフィルターを学習　→　プーリングして最も重要な概念を表現するベクトルを作成\n"
      ]
    },
    {
      "metadata": {
        "id": "UqcL2dODVptd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "from keras.layers import Dense, Dropout, Conv1D, Embedding, GlobalMaxPooling1D\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import codecs\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5sjBzQ2zOb90",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 前処理の流れ（英語の場合）\n",
        "- nltk.word_tokenize等を使って単語を分割\n",
        "- word_to_id、id_to_wordのディクショナリを作成\n",
        "- コーパスを単語IDのリストに変換\n",
        "\n",
        "- 日本語　https://qiita.com/Hironsan/items/2466fe0f344115aff177"
      ]
    },
    {
      "metadata": {
        "id": "ZFymEkwjhT3t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = 5000 #上位5000個のトークンのみを考慮\n",
        "EMBED_SIZE = 100\n",
        "NUM_FILTERS = 256\n",
        "NUM_WORDS = 3 # 各フィルターのサイズ（一度に畳み込む単語の数）\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# 語彙の構築\n",
        "counter = collections.Counter()\n",
        "with codecs.open(\"umich-sentiment-train.txt\", \"r\", encoding=\"utf-8\") as fin:\n",
        "  maxlen = 0\n",
        "  for line in fin:\n",
        "    _, sent = line.strip().split(\"\\t\")\n",
        "    try:\n",
        "      words = [x.lower() for x in nltk.word_tokenize(sent)] # 単語分割\n",
        "    except LookupError:\n",
        "      nltk.download(\"punkt\")\n",
        "      words = [x.lower() for x in nltk.word_tokenize(sent)]\n",
        "    maxlen = max(maxlen, len(words))\n",
        "    for word in words:\n",
        "      counter[word] += 1\n",
        "    word2index = collections.defaultdict(int) # 初期値が0になるdict\n",
        "    for wid, word in enumerate(counter.most_common(VOCAB_SIZE)):\n",
        "      word2index[word[0]] = wid + 1\n",
        "    vocab_sz = len(word2index) + 1\n",
        "    index2word = {v: k for k, v in word2index.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cy8pAUq3nZTG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "xs, ys = [], []\n",
        "with codecs.open(\"umich-sentiment-train.txt\", \"r\", encoding=\"utf-8\") as fin:\n",
        "  for line in fin:\n",
        "    label, sent = line.strip().split(\"\\t\")\n",
        "    ys.append(int(label))\n",
        "    words = [x.lower() for x in nltk.word_tokenize(sent)]\n",
        "    wids = [word2index[word] for word in words]\n",
        "    xs.append(wids)\n",
        "\n",
        "X = pad_sequences(xs, maxlen=maxlen)\n",
        "Y = np_utils.to_categorical(ys)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fC6hPw2QpSJI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.3, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mEIZh7GTokHk",
        "colab_type": "code",
        "outputId": "7dd4b6e3-4cfa-41ed-c46f-1b1435818d32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        }
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_sz, EMBED_SIZE, input_length=maxlen))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv1D(filters=NUM_FILTERS, kernel_size=NUM_WORDS, activation=\"relu\"))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(2, activation=\"softmax\"))\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(Xtrain, Ytrain, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,\n",
        "                   validation_data=(Xtest, Ytest))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 4960 samples, validate on 2126 samples\n",
            "Epoch 1/20\n",
            "4960/4960 [==============================] - 1s 252us/step - loss: 0.3100 - acc: 0.8911 - val_loss: 0.0395 - val_acc: 0.9845\n",
            "Epoch 2/20\n",
            "4960/4960 [==============================] - 1s 129us/step - loss: 0.0205 - acc: 0.9923 - val_loss: 0.0208 - val_acc: 0.9925\n",
            "Epoch 3/20\n",
            "4960/4960 [==============================] - 1s 128us/step - loss: 0.0071 - acc: 0.9980 - val_loss: 0.0175 - val_acc: 0.9939\n",
            "Epoch 4/20\n",
            "4960/4960 [==============================] - 1s 131us/step - loss: 0.0030 - acc: 0.9994 - val_loss: 0.0167 - val_acc: 0.9953\n",
            "Epoch 5/20\n",
            "4960/4960 [==============================] - 1s 126us/step - loss: 0.0019 - acc: 0.9996 - val_loss: 0.0164 - val_acc: 0.9934\n",
            "Epoch 6/20\n",
            "4960/4960 [==============================] - 1s 131us/step - loss: 0.0012 - acc: 0.9998 - val_loss: 0.0160 - val_acc: 0.9929\n",
            "Epoch 7/20\n",
            "4960/4960 [==============================] - 1s 135us/step - loss: 0.0013 - acc: 0.9998 - val_loss: 0.0161 - val_acc: 0.9958\n",
            "Epoch 8/20\n",
            "4960/4960 [==============================] - 1s 137us/step - loss: 0.0013 - acc: 0.9996 - val_loss: 0.0158 - val_acc: 0.9962\n",
            "Epoch 9/20\n",
            "4960/4960 [==============================] - 1s 140us/step - loss: 9.6072e-04 - acc: 0.9996 - val_loss: 0.0159 - val_acc: 0.9948\n",
            "Epoch 10/20\n",
            "4960/4960 [==============================] - 1s 132us/step - loss: 7.8549e-04 - acc: 0.9998 - val_loss: 0.0158 - val_acc: 0.9939\n",
            "Epoch 11/20\n",
            "4960/4960 [==============================] - 1s 135us/step - loss: 9.2373e-04 - acc: 0.9998 - val_loss: 0.0157 - val_acc: 0.9958\n",
            "Epoch 12/20\n",
            "4960/4960 [==============================] - 1s 139us/step - loss: 8.8358e-04 - acc: 0.9998 - val_loss: 0.0164 - val_acc: 0.9939\n",
            "Epoch 13/20\n",
            "4960/4960 [==============================] - 1s 140us/step - loss: 0.0013 - acc: 0.9996 - val_loss: 0.0161 - val_acc: 0.9939\n",
            "Epoch 14/20\n",
            "4960/4960 [==============================] - 1s 140us/step - loss: 7.6681e-04 - acc: 0.9996 - val_loss: 0.0160 - val_acc: 0.9958\n",
            "Epoch 15/20\n",
            "4960/4960 [==============================] - 1s 143us/step - loss: 9.2738e-04 - acc: 0.9996 - val_loss: 0.0157 - val_acc: 0.9958\n",
            "Epoch 16/20\n",
            "4960/4960 [==============================] - 1s 141us/step - loss: 5.7211e-04 - acc: 0.9998 - val_loss: 0.0169 - val_acc: 0.9958\n",
            "Epoch 17/20\n",
            "4960/4960 [==============================] - 1s 141us/step - loss: 8.6316e-04 - acc: 0.9998 - val_loss: 0.0169 - val_acc: 0.9939\n",
            "Epoch 18/20\n",
            "4960/4960 [==============================] - 1s 141us/step - loss: 0.0010 - acc: 0.9998 - val_loss: 0.0164 - val_acc: 0.9958\n",
            "Epoch 19/20\n",
            "4960/4960 [==============================] - 1s 140us/step - loss: 0.0010 - acc: 0.9998 - val_loss: 0.0168 - val_acc: 0.9939\n",
            "Epoch 20/20\n",
            "4960/4960 [==============================] - 1s 144us/step - loss: 8.0845e-04 - acc: 0.9998 - val_loss: 0.0169 - val_acc: 0.9958\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MVS_ti_vCe1k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## word2vecで学習した分散表現のファインチューニング\n",
        "- gensimを用いてword2vecモデルを読み込む\n",
        "\n",
        "いますぐ使える単語埋め込みベクトルのリスト\n",
        "https://qiita.com/Hironsan/items/8f7d35f0a36e0f99752c"
      ]
    },
    {
      "metadata": {
        "id": "KwAo20eQpn_l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "from keras.layers import Dense, Dropout, Conv1D, Embedding, GlobalMaxPooling1D\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import codecs\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2C2652TxDCGf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "WORD2VEC_MODEL = 'GoogleNews-vectors-negative300.bin.gz'\n",
        "VOCAB_SIZE = 5000 #上位5000個のトークンのみを考慮\n",
        "EMBED_SIZE = 300\n",
        "NUM_FILTERS = 256\n",
        "NUM_WORDS = 3 # 各フィルターのサイズ（一度に畳み込む単語の数）\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 10 # 少ないエポック数で学習可能\n",
        "\n",
        "# 語彙の構築\n",
        "counter = collections.Counter()\n",
        "with codecs.open(\"umich-sentiment-train.txt\", \"r\", encoding=\"utf-8\") as fin:\n",
        "  maxlen = 0\n",
        "  for line in fin:\n",
        "    _, sent = line.strip().split(\"\\t\")\n",
        "    try:\n",
        "      words = [x.lower() for x in nltk.word_tokenize(sent)] # 単語分割\n",
        "    except LookupError:\n",
        "      nltk.download(\"punkt\")\n",
        "      words = [x.lower() for x in nltk.word_tokenize(sent)]\n",
        "    maxlen = max(maxlen, len(words))\n",
        "    for word in words:\n",
        "      counter[word] += 1\n",
        "    word2index = collections.defaultdict(int) # 初期値が0になるdict\n",
        "    for wid, word in enumerate(counter.most_common(VOCAB_SIZE)):\n",
        "      word2index[word[0]] = wid + 1\n",
        "    vocab_sz = len(word2index) + 1\n",
        "    index2word = {v: k for k, v in word2index.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l5MuxnPPDLXs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "xs, ys = [], []\n",
        "with codecs.open(\"umich-sentiment-train.txt\", \"r\", encoding=\"utf-8\") as fin:\n",
        "  for line in fin:\n",
        "    label, sent = line.strip().split(\"\\t\")\n",
        "    ys.append(int(label))\n",
        "    words = [x.lower() for x in nltk.word_tokenize(sent)]\n",
        "    wids = [word2index[word] for word in words]\n",
        "    xs.append(wids)\n",
        "\n",
        "X = pad_sequences(xs, maxlen=maxlen)\n",
        "Y = np_utils.to_categorical(ys)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PaaM6s6tLWHR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.3, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WX36_p_KDPYE",
        "colab_type": "code",
        "outputId": "a52e85d4-247d-479c-93ce-36f2dcb3b0a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        }
      },
      "cell_type": "code",
      "source": [
        "# word2vecモデルの読み込み\n",
        "word2vec = KeyedVectors.load_word2vec_format(WORD2VEC_MODEL, binary=True)\n",
        "embedding_weights = np.zeros((vocab_sz, EMBED_SIZE))\n",
        "for word, index in word2index.items():\n",
        "  try:\n",
        "    embedding_weights[index, :] = word2vec[word]\n",
        "  except KeyError:\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-120-1859d3a2836e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWORD2VEC_MODEL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0membedding_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_sz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMBED_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword2index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0membedding_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1436\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1437\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                     \u001b[0mch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb' '\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_not_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mREAD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/_compression.py\u001b[0m in \u001b[0;36m_check_not_closed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_not_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"I/O operation on closed file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/gzip.py\u001b[0m in \u001b[0;36mclosed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclosed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Oi8pfp4SHL94",
        "colab_type": "code",
        "outputId": "ff689755-8897-48e1-e32b-177e05b75bfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_sz, EMBED_SIZE, input_length=maxlen, weights=[embedding_weights], trainable=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv1D(filters=NUM_FILTERS, kernel_size=NUM_WORDS, activation=\"relu\"))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(2, activation=\"softmax\"))\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(Xtrain, Ytrain, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,\n",
        "                   validation_data=(Xtest, Ytest))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 4960 samples, validate on 2126 samples\n",
            "Epoch 1/10\n",
            "4960/4960 [==============================] - 5s 1ms/step - loss: 0.1261 - acc: 0.9565 - val_loss: 0.0229 - val_acc: 0.9915\n",
            "Epoch 2/10\n",
            "4960/4960 [==============================] - 1s 169us/step - loss: 0.0117 - acc: 0.9968 - val_loss: 0.0161 - val_acc: 0.9934\n",
            "Epoch 3/10\n",
            "4960/4960 [==============================] - 1s 169us/step - loss: 0.0053 - acc: 0.9988 - val_loss: 0.0147 - val_acc: 0.9953\n",
            "Epoch 4/10\n",
            "4960/4960 [==============================] - 1s 171us/step - loss: 0.0025 - acc: 0.9996 - val_loss: 0.0134 - val_acc: 0.9958\n",
            "Epoch 5/10\n",
            "4960/4960 [==============================] - 1s 173us/step - loss: 0.0015 - acc: 0.9998 - val_loss: 0.0160 - val_acc: 0.9953\n",
            "Epoch 6/10\n",
            "4960/4960 [==============================] - 1s 171us/step - loss: 0.0013 - acc: 0.9998 - val_loss: 0.0135 - val_acc: 0.9944\n",
            "Epoch 7/10\n",
            "4960/4960 [==============================] - 1s 166us/step - loss: 0.0018 - acc: 0.9996 - val_loss: 0.0121 - val_acc: 0.9962\n",
            "Epoch 8/10\n",
            "4960/4960 [==============================] - 1s 168us/step - loss: 7.6114e-04 - acc: 0.9998 - val_loss: 0.0163 - val_acc: 0.9967\n",
            "Epoch 9/10\n",
            "4960/4960 [==============================] - 1s 181us/step - loss: 0.0030 - acc: 0.9994 - val_loss: 0.0182 - val_acc: 0.9967\n",
            "Epoch 10/10\n",
            "4960/4960 [==============================] - 1s 171us/step - loss: 0.0014 - acc: 0.9998 - val_loss: 0.0129 - val_acc: 0.9939\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9pjPAqT4fNgH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 日本語word2vec\n",
        "https://qiita.com/makaishi2/items/63b7986f6da93dc55edd"
      ]
    },
    {
      "metadata": {
        "id": "hlW2e5EsROu3",
        "colab_type": "code",
        "outputId": "7add4fc2-6b54-417d-8d15-31d02eaee9be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "# 夏目漱石「こゝろ」\n",
        "!wget https://www.aozora.gr.jp/cards/000148/files/773_ruby_5968.zip\n",
        "!unzip 773_ruby_5968.zip\n",
        "!ls -l kokoro.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-01-10 04:58:41--  https://www.aozora.gr.jp/cards/000148/files/773_ruby_5968.zip\n",
            "Resolving www.aozora.gr.jp (www.aozora.gr.jp)... 59.106.13.115\n",
            "Connecting to www.aozora.gr.jp (www.aozora.gr.jp)|59.106.13.115|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 153688 (150K) [application/zip]\n",
            "Saving to: ‘773_ruby_5968.zip.1’\n",
            "\n",
            "773_ruby_5968.zip.1 100%[===================>] 150.09K   398KB/s    in 0.4s    \n",
            "\n",
            "2019-01-10 04:58:43 (398 KB/s) - ‘773_ruby_5968.zip.1’ saved [153688/153688]\n",
            "\n",
            "Archive:  773_ruby_5968.zip\n",
            "Made with MacWinZipper™\n",
            "replace kokoro.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "-rw------- 1 root root 374152 Oct 31  2010 kokoro.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oWpSoqV8fWWm",
        "colab_type": "code",
        "outputId": "81753740-c5f0-4611-8f4b-0659e9a38f81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "cell_type": "code",
      "source": [
        "import codecs\n",
        "# ファイル読込み、内部表現化\n",
        "f = codecs.open('kokoro.txt', \"r\", \"sjis\")\n",
        "text = f.read()\n",
        "f.close()\n",
        "\n",
        "# ファイル整形\n",
        "import re\n",
        "# ヘッダ部分の除去\n",
        "text = re.split('\\-{5,}',text)[2]\n",
        "# フッタ部分の除去\n",
        "text = re.split('底本：',text)[0]\n",
        "# | の除去\n",
        "text = text.replace('|', '')\n",
        "# ルビの削除\n",
        "text = re.sub('《.+?》', '', text)\n",
        "# 入力注の削除\n",
        "text = re.sub('［＃.+?］', '',text)\n",
        "# 空行の削除\n",
        "text = re.sub('\\n\\n', '\\n', text) \n",
        "text = re.sub('\\r', '', text)\n",
        "\n",
        "# 整形結果確認\n",
        "\n",
        "# 頭の100文字の表示 \n",
        "print(text[:100])\n",
        "# 見やすくするため、空行 \n",
        "print()\n",
        "print()\n",
        "# 後ろの100文字の表示 \n",
        "print(text[-100:])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "上　先生と私\n",
            "\n",
            "\n",
            "一\n",
            "\n",
            "　私はその人を常に先生と呼んでいた。だからここでもただ先生と書くだけで本名は打ち明けない。これは世間を憚かる遠慮というよりも、その方が私にとって自然だからである。私はその人\n",
            "\n",
            "\n",
            "、なるべく純白に保存しておいてやりたいのが私の唯一の希望なのですから、私が死んだ後でも、妻が生きている以上は、あなた限りに打ち明けられた私の秘密として、すべてを腹の中にしまっておいて下さい。」\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DCuOtBsdfnFg",
        "colab_type": "code",
        "outputId": "4b4aacfe-e562-43d1-ef58-0082b7937dd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Janomeのインストール（形態素解析ツール）\n",
        "!pip install janome\n",
        "\n",
        "# Janomeのロード\n",
        "from janome.tokenizer import Tokenizer\n",
        "\n",
        "# Tokenneizerインスタンスの生成 \n",
        "t = Tokenizer()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: janome in /usr/local/lib/python3.6/dist-packages (0.3.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7Jwhxwozf9Jl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokens = t.tokenize(text)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I7Wda_7mgJU1",
        "colab_type": "code",
        "outputId": "6239d844-1830-4977-c02c-795911746822",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "print(tokens[13].base_form)\n",
        "print(tokens[13].part_of_speech)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "を\n",
            "助詞,格助詞,一般,*\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AYWNoT6QhSIi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# テキストを引数として、形態素解析の結果、名詞・動詞原型のみを配列で抽出する関数を定義 \n",
        "def extract_words(text):\n",
        "    tokens = t.tokenize(text)\n",
        "    return [token.base_form for token in tokens \n",
        "        if token.part_of_speech.split(',')[0] in['名詞', '動詞']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BrEAKUGBiBug",
        "colab_type": "code",
        "outputId": "5d3215fd-0305-4c1c-9e20-bfc6fdd9792e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "ret = extract_words('純白に保存しておいてやりたいのが私の唯一の希望なのですから')\n",
        "for word in ret:\n",
        "    print(word)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "純白\n",
            "保存\n",
            "する\n",
            "おく\n",
            "やる\n",
            "の\n",
            "私\n",
            "唯一\n",
            "希望\n",
            "の\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gyUgkaWxiE3h",
        "colab_type": "code",
        "outputId": "0cff1b4e-6e75-4bd8-a647-af7ef20fe1a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "# 全体のテキストを句点('。')で区切った配列にする。 \n",
        "sentences = text.split('。')\n",
        "# それぞれの文章を単語リストに変換(処理に数分かかります)\n",
        "word_list = [extract_words(sentence) for sentence in sentences]\n",
        "\n",
        "# 結果の一部を確認 \n",
        "for word in word_list[0]:\n",
        "    print(word)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "上\n",
            "先生\n",
            "私\n",
            "一\n",
            "私\n",
            "人\n",
            "先生\n",
            "呼ぶ\n",
            "いる\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g5okzJQ9iSIF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Word2Vecライブラリのロード\n",
        "from gensim.models import word2vec\n",
        "\n",
        "# size: 圧縮次元数\n",
        "# min_count: 出現頻度の低いものをカットする\n",
        "# window: 前後の単語を拾う際の窓の広さを決める\n",
        "# iter: 機械学習の繰り返し回数(デフォルト:5)十分学習できていないときにこの値を調整する\n",
        "# model.wv.most_similarの結果が1に近いものばかりで、model.dict['wv']のベクトル値が小さい値ばかりの \n",
        "# ときは、学習回数が少ないと考えられます。\n",
        "# その場合、iterの値を大きくして、再度学習を行います。\n",
        "\n",
        "# 事前準備したword_listを使ってWord2Vecの学習実施\n",
        "model = word2vec.Word2Vec(word_list, size=100,min_count=5,window=5,iter=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rstVghyVjaDA",
        "colab_type": "code",
        "outputId": "a50b5b6f-a17a-4aed-e445-18a371d0939d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(list(model.wv.vocab.keys()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1103"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "metadata": {
        "id": "DMroLVVvifF7",
        "colab_type": "code",
        "outputId": "aa5c2963-fa2f-47ec-f25d-8a81fe053a4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "cell_type": "code",
      "source": [
        "model.wv['先生']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.63152605, -0.4034008 ,  0.49717602, -0.8836001 ,  0.0215765 ,\n",
              "        0.07995319,  0.46121264,  0.47798818, -1.1586196 ,  1.0796764 ,\n",
              "       -0.8952845 ,  0.7590851 ,  0.17977805, -0.39549515,  0.49009183,\n",
              "        0.01022144,  0.04203074,  0.33179945,  0.0563493 , -0.2233401 ,\n",
              "        0.53783077, -0.0076797 , -0.46059194, -0.24684972, -0.48515633,\n",
              "        1.2881328 ,  0.47631183,  0.37234816, -0.7335672 ,  0.04842857,\n",
              "        1.0791168 , -0.07906754, -1.168488  ,  0.7914109 ,  0.68883497,\n",
              "        0.11833351,  0.34808576,  0.05332313,  0.1454438 , -0.44862634,\n",
              "       -0.5389132 , -0.26333973, -0.03994103, -0.5803978 ,  0.3351799 ,\n",
              "       -1.607527  , -0.50265354,  0.63783556, -0.8477243 , -0.5878949 ,\n",
              "        0.00829663, -0.4590744 ,  0.9432696 ,  0.4642421 , -0.01725088,\n",
              "        0.2717031 ,  0.11149205, -0.423973  ,  0.6660051 , -0.5853358 ,\n",
              "       -0.7193064 , -0.2939907 , -0.01995611,  0.92258596,  0.30660108,\n",
              "        0.8585386 ,  0.29963535,  0.6583268 ,  0.06508068, -0.41248184,\n",
              "       -0.28941277, -1.7798368 , -1.0173247 ,  0.5275156 , -0.07146647,\n",
              "        0.29916567, -0.23791009, -0.5384943 , -0.01458816,  0.10340828,\n",
              "       -0.7395678 ,  0.09189656,  0.28803983,  0.5498398 ,  1.036149  ,\n",
              "       -0.96739715,  0.23577008,  1.0713229 ,  1.0725569 , -0.25788638,\n",
              "       -0.1601295 ,  0.70868486,  0.11248059,  0.6604036 ,  0.7887639 ,\n",
              "       -0.54858404, -0.3656097 , -0.64352953,  1.143566  ,  0.11296365],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "metadata": {
        "id": "Bd26_wsVit4p",
        "colab_type": "code",
        "outputId": "2ed8d792-01be-4cb4-f5fb-757e03c3eac8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('私')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('の', 0.2945314049720764),\n",
              " ('相応', 0.27585119009017944),\n",
              " ('いる', 0.264004647731781),\n",
              " ('与える', 0.2619418501853943),\n",
              " ('事情', 0.2493799924850464),\n",
              " ('あなた', 0.24713750183582306),\n",
              " ('引き取る', 0.23413300514221191),\n",
              " ('約束', 0.22421950101852417),\n",
              " ('遍', 0.22350457310676575),\n",
              " ('限り', 0.22086213529109955)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "metadata": {
        "id": "2HqzgjSgjOet",
        "colab_type": "code",
        "outputId": "c267bb13-b204-45c7-e51d-a289040a1c00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "cell_type": "code",
      "source": [
        "model.wv.most_similar(positive=['世間', '罪悪'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('笑', 0.6247190237045288),\n",
              " ('性', 0.6137335300445557),\n",
              " ('談', 0.6023606657981873),\n",
              " ('本当', 0.5751911997795105),\n",
              " ('立場', 0.5639123916625977),\n",
              " ('活動', 0.5386861562728882),\n",
              " ('いらっしゃる', 0.5336498618125916),\n",
              " ('昂奮', 0.49236541986465454),\n",
              " ('倫理', 0.49098604917526245),\n",
              " ('君', 0.482044517993927)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "metadata": {
        "id": "zdm67n9Sjk9O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}